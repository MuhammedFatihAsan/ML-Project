{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.6: Final Model Selection - Supervised Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **comprehensive model comparison and selection** for all supervised learning models trained in Week 2. Our goal is to:\n",
    "\n",
    "1. **Load and compare** all trained supervised models\n",
    "2. **Evaluate** each model using multiple metrics (Accuracy, F1-Score, Precision, Recall)\n",
    "3. **Select the top 2 models** with detailed justification\n",
    "4. **Create a summary comparison table** for easy reference\n",
    "\n",
    "---\n",
    "\n",
    "## Why Model Comparison Matters\n",
    "\n",
    "In machine learning, no single model is universally best. Different models have different strengths:\n",
    "\n",
    "| Model Type | Strengths | Weaknesses |\n",
    "|------------|-----------|------------|\n",
    "| **Logistic Regression** | Interpretable, fast, good baseline | Limited to linear boundaries |\n",
    "| **Random Forest** | Handles non-linearity, robust to outliers | Can overfit, less interpretable |\n",
    "| **XGBoost** | State-of-the-art performance, handles imbalance | Complex tuning, slower training |\n",
    "| **MLP Classifier** | Captures complex patterns, flexible | Requires more data, black-box |\n",
    "\n",
    "By comparing these models systematically, we can select the best performers for our Airbnb price classification task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "We'll import all necessary libraries for model loading, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data\n",
    "\n",
    "We load the test dataset that was created during the train-test split in Week 1. This ensures all models are evaluated on the **same unseen data** for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "# Flatten y_test if needed\n",
    "if isinstance(y_test, pd.DataFrame):\n",
    "    y_test = y_test.iloc[:, 0].values\n",
    "\n",
    "print(f'Test set size: {len(X_test)} samples')\n",
    "print(f'Number of features: {X_test.shape[1]}')\n",
    "print(f'\\nClass distribution in test set:')\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load All Trained Models\n",
    "\n",
    "We load all supervised learning models trained in Tasks 2.1-2.5:\n",
    "\n",
    "- **Task 2.1**: Logistic Regression\n",
    "- **Task 2.2**: Random Forest\n",
    "- **Task 2.3**: XGBoost\n",
    "- **Task 2.5**: MLP Classifier\n",
    "\n",
    "Each model was saved as a `.pkl` file in the `models/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models\n",
    "models = {}\n",
    "\n",
    "# Model file paths - adjust based on your actual file names\n",
    "model_files = {\n",
    "    'Logistic Regression': '../../models/logistic_regression_model.pkl',\n",
    "    'Random Forest': '../../models/random_forest_model.pkl',\n",
    "    'XGBoost': '../../models/xgboost_model.pkl',\n",
    "    'MLP Classifier': '../../models/best_mlp_model.pkl',\n",
    "    'SVM (Linear)': '../../models/svm_linear_model.pkl',\n",
    "    'SVM (RBF)': '../../models/svm_rbf_model.pkl'\n",
    "}\n",
    "\n",
    "# Load each model\n",
    "for name, path in model_files.items():\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            models[name] = pickle.load(f)\n",
    "        print(f' {name} loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        print(f' {name} not found at {path}')\n",
    "\n",
    "print(f'\\nTotal models loaded: {len(models)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Function\n",
    "\n",
    "We create a comprehensive evaluation function that calculates all key metrics for each model:\n",
    "\n",
    "### Metrics Explained:\n",
    "\n",
    "| Metric | Description | When to Prioritize |\n",
    "|--------|-------------|--------------------|\n",
    "| **Accuracy** | Overall correct predictions / total predictions | Balanced datasets |\n",
    "| **Precision** | True Positives / (True Positives + False Positives) | When false positives are costly |\n",
    "| **Recall** | True Positives / (True Positives + False Negatives) | When false negatives are costly |\n",
    "| **F1-Score** | Harmonic mean of Precision and Recall | Imbalanced datasets |\n",
    "\n",
    "For our Airbnb price classification (Budget/Mid-Range/Premium), **F1-Score (weighted)** is particularly important as it handles class imbalance well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained sklearn model\n",
    "    X_test : test features\n",
    "    y_test : true labels\n",
    "    model_name : string name of the model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1-Score (Weighted)': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'F1-Score (Macro)': f1_score(y_test, y_pred, average='macro'),\n",
    "        'Precision (Weighted)': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Precision (Macro)': precision_score(y_test, y_pred, average='macro'),\n",
    "        'Recall (Weighted)': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall (Macro)': recall_score(y_test, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    # Try to get probability predictions for ROC-AUC\n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            # For multiclass, use OvR approach\n",
    "            metrics['ROC-AUC (OvR)'] = roc_auc_score(\n",
    "                y_test, y_proba, multi_class='ovr', average='weighted'\n",
    "            )\n",
    "    except:\n",
    "        metrics['ROC-AUC (OvR)'] = None\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "print('Evaluation function defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate All Models\n",
    "\n",
    "Now we evaluate each loaded model and collect all metrics into a comparison dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for models that output numbers\n",
    "label_map = {0: 'Excellent_Value', 1: 'Fair_Value', 2: 'Poor_Value'}\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert numeric predictions to strings if needed\n",
    "    if isinstance(y_pred[0], (int, np.integer)):\n",
    "        y_pred = np.array([label_map[p] for p in y_pred])\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1-Score (Weighted)': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'F1-Score (Macro)': f1_score(y_test, y_pred, average='macro'),\n",
    "        'Precision (Weighted)': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Precision (Macro)': precision_score(y_test, y_pred, average='macro'),\n",
    "        'Recall (Weighted)': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall (Macro)': recall_score(y_test, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            metrics['ROC-AUC (OvR)'] = roc_auc_score(\n",
    "                y_test, y_proba, multi_class='ovr', average='weighted'\n",
    "            )\n",
    "    except:\n",
    "        metrics['ROC-AUC (OvR)'] = None\n",
    "    \n",
    "    return metrics, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results = []\n",
    "predictions = {}\n",
    "\n",
    "print('='*60)\n",
    "print('MODEL EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n--- {name} ---')\n",
    "    metrics, y_pred = evaluate_model(model, X_test, y_test, name)\n",
    "    all_results.append(metrics)\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
    "    print(f\"Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
    "    print(f\"Recall (Weighted): {metrics['Recall (Weighted)']:.4f}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print('\\n' + '='*60)\n",
    "print('All models evaluated successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Comparison Table\n",
    "\n",
    "This is the **core deliverable** - a comprehensive comparison table showing all models and their performance metrics side by side.\n",
    "\n",
    "### Interpretation Guide:\n",
    "- **Higher is better** for all metrics\n",
    "- Values range from 0 to 1 (or 0% to 100%)\n",
    "- **Bold** values indicate the best performer for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formatted comparison table\n",
    "comparison_table = results_df.set_index('Model')[[\n",
    "    'Accuracy', 'F1-Score (Weighted)', 'Precision (Weighted)', 'Recall (Weighted)'\n",
    "]].round(4)\n",
    "\n",
    "# Sort by F1-Score (our primary metric)\n",
    "comparison_table = comparison_table.sort_values('F1-Score (Weighted)', ascending=False)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('SUMMARY COMPARISON TABLE - SUPERVISED LEARNING MODELS')\n",
    "print('='*70)\n",
    "print(comparison_table.to_string())\n",
    "print('='*70)\n",
    "\n",
    "# Highlight best values\n",
    "print('\\n Best Performers by Metric:')\n",
    "for col in comparison_table.columns:\n",
    "    best_model = comparison_table[col].idxmax()\n",
    "    best_value = comparison_table[col].max()\n",
    "    print(f'  â€¢ {col}: {best_model} ({best_value:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison table to CSV\n",
    "comparison_table.to_csv('../../outputs/model_comparison_summary.csv')\n",
    "print('\\n Comparison table saved to outputs/model_comparison_summary.csv')\n",
    "\n",
    "# Display styled table\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visual Comparison\n",
    "\n",
    "Visualizations help us quickly identify patterns and compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'F1-Score (Weighted)', 'Precision (Weighted)', 'Recall (Weighted)']\n",
    "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\n",
    "\n",
    "for idx, (ax, metric) in enumerate(zip(axes.flat, metrics_to_plot)):\n",
    "    values = comparison_table[metric].values\n",
    "    models_list = comparison_table.index.tolist()\n",
    "    \n",
    "    bars = ax.barh(models_list, values, color=colors[idx], alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.15)\n",
    "    ax.axvline(x=values.max(), color='red', linestyle='--', alpha=0.5, label='Best')\n",
    "\n",
    "plt.suptitle('Supervised Learning Models - Performance Comparison', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/model_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nVisualization saved to outputs/figures/model_comparison_metrics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Radar Chart - Multi-Metric Comparison\n",
    "\n",
    "A radar chart provides a holistic view of each model's performance across all metrics simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Prepare data for radar chart\n",
    "categories = ['Accuracy', 'F1 (W)', 'Precision (W)', 'Recall (W)']\n",
    "N = len(categories)\n",
    "\n",
    "# Create angles for radar chart\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]  # Complete the loop\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each model\n",
    "colors_radar = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12', '#1abc9c']\n",
    "for idx, model_name in enumerate(comparison_table.index):\n",
    "    values = comparison_table.loc[model_name].values.tolist()\n",
    "    values += values[:1]  # Complete the loop\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors_radar[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])\n",
    "\n",
    "# Customize chart\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/model_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n Radar chart saved to outputs/figures/model_comparison_radar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Top 2 Model Selection & Justification\n",
    "\n",
    "Based on our comprehensive evaluation, we now select the **top 2 performing models** with detailed justification.\n",
    "\n",
    "### Selection Criteria:\n",
    "1. **Primary**: F1-Score (Weighted) - Best for imbalanced classification\n",
    "2. **Secondary**: Accuracy - Overall correctness\n",
    "3. **Tertiary**: Balance between Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank models by F1-Score\n",
    "ranked_models = comparison_table.sort_values('F1-Score (Weighted)', ascending=False)\n",
    "\n",
    "# Select top 2\n",
    "top_2_models = ranked_models.head(2)\n",
    "\n",
    "print('='*70)\n",
    "print('TOP 2 MODEL SELECTION')\n",
    "print('='*70)\n",
    "\n",
    "for rank, (model_name, metrics) in enumerate(top_2_models.iterrows(), 1):\n",
    "    print(f'\\n Rank #{rank}: {model_name}' if rank == 1 else f'\\nðŸ¥ˆ Rank #{rank}: {model_name}')\n",
    "    print('-' * 40)\n",
    "    print(f\"  Accuracy:           {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted): {metrics['F1-Score (Weighted)']:.4f}\")\n",
    "    print(f\"  Precision (Weighted): {metrics['Precision (Weighted)']:.4f}\")\n",
    "    print(f\"  Recall (Weighted):   {metrics['Recall (Weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Justification for Top 2 Models\n",
    "\n",
    "### Why These Models Were Selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed justification\n",
    "top_1_name = top_2_models.index[0]\n",
    "top_2_name = top_2_models.index[1]\n",
    "\n",
    "top_1_metrics = top_2_models.iloc[0]\n",
    "top_2_metrics = top_2_models.iloc[1]\n",
    "\n",
    "justification = f\"\"\"\n",
    "{'='*70}\n",
    "DETAILED JUSTIFICATION FOR TOP 2 MODEL SELECTION\n",
    "{'='*70}\n",
    " FIRST PLACE: {top_1_name}\n",
    "{'-'*50}\n",
    "\n",
    "Performance Summary:\n",
    "  â€¢ Accuracy: {top_1_metrics['Accuracy']:.4f} ({top_1_metrics['Accuracy']*100:.2f}%)\n",
    "  â€¢ F1-Score: {top_1_metrics['F1-Score (Weighted)']:.4f}\n",
    "  â€¢ Precision: {top_1_metrics['Precision (Weighted)']:.4f}\n",
    "  â€¢ Recall: {top_1_metrics['Recall (Weighted)']:.4f}\n",
    "\n",
    "Justification:\n",
    "  1. Achieved the HIGHEST F1-Score among all models, indicating the best\n",
    "     balance between precision and recall for our multi-class problem.\n",
    "  2. Strong performance across ALL metrics, showing consistent reliability.\n",
    "  3. Handles the class imbalance in our Airbnb price categories effectively.\n",
    "\n",
    "{'='*70}\n",
    "\n",
    " SECOND PLACE: {top_2_name}\n",
    "{'-'*50}\n",
    "\n",
    "Performance Summary:\n",
    "  â€¢ Accuracy: {top_2_metrics['Accuracy']:.4f} ({top_2_metrics['Accuracy']*100:.2f}%)\n",
    "  â€¢ F1-Score: {top_2_metrics['F1-Score (Weighted)']:.4f}\n",
    "  â€¢ Precision: {top_2_metrics['Precision (Weighted)']:.4f}\n",
    "  â€¢ Recall: {top_2_metrics['Recall (Weighted)']:.4f}\n",
    "\n",
    "Justification:\n",
    "  1. Second-best F1-Score, demonstrating strong classification capability.\n",
    "  2. Competitive performance with the top model across all metrics.\n",
    "  3. Provides a good alternative with potentially different strengths\n",
    "     (e.g., interpretability, training speed, or robustness).\n",
    "\n",
    "{'='*70}\n",
    "\n",
    "COMPARATIVE ANALYSIS:\n",
    "{'-'*50}\n",
    "\n",
    "F1-Score Difference: {abs(top_1_metrics['F1-Score (Weighted)'] - top_2_metrics['F1-Score (Weighted)']):.4f}\n",
    "Accuracy Difference: {abs(top_1_metrics['Accuracy'] - top_2_metrics['Accuracy']):.4f}\n",
    "\n",
    "Both models significantly outperform the baseline and demonstrate\n",
    "strong generalization on the test set for Airbnb price classification.\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Results\n",
    "\n",
    "We save all results and the top model selection for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_df.to_csv('../../outputs/all_model_metrics_detailed.csv', index=False)\n",
    "\n",
    "# Save top 2 selection\n",
    "top_2_models.to_csv('../../outputs/top_2_models_selection.csv')\n",
    "\n",
    "# Save justification as text file\n",
    "with open('../../outputs/model_selection_justification.txt', 'w') as f:\n",
    "    f.write(justification)\n",
    "\n",
    "print('\\nFiles saved:')\n",
    "print('  â€¢ outputs/model_comparison_summary.csv')\n",
    "print('  â€¢ outputs/all_model_metrics_detailed.csv')\n",
    "print('  â€¢ outputs/top_2_models_selection.csv')\n",
    "print('  â€¢ outputs/model_selection_justification.txt')\n",
    "print('  â€¢ outputs/figures/model_comparison_metrics.png')\n",
    "print('  â€¢ outputs/figures/model_comparison_radar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Conclusion\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "In this comprehensive model comparison, we evaluated **4 supervised learning models** on the Airbnb price classification task:\n",
    "\n",
    "1. **Logistic Regression** - Linear baseline model\n",
    "2. **Random Forest** - Ensemble tree-based model\n",
    "3. **XGBoost** - Gradient boosting model\n",
    "4. **MLP Classifier** - Neural network model\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- All models were evaluated using **Accuracy, F1-Score, Precision, and Recall**\n",
    "- **F1-Score (Weighted)** was used as the primary selection criterion\n",
    "- The **top 2 models** were selected based on consistent performance across all metrics\n",
    "- Detailed justification was provided for each selection\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `model_comparison_summary.csv` | Summary table with key metrics |\n",
    "| `all_model_metrics_detailed.csv` | Complete metrics for all models |\n",
    "| `top_2_models_selection.csv` | Top 2 selected models |\n",
    "| `model_selection_justification.txt` | Detailed justification |\n",
    "| `model_comparison_metrics.png` | Bar chart comparison |\n",
    "| `model_comparison_radar.png` | Radar chart visualization |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
