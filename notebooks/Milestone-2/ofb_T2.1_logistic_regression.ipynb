{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.1: Baseline Model - Logistic Regression Implementation \n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to establish a baseline classification model using Logistic Regression. This model will serve as the benchmark for comparing more complex supervised learning algorithms in subsequent tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Understanding Key Metrics\n",
    "\n",
    "Since this is a supervised classification task, we use specific metrics to evaluate how well our model performs:\n",
    "\n",
    "1. **Accuracy:** - Measures the overall correctness of predictions. Range is 0 to 1, where 1 indicates perfect classification.\n",
    "2. **Precision (Macro):** - Average precision across all classes, measuring the proportion of correct positive predictions.\n",
    "3. **Recall (Macro):** - Average recall across all classes, measuring the proportion of actual positives correctly identified.\n",
    "4. **F1-Score (Macro):** - Harmonic mean of precision and recall, providing a balanced measure of model performance. Range is 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "\n",
    "Import all required libraries for data manipulation, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Load Clean Data from T1.5\n",
    "\n",
    "**CRITICAL:** We load the pre-processed data from T1.5 which contains only landlord-controlled features.\n",
    "\n",
    "**Features excluded (by T1.5):**\n",
    "- **Review-based:** All review scores, number of reviews, reviews per month, etc.\n",
    "- **Target leakage:** fp_score, rating_normalized, price_normalized, value_category\n",
    "- **Identifiers:** id, host_id\n",
    "- **Dates:** host_since, first_review, last_review\n",
    "\n",
    "**Features included:**\n",
    "- Price and accommodation features (price, bedrooms, beds, bathrooms, accommodates)\n",
    "- Host characteristics (superhost, response rate, listings count)\n",
    "- Location (latitude, longitude, neighbourhood, city)\n",
    "- Availability metrics\n",
    "- Property attributes (property_type, room_type)\n",
    "- Engineered features (host_years, space_efficiency, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLEAN dataset from T1.5 (landlord features only)\n",
    "print(\"=\"*80)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load pre-split data from T1.5\n",
    "X_train = pd.read_csv('../../data/processed/X_train_landlord.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_landlord.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train_landlord.csv').squeeze()\n",
    "y_test = pd.read_csv('../../data/processed/y_test_landlord.csv').squeeze()\n",
    "\n",
    "print(f\"\\n Data loaded successfully!\")\n",
    "print(f\" Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\" Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n Target distribution (Training):\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\n Class balance (Training):\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\n Target distribution (Test):\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(f\"\\n Features included:\")\n",
    "feature_cols = X_train.columns.tolist()\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "for i, col in enumerate(feature_cols[:15], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(feature_cols) > 15:\n",
    "    print(f\"  ... and {len(feature_cols) - 15} more\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove non-numeric columns\n",
    "print(\"Checking for non-numeric columns...\")\n",
    "non_numeric = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Non-numeric columns found: {non_numeric}\")\n",
    "\n",
    "if non_numeric:\n",
    "    X_train = X_train.drop(columns=non_numeric)\n",
    "    X_test = X_test.drop(columns=non_numeric)\n",
    "    feature_cols = X_train.columns.tolist()\n",
    "    print(f\" Dropped {len(non_numeric)} columns. New shape: {X_train.shape}\")\n",
    "\n",
    "   \n",
    "feature_cols = X_train.columns.tolist()\n",
    "print(f\"Final feature set has {len(feature_cols)} numeric features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Train Logistic Regression Model\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **C=1.0:** Regularization strength (inverse, smaller = stronger regularization)\n",
    "- **penalty='l2':** Ridge regularization (prevents overfitting)\n",
    "- **max_iter=1000:** Maximum iterations for convergence\n",
    "- **random_state=42:** For reproducibility\n",
    "- **class_weight='balanced':** Automatically adjust weights for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "X_train_scaled = pd.read_csv('../../data/processed/X_train_landlord_scaled.csv')\n",
    "X_test_scaled = pd.read_csv('../../data/processed/X_test_landlord_scaled.csv')\n",
    "\n",
    "\n",
    "print(\"Training model...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\" Model training complete!\")\n",
    "\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Classes: {lr_model.classes_}\")\n",
    "print(f\"  Number of iterations: {lr_model.n_iter_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Model Evaluation\n",
    "\n",
    "Evaluate model performance on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Model Performance Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:    {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Precision (Macro):   {test_precision:.4f}\")\n",
    "print(f\"Recall (Macro):      {test_recall:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {test_f1:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfit_gap = train_acc - test_acc\n",
    "print(f\"\\nOverfitting check:\")\n",
    "print(f\"  Train-Test Gap: {overfit_gap:.4f} ({overfit_gap*100:.2f}%)\")\n",
    "if overfit_gap < 0.05:\n",
    "    print(\"  Good generalization!\")\n",
    "elif overfit_gap < 0.10:\n",
    "    print(\"  Slight overfitting\")\n",
    "else:\n",
    "    print(\"  Significant overfitting\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"This model can predict value for NEW listings without reviews!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Detailed Classification Report\n",
    "\n",
    "Per-class performance breakdown showing precision, recall, and F1-score for each value category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Prediction distribution\n",
    "print(\"\\nPrediction distribution on test set:\")\n",
    "pred_counts = pd.Series(y_test_pred).value_counts()\n",
    "for category in sorted(pred_counts.index):\n",
    "    count = pred_counts[category]\n",
    "    pct = count / len(y_test_pred) * 100\n",
    "    print(f\"  {category}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "# Actual distribution\n",
    "print(\"\\nActual distribution on test set:\")\n",
    "actual_counts = pd.Series(y_test).value_counts()\n",
    "for category in sorted(actual_counts.index):\n",
    "    count = actual_counts[category]\n",
    "    pct = count / len(y_test) * 100\n",
    "    print(f\"  {category}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for classification by examining model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients for each class\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'Excellent_coef': lr_model.coef_[0],\n",
    "    'Fair_coef': lr_model.coef_[1],\n",
    "    'Poor_coef': lr_model.coef_[2]\n",
    "})\n",
    "\n",
    "# Calculate average absolute coefficient\n",
    "feature_importance['avg_abs_coef'] = feature_importance[['Excellent_coef', 'Fair_coef', 'Poor_coef']].abs().mean(axis=1)\n",
    "feature_importance = feature_importance.sort_values('avg_abs_coef', ascending=False)\n",
    "\n",
    "\n",
    "for i, row in feature_importance.head(20).iterrows():\n",
    "    print(f\"{row['feature']:50s} | Avg |Coef|: {row['avg_abs_coef']:.4f}\")\n",
    "\n",
    "print(\"\\n Interpretation:\")\n",
    "print(\"  - Larger |coefficient| = more important for classification\")\n",
    "print(\"  - Positive coef = increases probability of that class\")\n",
    "print(\"  - Negative coef = decreases probability of that class\")\n",
    "print(\"\\n Key Insight:\")\n",
    "print(\"  Features related to pricing, location, and amenities are most influential.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 7: Save Results and Model\n",
    "\n",
    "Save all outputs for future use and comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "Path('../../models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save scaled data with _landlord suffix\n",
    "X_train_scaled.to_csv('../../data/processed/X_train_landlord_scaled_lr.csv', index=False)\n",
    "X_test_scaled.to_csv('../../data/processed/X_test_landlord_scaled_lr.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Save model\n",
    "with open('../../models/logistic_regression_landlord.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'model': ['Logistic Regression (Landlord Features)'],\n",
    "    'train_accuracy': [train_acc],\n",
    "    'test_accuracy': [test_acc],\n",
    "    'precision_macro': [test_precision],\n",
    "    'recall_macro': [test_recall],\n",
    "    'f1_macro': [test_f1],\n",
    "    'num_features': [len(feature_cols)],\n",
    "    'data_leakage': ['NO - Review features removed'],\n",
    "    'production_ready': ['YES - Can predict for new listings']\n",
    "})\n",
    "results_df.to_csv('../../data/processed/logistic_regression_landlord_results.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test.values,\n",
    "    'y_pred': y_test_pred\n",
    "})\n",
    "predictions_df.to_csv('../../data/processed/logistic_regression_landlord_predictions.csv', index=False)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('../../data/processed/logistic_regression_landlord_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(\" data/processed/\")\n",
    "print(\"     ├── X_train_landlord_scaled_lr.csv\")\n",
    "print(\"     ├── X_test_landlord_scaled_lr.csv\")\n",
    "print(\"     ├── logistic_regression_landlord_results.csv\")\n",
    "print(\"     ├── logistic_regression_landlord_predictions.csv\")\n",
    "print(\"     └── logistic_regression_landlord_feature_importance.csv\")\n",
    "print(\"\\n models/\")\n",
    "print(\"     ├── logistic_regression_landlord.pkl\")\n",
    "print(\"     └── standard_scaler_landlord.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. **Fixed Data Leakage:** Removed all review-based features from model input\n",
    "2. **Production-Ready:** Model can predict for new listings without reviews\n",
    "3. **Baseline Established:** This serves as the benchmark for more complex models\n",
    "\n",
    "###  Key Findings:\n",
    "- **Features Used:** Only landlord-controlled attributes\n",
    "- **Generalization:** Good (Small train-test gap)\n",
    "- **Class Balance:** Model handles all three value categories fairly\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
