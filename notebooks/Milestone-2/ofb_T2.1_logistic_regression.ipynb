{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.1: Baseline Model - Logistic Regression Implementation \n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to establish a baseline classification model using Logistic Regression. This model will serve as the benchmark for comparing more complex supervised learning algorithms in subsequent tasks.\n",
    "\n",
    "## Understanding Key Metrics\n",
    "\n",
    "Since this is a supervised classification task, we use specific metrics to evaluate how well our model performs:\n",
    "\n",
    "1. **Accuracy:** - Measures the overall correctness of predictions. Range is 0 to 1, where 1 indicates perfect classification.\n",
    "2. **Precision (Macro):** - Average precision across all classes, measuring the proportion of correct positive predictions.\n",
    "3. **Recall (Macro):** - Average recall across all classes, measuring the proportion of actual positives correctly identified.\n",
    "4. **F1-Score (Macro):** - Harmonic mean of precision and recall, providing a balanced measure of model performance. Range is 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "\n",
    "Import all required libraries for data manipulation, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Load Data and Feature Selection\n",
    "\n",
    "\n",
    "**Features to EXCLUDE:**\n",
    "- **IDs:** `id`, `host_id` (unique identifiers, not predictive)\n",
    "- **Dates:** `host_since`, `first_review`, `last_review` (already converted to numeric features)\n",
    "- **Target:** `value_category` (what we're predicting)\n",
    "- **Data Leakage:** `fp_score`, `rating_normalized`, `price_normalized` (used to create the target)\n",
    "\n",
    "**Features to INCLUDE (62 total):**\n",
    "- Price and accommodation features\n",
    "- Review scores (CRITICAL!)\n",
    "- Host characteristics\n",
    "- Availability metrics\n",
    "- Location features\n",
    "- Categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "df = pd.read_csv('../../data/processed/listings_cleaned_with_target.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Define features to EXCLUDE\n",
    "exclude_cols = [\n",
    "    'id', 'host_id', 'host_since', 'first_review', 'last_review',\n",
    "    'value_category',  # Target variable\n",
    "    'fp_score',  # Data leakage: used to create target\n",
    "    'rating_normalized',  # Data leakage: part of fp_score\n",
    "    'price_normalized'  # Data leakage: part of fp_score\n",
    "]\n",
    "\n",
    "# Select all other columns as features\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nFeatures selected: {len(feature_cols)}\")\n",
    "print(f\"\\nTop 10 features:\")\n",
    "for i, col in enumerate(feature_cols[:10], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df[feature_cols].copy()\n",
    "y = df['value_category'].copy()\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(y.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test Split (80-20)\n",
    "\n",
    "Split data into training (80%) and testing (20%) sets with stratification to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "# Remove leaky features\n",
    "leaky_features = [\n",
    "    'price', 'price_normalized', 'price_per_person', 'price_per_bathroom',\n",
    "    'price_per_bedroom', 'review_scores_rating', 'review_scores_value',\n",
    "    'value_density', 'estimated_revenue_l365d'  # these also use price\n",
    "]\n",
    "\n",
    "# Drop leaky features that exist in the dataset\n",
    "cols_to_drop = [col for col in leaky_features if col in X_train.columns]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "# Updated feature_cols to match\n",
    "feature_cols = X_train.columns.tolist()\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} leaky features: {cols_to_drop}\")\n",
    "print(f\"Remaining features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "for category in sorted(y_train.unique()):\n",
    "    count = (y_train == category).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"  {category}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 4: Feature Scaling (Standardization)\n",
    "\n",
    "**Why Scale?**\n",
    "- Features have different ranges (e.g., price: 0-1000, bedrooms: 1-5)\n",
    "- Logistic Regression is sensitive to feature scales\n",
    "- Standardization: Transform to mean=0, std=1\n",
    "\n",
    "**Important:** Fit scaler on training data only, then transform both train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\" Features scaled successfully!\")\n",
    "print(f\"\\nScaling verification (first 3 features):\")\n",
    "for col in feature_cols[:3]:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Train - Mean: {X_train_scaled[col].mean():.6f}, Std: {X_train_scaled[col].std():.6f}\")\n",
    "    print(f\"  Test  - Mean: {X_test_scaled[col].mean():.6f}, Std: {X_test_scaled[col].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 5: Train Logistic Regression Model\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **C=1.0:** Regularization strength (inverse, smaller = stronger regularization)\n",
    "- **penalty='l2':** Ridge regularization (prevents overfitting)\n",
    "- **max_iter=1000:** Maximum iterations for convergence\n",
    "- **random_state=42:** For reproducibility\n",
    "- **class_weight='balanced':** Automatically adjust weights for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\" Model training complete!\")\n",
    "\n",
    "print(f\"\\nModel details:\")\n",
    "print(f\"  Classes: {lr_model.classes_}\")\n",
    "print(f\"  Number of iterations: {lr_model.n_iter_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation\n",
    "\n",
    "Evaluate model performance on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_scaled)\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:    {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Precision (Macro):   {test_precision:.4f}\")\n",
    "print(f\"Recall (Macro):      {test_recall:.4f}\")\n",
    "print(f\"F1-Score (Macro):    {test_f1:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfit_gap = train_acc - test_acc\n",
    "print(f\"\\nOverfitting check:\")\n",
    "print(f\" Train-Test Gap: {overfit_gap:.4f} ({overfit_gap*100:.2f}%)\")\n",
    "if overfit_gap < 0.05:\n",
    "    print(\" Good generalization!\")\n",
    "elif overfit_gap < 0.10:\n",
    "    print(\" Slight overfitting\")\n",
    "else:\n",
    "    print(\" Significant overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 7: Detailed Classification Report\n",
    "\n",
    "Per-class performance breakdown showing precision, recall, and F1-score for each value category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Prediction distribution\n",
    "print(\"\\nPrediction distribution on test set:\")\n",
    "pred_counts = pd.Series(y_test_pred).value_counts()\n",
    "for category in sorted(pred_counts.index):\n",
    "    count = pred_counts[category]\n",
    "    pct = count / len(y_test_pred) * 100\n",
    "    print(f\"  {category}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 8: Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for classification by examining model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients for each class\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'Excellent_coef': lr_model.coef_[0],\n",
    "    'Fair_coef': lr_model.coef_[1],\n",
    "    'Poor_coef': lr_model.coef_[2]\n",
    "})\n",
    "\n",
    "# Calculate average absolute coefficient\n",
    "feature_importance['avg_abs_coef'] = feature_importance[['Excellent_coef', 'Fair_coef', 'Poor_coef']].abs().mean(axis=1)\n",
    "feature_importance = feature_importance.sort_values('avg_abs_coef', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" TOP 15 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in feature_importance.head(15).iterrows():\n",
    "    print(f\"{row['feature']:45s} | Avg |Coef|: {row['avg_abs_coef']:.4f}\")\n",
    "\n",
    "print(\"\\n Interpretation:\")\n",
    "print(\"  - Larger |coefficient| = more important for classification\")\n",
    "print(\"  - Positive coef = increases probability of that class\")\n",
    "print(\"  - Negative coef = decreases probability of that class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 9: Save Results and Model\n",
    "\n",
    "Save all outputs for future use and comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "Path('../../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save scaled data\n",
    "X_train_scaled.to_csv('../../data/processed/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../../data/processed/X_test_scaled.csv', index=False)\n",
    "y_train.to_frame(name='value_category').to_csv('../../data/processed/y_train.csv', index=False)\n",
    "y_test.to_frame(name='value_category').to_csv('../../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save scaler\n",
    "with open('../../models/standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save model\n",
    "with open('../../models/logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'model': ['Logistic Regression'],\n",
    "    'train_accuracy': [train_acc],\n",
    "    'test_accuracy': [test_acc],\n",
    "    'precision_macro': [test_precision],\n",
    "    'recall_macro': [test_recall],\n",
    "    'f1_macro': [test_f1]\n",
    "})\n",
    "results_df.to_csv('../../data/processed/logistic_regression_results.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test.values,\n",
    "    'y_pred': y_test_pred\n",
    "})\n",
    "predictions_df.to_csv('../../data/processed/logistic_regression_predictions.csv', index=False)\n",
    "\n",
    "print(\" ALL FILES SAVED SUCCESSFULLY!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"   data/processed/\")\n",
    "print(\"     ├── X_train_scaled.csv\")\n",
    "print(\"     ├── X_test_scaled.csv\")\n",
    "print(\"     ├── y_train.csv\")\n",
    "print(\"     ├── y_test.csv\")\n",
    "print(\"     ├── logistic_regression_results.csv\")\n",
    "print(\"     └── logistic_regression_predictions.csv\")\n",
    "print(\"\\n   models/\")\n",
    "print(\"     ├── logistic_regression_model.pkl\")\n",
    "print(\"     └── standard_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
