{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.7: K-Means Clustering Implementation\n",
    "\n",
    "## Objective\n",
    "The goal of this task is to group Airbnb listings into 3 clusters (Excellent, Fair, and Poor Value) using an unsupervised approach and validate these clusters using Elbow and Silhouette methods.\n",
    "\n",
    "## Understanding Key Metrics\n",
    "\n",
    "Since this is an unsupervised task, we use specific metrics to evaluate how well our algorithm performs without seeing the ground truth labels:\n",
    "\n",
    "1. **Purity Score:** - Measures the extent to which each cluster contains a single class. A purity of 1 indicates perfect clustering.\n",
    "2. **Adjusted Rand Index (ARI):** - Measures the similarity between two clusterings (predicted vs actual) while adjusting for chance. Range is -1 to 1, where 1 is a perfect match.\n",
    "3. **Silhouette Score:** - Measures how similar an object is to its own cluster compared to other clusters. Range is -1 to 1, where higher values indicate well-separated clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Discovery\n",
    "In this step, we import the required libraries and locate the preprocessed dataset in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Define the relative path to the data folder\n",
    "data_folder = \"../../data/\"\n",
    "\n",
    "# List all files to identify the correct dataset\n",
    "print(\"Searching for data files in:\", os.path.abspath(data_folder))\n",
    "try:\n",
    "    data_files = os.listdir(data_folder)\n",
    "    print(\"Files found in data directory:\")\n",
    "    for file in data_files:\n",
    "        print(f\"- {file}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The system cannot find the specified data path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Data Verification\n",
    "Now, we will examine the 'processed' index to find the processed version of our dataset for our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the contents of the processed folder\n",
    "processed_folder = \"../../data/processed/\"\n",
    "\n",
    "print(\"Files in processed directory:\")\n",
    "try:\n",
    "    processed_files = os.listdir(processed_folder)\n",
    "    for file in processed_files:\n",
    "        print(f\"- {file}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: processed folder not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Loading the Dataset\n",
    "We will load the pre-scaled training features and their corresponding labels. The features will be used for clustering, while the labels will serve as the ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for the specific files\n",
    "x_train_path = os.path.join(processed_folder, \"X_train_scaled.csv\")\n",
    "y_train_path = os.path.join(processed_folder, \"y_train.csv\")\n",
    "\n",
    "# Loading the datasets\n",
    "X_train = pd.read_csv(x_train_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "\n",
    "# Verification\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Displaying the first few rows of features\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 4: K-Means Implementation (K=3)\n",
    "In this step, we initialize the K-Means algorithm with 3 clusters. We use the 'k-means++' initialization method to ensure better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize K-Means with K=3\n",
    "# random_state is set to 42 for reproducibility\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42, n_init=10)\n",
    "\n",
    "# Fitting the model to the scaled data\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Assigning the cluster labels to our data\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "print(\"K-Means clustering completed.\")\n",
    "print(\"Cluster assignments for the first 10 samples:\")\n",
    "print(cluster_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 5: Elbow Method for Optimal K\n",
    "The Elbow Method helps us validate if K=3 is a reasonable choice by plotting the \"Within-Cluster Sum of Squares\" (Inertia) for different values of K. We look for a \"bend\" in the graph, similar to an elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the inertia (within-cluster sum of squares) for each K\n",
    "inertia_values = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "# Loop through different K values from 1 to 10\n",
    "for k in k_range:\n",
    "    kmeans_model = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "    kmeans_model.fit(X_train)\n",
    "    inertia_values.append(kmeans_model.inertia_)\n",
    "\n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia_values, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (WCSS)')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Interpretation of the Elbow Plot\n",
    "\n",
    "The Elbow Method provides a visual representation of the **Within-Cluster Sum of Squares (Inertia)** as a function of the number of clusters (K).\n",
    "\n",
    "### Observations:\n",
    "1. **Diminishing Returns:** As $K$ increases, the inertia consistently decreases because the clusters become smaller and the points are closer to their respective centroids.\n",
    "2. **The \"Elbow\" Point:** Looking at the graph, we observe a significant \"bend\" or \"elbow\" typically between **$K=2$** and **$K=3$**. This point indicates that adding more clusters beyond this value does not provide a substantial decrease in inertia.\n",
    "3. **Consistency with Ground Truth:** Since our target labels (Excellent, Fair, and Poor Value) naturally form 3 categories, selecting **$K=3$** is mathematically justifiable as it aligns with the business logic of our project while still being near the elbow point.\n",
    "\n",
    "**Conclusion:** We will proceed with $K=3$ for our clustering analysis, as it offers a good balance between model simplicity and data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Silhouette Analysis for K=3\n",
    "While the Elbow Method looks at distances within clusters, the Silhouette Score measures how well-separated the clusters are from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Silhouette Score for our K=3 model\n",
    "score = silhouette_score(X_train, cluster_labels)\n",
    "\n",
    "print(f\"Silhouette Score for K=3: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Interpretation of the Silhouette Score\n",
    "\n",
    "The Silhouette Score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
    "\n",
    "### Analysis of Result:\n",
    "- **Silhouette Score:** 0.7800\n",
    "- **Strength of Structure:** According to standard clustering interpretation, a score above 0.70 indicates a **strong structure**. This means the clusters are well-defined, dense, and clearly separated from each other.\n",
    "- **Project Insight:** This high score suggests that the features used (after scaling) have a high discriminatory power.\n",
    "\n",
    "**Conclusion:** The K-Means algorithm has successfully identified 3 distinct groups within the dataset. The high silhouette value confirms that our 3-cluster assumption is mathematically robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 7: Performance Evaluation against True Labels\n",
    "\n",
    "In this final step, we compare the clusters generated by K-Means with the actual categories (Excellent, Fair, and Poor Value). \n",
    "\n",
    "We will use two main metrics:\n",
    "1. **Adjusted Rand Index (ARI):** To measure the similarity between the two assignments while accounting for chance.\n",
    "2. **Purity Score:** To see how \"pure\" each cluster is in terms of containing a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assigning y_true from the encoded column (integer format)\n",
    "y_true = y_train['value_encoded'].values\n",
    "\n",
    "# Step 2: Recalculating Adjusted Rand Index (ARI)\n",
    "ari_score = adjusted_rand_score(y_true, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
    "\n",
    "# Step 3: Purity Calculation\n",
    "def calculate_purity(y_true, y_pred):\n",
    "    # Generating the contingency matrix (cross-tabulation matrix)\n",
    "    contingency_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # Summing the maximum matches in each cluster\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "purity = calculate_purity(y_true, cluster_labels)\n",
    "print(f\"Purity Score: {purity:.4f}\")\n",
    "\n",
    "# Step 4: Cross-tabulation to see the mapping\n",
    "# This table shows which cluster (0, 1, 2) matches which encoded value (0, 1, 2)\n",
    "comparison_df = pd.DataFrame({'Actual_Encoded': y_true, 'Cluster_Predicted': cluster_labels})\n",
    "crosstab = pd.crosstab(comparison_df['Actual_Encoded'], comparison_df['Cluster_Predicted'])\n",
    "\n",
    "print(\"\\nCross-tabulation Table:\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 8: Final Clustering Performance Results\n",
    "\n",
    "The cross-tabulation table below shows the distribution of ground truth labels (Actual_Encoded) across the clusters discovered by K-Means.\n",
    "\n",
    "### Performance Summary:\n",
    "| Metric | Value | Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Silhouette Score** | 0.7800 | Very Strong Separation |\n",
    "| **Adjusted Rand Index (ARI)** | 0.0048 | No Correlation with Labels |\n",
    "| **Purity Score** | 0.3652 | Near-Random Assignment |\n",
    "\n",
    "### Cross-tabulation Table:\n",
    "| Actual \\ Cluster | Cluster 0 | Cluster 1 | Cluster 2 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **0 (Poor_Value)** | 1879 | 1705 | 1673 |\n",
    "| **1 (Fair_Value)** | 2339 | 1557 | 1522 |\n",
    "| **2 (Excellent_Value)** | 2439 | 1566 | 1249 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Deep Dive into Clustering Results\n",
    "\n",
    "We are observing a significant gap between the **Silhouette Score (0.78)** and the **ARI (0.0048)**. This leads to several technical conclusions:\n",
    "\n",
    "1. **Feature Dominance:** The K-Means algorithm found 3 very distinct clusters. However, these clusters are formed based on features that do not represent \"Value Categories.\" For example, the clusters might be grouping listings by **Geography** (San Francisco vs. San Diego) or **Room Type** instead of the FP Score.\n",
    "2. **The \"Unsupervised\" Reality:** Since K-Means does not see the labels, it optimizes for spatial distance. In our feature space, the listings in \"Excellent Value\" are spatially mixed with \"Poor Value\" listings.\n",
    "3. **Data Overlap:** The cross-tabulation table shows that each cluster contains a nearly equal number of Poor, Fair, and Excellent listings. This confirms that our current feature set, when used in an unsupervised manner, cannot distinguish between the value categories.\n",
    "\n",
    "**Action Item:** In the upcoming tasks (T2.11 PCA), we should investigate which features are driving these 0.78-score clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Task 2.8: Hierarchical Clustering Analysis\n",
    "\n",
    "## Objective\n",
    "The goal of this task is to perform Agglomerative Hierarchical Clustering using three different linkage methods: **Ward, Average, and Complete**. We will visualize the data structure using a Dendrogram and evaluate the results using Silhouette and ARI metrics.\n",
    "\n",
    "## Linkage Methods Explained:\n",
    "1. **Ward:** Minimizes the variance of the clusters being merged. It usually creates clusters of similar sizes.\n",
    "2. **Average:** Uses the average distance between all points in two clusters.\n",
    "3. **Complete:** Uses the maximum distance between points in two clusters (farthest neighbor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step 1: Dendrogram Visualization (Sampling)\n",
    "Hierarchical clustering is computationally expensive. Therefore, we will use a sample of 2,000 rows to visualize the Dendrogram and understand the hierarchical structure of our Airbnb data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sampling 2,000 rows for visualization purposes\n",
    "X_sample = X_train.sample(n=2000, random_state=42)\n",
    "\n",
    "# Computing the linkage matrix using 'ward' method\n",
    "Z = linkage(X_sample, method='ward')\n",
    "\n",
    "# Plotting the Dendrogram\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage - Sampled)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z, truncate_mode='lastp', p=30, leaf_rotation=90., leaf_font_size=10., show_contracted=True)\n",
    "plt.axhline(y=150, color='r', linestyle='--') # Example threshold line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 2: Training Agglomerative Models (K=3)\n",
    "We will now train three separate Hierarchical models on the full training set using Ward, Average, and Complete linkage methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Initializing models with K=3\n",
    "linkage_methods = ['ward', 'average', 'complete']\n",
    "results = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    print(f\"Running Agglomerative Clustering with {method} linkage...\")\n",
    "    model = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    labels = model.fit_predict(X_train)\n",
    "    \n",
    "    # Calculate scores\n",
    "    s_score = silhouette_score(X_train, labels)\n",
    "    a_score = adjusted_rand_score(y_true, labels)\n",
    "    \n",
    "    results[method] = {'Silhouette': s_score, 'ARI': a_score, 'Labels': labels}\n",
    "    print(f\"Finished {method}. Silhouette: {s_score:.4f}, ARI: {a_score:.4f}\")\n",
    "\n",
    "# Convert results to a DataFrame for easy comparison\n",
    "df_results = pd.DataFrame(results).T.drop(columns=['Labels'])\n",
    "print(\"\\nFinal Comparison Table:\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Interpretation of Hierarchical Clustering Results\n",
    "\n",
    "After testing three different linkage methods, we observe a consistent pattern that mirrors our K-Means results.\n",
    "\n",
    "### 1. High Silhouette Scores (>0.71)\n",
    "All three methods (Ward, Average, Complete) yielded very high Silhouette scores. This confirms that the data has a **strong intrinsic structure**. The algorithm is successfully finding distinct groups that are spatially far apart from each other.\n",
    "\n",
    "### 2. Low ARI Scores (~0.005)\n",
    "Despite the strong physical separation of the clusters, the **Adjusted Rand Index (ARI)** remains near zero. This indicates that the natural groupings in the data do not correspond to our 'Excellent', 'Fair', or 'Poor' value categories.\n",
    "\n",
    "### Comparison Table:\n",
    "- **Average Linkage** provided the highest Silhouette score (0.7793), suggesting it found the most compact and well-separated clusters.\n",
    "- **Ward Linkage** provided the (slightly) best ARI (0.0055), though it is still not statistically significant.\n",
    "\n",
    "**Final Conclusion:** The features that dominate the clustering process are likely related to listing characteristics (e.g., location, property type) rather than the price-quality relationship we defined in the target labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
