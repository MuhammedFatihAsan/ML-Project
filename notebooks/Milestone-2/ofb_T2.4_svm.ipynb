{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.4: Support Vector Machine (SVM) Implementation\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to implement Support Vector Machine (SVM) classifiers with both RBF (Radial Basis Function) and Linear kernels. SVM is a powerful algorithm that works by finding the optimal hyperplane that separates different classes with maximum margin. We will tune the C and gamma parameters and compare performance with tree-based models (Random Forest and XGBoost).\n",
    "\n",
    "## Understanding SVM\n",
    "\n",
    "**What is SVM?**\n",
    "\n",
    "Support Vector Machine is a supervised learning algorithm that:\n",
    "1. **Finds the best boundary (hyperplane)** between classes\n",
    "2. **Maximizes the margin** - the distance between the boundary and nearest data points\n",
    "3. **Uses support vectors** - the critical data points closest to the boundary\n",
    "4. **Can handle non-linear patterns** using kernel tricks\n",
    "\n",
    "**Think of it like this:**\n",
    "- Imagine drawing a line to separate two groups of points\n",
    "- SVM finds the line that's as far as possible from both groups\n",
    "- The closest points to this line are the \"support vectors\"\n",
    "- For complex patterns, SVM uses \"kernels\" to transform the data\n",
    "\n",
    "## SVM vs Tree-Based Models\n",
    "\n",
    "| Aspect | SVM | Random Forest | XGBoost |\n",
    "|--------|-----|---------------|----------|\n",
    "| Learning Method | Margin maximization | Ensemble of trees | Sequential boosting |\n",
    "| Decision Boundary | Smooth hyperplane | Rectangular regions | Rectangular regions |\n",
    "| Feature Scaling | **Required** | Not required | Not required |\n",
    "| Interpretability | Low | Medium (feature importance) | Medium (feature importance) |\n",
    "| Training Speed | Slow (large datasets) | Fast | Medium |\n",
    "| High-Dimensional Data | Excellent | Good | Good |\n",
    "| Non-Linear Patterns | Excellent (with kernels) | Good | Excellent |\n",
    "\n",
    "## Why Use SVM?\n",
    "\n",
    "1. **Effective in high-dimensional spaces** - Works well when you have many features\n",
    "2. **Memory efficient** - Only uses support vectors (subset of training data)\n",
    "3. **Versatile** - Different kernel functions for different decision boundaries\n",
    "4. **Robust to overfitting** - Especially in high-dimensional space with proper regularization\n",
    "5. **Works well with clear margin of separation** - Excellent when classes are well-separated\n",
    "\n",
    "## Understanding Kernels\n",
    "\n",
    "**What is a Kernel?**\n",
    "\n",
    "A kernel is a mathematical function that transforms data into a higher dimension where it becomes linearly separable.\n",
    "\n",
    "**Linear Kernel:**\n",
    "- No transformation, works in original space\n",
    "- Best for linearly separable data\n",
    "- Faster training, simpler model\n",
    "- Formula: K(x, y) = x · y\n",
    "\n",
    "**RBF (Radial Basis Function) Kernel:**\n",
    "- Transforms data into infinite dimensions\n",
    "- Can handle complex, non-linear patterns\n",
    "- Most popular kernel for general use\n",
    "- Formula: K(x, y) = exp(-gamma * ||x - y||²)\n",
    "- Creates circular/spherical decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Loading\n",
    "\n",
    "In this step, we import the required libraries and load the preprocessed dataset.\n",
    "\n",
    "### Libraries Used:\n",
    "- **pandas & numpy:** For data manipulation and numerical operations\n",
    "- **SVC:** Support Vector Classifier from sklearn\n",
    "- **sklearn.metrics:** For evaluating model performance\n",
    "- **LabelEncoder:** To convert categorical target labels to numeric format\n",
    "- **pickle:** For saving the trained models\n",
    "- **time:** To measure training time (SVM can be slow)\n",
    "\n",
    "### Why Scaled Data is CRITICAL for SVM:\n",
    "\n",
    "Unlike tree-based models, **SVM is extremely sensitive to feature scales** because:\n",
    "1. **Distance-based algorithm:** SVM uses distances between points\n",
    "2. **Features with larger scales dominate:** A feature ranging 0-1000 will dominate one ranging 0-1\n",
    "3. **Affects kernel calculations:** RBF kernel uses distances, so scale matters\n",
    "4. **Impacts convergence:** Unscaled data makes training slower and less accurate\n",
    "\n",
    "**Example:**\n",
    "- Feature A: price (range 50-500)\n",
    "- Feature B: number of bedrooms (range 1-5)\n",
    "- Without scaling, price dominates the distance calculation\n",
    "- With scaling, both features contribute equally\n",
    "\n",
    "This is why we MUST use the scaled datasets (X_train_landlord_scaled.csv, X_test_landlord_scaled.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load SCALED data (CRITICAL for SVM!)\n",
    "X_train = pd.read_csv('../../data/processed/X_train_landlord_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_landlord_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train_landlord.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test_landlord.csv')\n",
    "\n",
    "# Remove ID columns if present\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "# Encode target labels (SVM requires numeric labels)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train['value_category'])\n",
    "y_test_encoded = label_encoder.transform(y_test['value_category'])\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    category = label_encoder.classes_[val]\n",
    "    print(f\"  Class {val} ({category}): {count} samples ({count/len(y_train_encoded)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: SVM with RBF Kernel\n",
    "\n",
    "We start with the RBF (Radial Basis Function) kernel, which is the most popular and versatile kernel for SVM.\n",
    "\n",
    "### Understanding RBF Kernel Parameters:\n",
    "\n",
    "**1. C (Regularization Parameter):**\n",
    "- Controls the trade-off between smooth decision boundary and classifying training points correctly\n",
    "- **Small C (e.g., 0.1):**\n",
    "  - Wider margin, more tolerance for misclassification\n",
    "  - Simpler model, less overfitting\n",
    "  - May underfit if too small\n",
    "- **Large C (e.g., 100):**\n",
    "  - Narrow margin, tries to classify all training points correctly\n",
    "  - More complex model, risk of overfitting\n",
    "  - Better training accuracy but may not generalize well\n",
    "- **Our choice: C=10** - Good balance for most problems\n",
    "\n",
    "**2. gamma (Kernel Coefficient):**\n",
    "- Defines how far the influence of a single training example reaches\n",
    "- **Small gamma (e.g., 0.001):**\n",
    "  - Far reach, considers points far away\n",
    "  - Smoother decision boundary\n",
    "  - May underfit\n",
    "- **Large gamma (e.g., 1.0):**\n",
    "  - Close reach, only considers nearby points\n",
    "  - More complex, wiggly decision boundary\n",
    "  - Risk of overfitting\n",
    "- **Our choice: gamma='scale'** - Automatically calculated as 1/(n_features * X.var())\n",
    "\n",
    "**3. decision_function_shape='ovr':**\n",
    "- One-vs-Rest strategy for multi-class classification\n",
    "- Trains one classifier per class vs all other classes\n",
    "- Alternative: 'ovo' (One-vs-One) trains one classifier for each pair\n",
    "\n",
    "### Why RBF Kernel?\n",
    "\n",
    "1. **Handles non-linear patterns** - Can create circular/curved decision boundaries\n",
    "2. **Works well when classes overlap** - Can separate complex patterns\n",
    "3. **Default choice** - Good starting point for most problems\n",
    "4. **Flexible** - Can approximate many different decision boundaries\n",
    "\n",
    "### Expected Training Time:\n",
    "\n",
    "SVM with RBF kernel can be slow on large datasets (10,000+ samples). Training may take 1-5 minutes depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "start_time = time.time()\n",
    "\n",
    "svm_rbf = SVC(\n",
    "    kernel='rbf',\n",
    "    C=10,\n",
    "    gamma='scale',\n",
    "    decision_function_shape='ovr',\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svm_rbf.fit(X_train, y_train_encoded)\n",
    "\n",
    "rbf_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n RBF SVM training complete!\")\n",
    "print(f\"Training time: {rbf_training_time:.2f} seconds\")\n",
    "print(f\"Number of support vectors: {len(svm_rbf.support_)}\")\n",
    "print(f\"Support vectors per class: {svm_rbf.n_support_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate RBF SVM Performance\n",
    "\n",
    "Now we evaluate how well the RBF SVM performs on both training and testing data.\n",
    "\n",
    "### Understanding Support Vectors:\n",
    "\n",
    "- **Support vectors** are the training samples that lie closest to the decision boundary\n",
    "- They are the \"critical\" points that define the boundary\n",
    "- Fewer support vectors = simpler model, faster predictions\n",
    "- More support vectors = more complex model, may indicate difficult classification\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Training vs Testing Accuracy Gap:**\n",
    "   - Small gap (< 5%) = good generalization\n",
    "   - Large gap (> 10%) = overfitting, consider reducing C or increasing gamma\n",
    "\n",
    "2. **Comparison with Tree Models:**\n",
    "   - SVM may perform better on high-dimensional data\n",
    "   - Tree models may be faster to train\n",
    "   - Check which has better test accuracy\n",
    "\n",
    "3. **Training Time:**\n",
    "   - SVM is typically slower than Random Forest\n",
    "   - But may be faster than XGBoost on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_rbf = svm_rbf.predict(X_train)\n",
    "y_test_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_rbf = accuracy_score(y_train_encoded, y_train_pred_rbf)\n",
    "test_acc_rbf = accuracy_score(y_test_encoded, y_test_pred_rbf)\n",
    "test_precision_rbf = precision_score(y_test_encoded, y_test_pred_rbf, average='macro')\n",
    "test_recall_rbf = recall_score(y_test_encoded, y_test_pred_rbf, average='macro')\n",
    "test_f1_rbf = f1_score(y_test_encoded, y_test_pred_rbf, average='macro')\n",
    "\n",
    "print(\"\\nSVM with RBF Kernel Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc_rbf:.4f}\")\n",
    "print(f\"  Testing Accuracy:  {test_acc_rbf:.4f}\")\n",
    "print(f\"  Precision (Macro): {test_precision_rbf:.4f}\")\n",
    "print(f\"  Recall (Macro):    {test_recall_rbf:.4f}\")\n",
    "print(f\"  F1-Score (Macro):  {test_f1_rbf:.4f}\")\n",
    "print(f\"\\n Overfitting Check: {abs(train_acc_rbf - test_acc_rbf):.4f} gap\")\n",
    "if abs(train_acc_rbf - test_acc_rbf) < 0.05:\n",
    "    print(\"Good generalization!\")\n",
    "elif abs(train_acc_rbf - test_acc_rbf) < 0.10:\n",
    "    print(\"Slight overfitting\")\n",
    "else:\n",
    "    print(\"Significant overfitting - consider reducing C\")\n",
    "\n",
    "print(\"\\nClassification Report (RBF Kernel):\")\n",
    "print(classification_report(y_test_encoded, y_test_pred_rbf, \n",
    "                          target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 4: SVM with Linear Kernel\n",
    "\n",
    "Now we train SVM with a Linear kernel, which works best when data is linearly separable.\n",
    "\n",
    "### Understanding Linear Kernel:\n",
    "\n",
    "**What is Linear Kernel?**\n",
    "- Creates a straight line (2D) or flat hyperplane (higher dimensions) to separate classes\n",
    "- No transformation of data, works in original feature space\n",
    "- Much faster to train than RBF kernel\n",
    "- Simpler model, easier to interpret\n",
    "\n",
    "**When to Use Linear Kernel:**\n",
    "1. **Large number of features** - When features >> samples\n",
    "2. **Text classification** - Sparse, high-dimensional data\n",
    "3. **Linearly separable data** - When classes can be separated by a straight line\n",
    "4. **Need for speed** - When training time is critical\n",
    "5. **Interpretability** - When you need to understand feature weights\n",
    "\n",
    "### Linear Kernel Parameters:\n",
    "\n",
    "**C (Regularization Parameter):**\n",
    "- Same meaning as in RBF kernel\n",
    "- Controls margin width vs classification accuracy\n",
    "- **Our choice: C=1.0** - Standard default value\n",
    "- Linear kernel is less sensitive to C than RBF\n",
    "\n",
    "**No gamma parameter:**\n",
    "- Linear kernel doesn't use gamma\n",
    "- Only needs to tune C parameter\n",
    "- Simpler hyperparameter tuning\n",
    "\n",
    "### Linear vs RBF Kernel:\n",
    "\n",
    "| Aspect | Linear | RBF |\n",
    "|--------|--------|-----|\n",
    "| Decision Boundary | Straight line/plane | Curved/circular |\n",
    "| Training Speed | Fast | Slow |\n",
    "| Parameters to Tune | C only | C and gamma |\n",
    "| Overfitting Risk | Lower | Higher |\n",
    "| Flexibility | Low | High |\n",
    "| Best For | Linearly separable data | Complex patterns |\n",
    "\n",
    "### Expected Performance:\n",
    "\n",
    "- Linear kernel should train much faster (seconds vs minutes)\n",
    "- May have lower accuracy than RBF if data is non-linear\n",
    "- But could perform similarly if data is mostly linear\n",
    "- Less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train SVM with Linear kernel\n",
    "start_time = time.time()\n",
    "\n",
    "svm_linear = SVC(\n",
    "    kernel='linear',\n",
    "    C=1.0,\n",
    "    decision_function_shape='ovr',\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "svm_linear.fit(X_train, y_train_encoded)\n",
    "\n",
    "linear_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nLinear SVM training complete!\")\n",
    "print(f\"Training time: {linear_training_time:.2f} seconds\")\n",
    "print(f\"Number of support vectors: {len(svm_linear.support_)}\")\n",
    "print(f\"Support vectors per class: {svm_linear.n_support_}\")\n",
    "print(f\"\\nSpeed comparison: Linear is {rbf_training_time/linear_training_time:.2f}x faster than RBF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Linear SVM Performance\n",
    "\n",
    "Evaluate the Linear SVM and compare it with RBF SVM.\n",
    "\n",
    "### Key Comparison Points:\n",
    "\n",
    "1. **Accuracy:** Which kernel gives better test accuracy?\n",
    "2. **Training Time:** How much faster is Linear?\n",
    "3. **Overfitting:** Which kernel generalizes better?\n",
    "4. **Support Vectors:** Fewer support vectors = simpler model\n",
    "\n",
    "### Decision Guide:\n",
    "\n",
    "**Choose Linear Kernel if:**\n",
    "- Accuracy is similar to RBF (within 1-2%)\n",
    "- Training time is much faster\n",
    "- You need a simpler, more interpretable model\n",
    "- You have many features\n",
    "\n",
    "**Choose RBF Kernel if:**\n",
    "- Accuracy is significantly better (> 2%)\n",
    "- Training time is acceptable\n",
    "- Data has complex, non-linear patterns\n",
    "- You can afford the computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_linear = svm_linear.predict(X_train)\n",
    "y_test_pred_linear = svm_linear.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_linear = accuracy_score(y_train_encoded, y_train_pred_linear)\n",
    "test_acc_linear = accuracy_score(y_test_encoded, y_test_pred_linear)\n",
    "test_precision_linear = precision_score(y_test_encoded, y_test_pred_linear, average='macro')\n",
    "test_recall_linear = recall_score(y_test_encoded, y_test_pred_linear, average='macro')\n",
    "test_f1_linear = f1_score(y_test_encoded, y_test_pred_linear, average='macro')\n",
    "\n",
    "print(\"\\nSVM with Linear Kernel Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc_linear:.4f}\")\n",
    "print(f\"  Testing Accuracy:  {test_acc_linear:.4f}\")\n",
    "print(f\"  Precision (Macro): {test_precision_linear:.4f}\")\n",
    "print(f\"  Recall (Macro):    {test_recall_linear:.4f}\")\n",
    "print(f\"  F1-Score (Macro):  {test_f1_linear:.4f}\")\n",
    "print(f\"\\n  Overfitting Check: {abs(train_acc_linear - test_acc_linear):.4f} gap\")\n",
    "if abs(train_acc_linear - test_acc_linear) < 0.05:\n",
    "    print(\"Good generalization!\")\n",
    "elif abs(train_acc_linear - test_acc_linear) < 0.10:\n",
    "    print(\"Slight overfitting\")\n",
    "else:\n",
    "    print(\"Significant overfitting - consider reducing C\")\n",
    "\n",
    "print(\"\\nClassification Report (Linear Kernel):\")\n",
    "print(classification_report(y_test_encoded, y_test_pred_linear, \n",
    "                          target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 6: RBF vs Linear Kernel Comparison\n",
    "\n",
    "Direct comparison between the two SVM kernels to determine which performs better for our Airbnb value category classification problem.\n",
    "\n",
    "### What This Comparison Tells Us:\n",
    "\n",
    "1. **Data Linearity:**\n",
    "   - If Linear performs similarly to RBF → Data is mostly linearly separable\n",
    "   - If RBF significantly outperforms Linear → Data has non-linear patterns\n",
    "\n",
    "2. **Model Complexity:**\n",
    "   - More support vectors = more complex decision boundary\n",
    "   - RBF typically needs more support vectors than Linear\n",
    "\n",
    "3. **Practical Trade-offs:**\n",
    "   - Is the accuracy gain of RBF worth the extra training time?\n",
    "   - For deployment, Linear is faster for predictions too\n",
    "\n",
    "4. **Overfitting Risk:**\n",
    "   - Compare training-testing gaps\n",
    "   - Smaller gap indicates better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"\\n{'Metric':<25} {'RBF':<15} {'Linear':<15} {'Difference'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Training Accuracy':<25} {train_acc_rbf:<15.4f} {train_acc_linear:<15.4f} {train_acc_rbf - train_acc_linear:+.4f}\")\n",
    "print(f\"{'Testing Accuracy':<25} {test_acc_rbf:<15.4f} {test_acc_linear:<15.4f} {test_acc_rbf - test_acc_linear:+.4f}\")\n",
    "print(f\"{'F1-Score (Macro)':<25} {test_f1_rbf:<15.4f} {test_f1_linear:<15.4f} {test_f1_rbf - test_f1_linear:+.4f}\")\n",
    "print(f\"{'Precision (Macro)':<25} {test_precision_rbf:<15.4f} {test_precision_linear:<15.4f} {test_precision_rbf - test_precision_linear:+.4f}\")\n",
    "print(f\"{'Recall (Macro)':<25} {test_recall_rbf:<15.4f} {test_recall_linear:<15.4f} {test_recall_rbf - test_recall_linear:+.4f}\")\n",
    "\n",
    "print(\"\\n Model Complexity:\")\n",
    "print(f\"{'Support Vectors (RBF)':<25} {len(svm_rbf.support_)}\")\n",
    "print(f\"{'Support Vectors (Linear)':<25} {len(svm_linear.support_)}\")\n",
    "print(f\"{'Training Time (RBF)':<25} {rbf_training_time:.2f} seconds\")\n",
    "print(f\"{'Training Time (Linear)':<25} {linear_training_time:.2f} seconds\")\n",
    "print(f\"{'Speed Advantage':<25} Linear is {rbf_training_time/linear_training_time:.2f}x faster\")\n",
    "\n",
    "print(\"\\nWinner Determination:\")\n",
    "acc_diff = test_acc_rbf - test_acc_linear\n",
    "if abs(acc_diff) < 0.01:\n",
    "    print(\" Tie: Both kernels perform similarly\")\n",
    "    print(\" Recommendation: Use Linear kernel (faster, simpler)\")\n",
    "elif acc_diff > 0.02:\n",
    "    print(f\"RBF wins by {acc_diff:.4f} ({acc_diff*100:.2f}%)\")\n",
    "    print(\" Recommendation: Use RBF kernel (better accuracy, worth the extra time)\")\n",
    "elif acc_diff > 0:\n",
    "    print(f\"RBF slightly better by {acc_diff:.4f} ({acc_diff*100:.2f}%)\")\n",
    "    print(\" Recommendation: Consider Linear kernel (marginal difference, much faster)\")\n",
    "elif acc_diff < -0.02:\n",
    "    print(f\"Linear wins by {abs(acc_diff):.4f} ({abs(acc_diff)*100:.2f}%)\")\n",
    "    print(\" Recommendation: Use Linear kernel (better accuracy AND faster!)\")\n",
    "else:\n",
    "    print(f\"Linear slightly better by {abs(acc_diff):.4f} ({abs(acc_diff)*100:.2f}%)\")\n",
    "    print(\" Recommendation: Use Linear kernel (better accuracy AND faster!)\")\n",
    "\n",
    "print(\"\\n Data Insights:\")\n",
    "if abs(acc_diff) < 0.02:\n",
    "    print(\" Data appears to be mostly linearly separable\")\n",
    "    print(\" Complex non-linear patterns are minimal\")\n",
    "else:\n",
    "    print(\" Data contains non-linear patterns\")\n",
    "    print(\" Kernel transformation provides benefit\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 7: Comparison with Tree-Based Models\n",
    "\n",
    "Now we compare SVM performance with Random Forest and XGBoost to determine which algorithm works best for our problem.\n",
    "\n",
    "### Why Compare Different Algorithm Types?\n",
    "\n",
    "1. **Different Strengths:** Each algorithm excels in different scenarios\n",
    "2. **No Free Lunch:** No single algorithm is best for all problems\n",
    "3. **Practical Considerations:** Training time, interpretability, deployment complexity\n",
    "4. **Model Selection:** Choose the best model for production deployment\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "**Performance:**\n",
    "- Which model has the highest test accuracy?\n",
    "- Which has the best F1-score (balanced metric)?\n",
    "- Are the differences significant (> 2%) or marginal?\n",
    "\n",
    "**Generalization:**\n",
    "- Which model has the smallest train-test gap?\n",
    "- Lower gap = better generalization\n",
    "\n",
    "**Efficiency:**\n",
    "- Training time comparison\n",
    "- Prediction speed (SVM can be slow with many support vectors)\n",
    "\n",
    "**Interpretability:**\n",
    "- Tree models provide feature importance\n",
    "- SVM is more of a \"black box\"\n",
    "\n",
    "### Expected Outcomes:\n",
    "\n",
    "**SVM might win if:**\n",
    "- Data is high-dimensional\n",
    "- Clear margin between classes\n",
    "- Features are well-scaled\n",
    "\n",
    "**Tree models might win if:**\n",
    "- Data has complex interactions\n",
    "- Features have different scales (though we scaled them)\n",
    "- Need feature importance for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Random Forest and XGBoost results\n",
    "try:\n",
    "    rf_results = pd.read_csv('../../data/processed/random_forest_results.csv')\n",
    "    xgb_results = pd.read_csv('../../data/processed/xgboost_results.csv')\n",
    "    \n",
    "    print(\"\\n Performance Comparison:\")\n",
    "    print(f\"\\n{'Model':<20} {'Train Acc':<12} {'Test Acc':<12} {'F1-Score':<12} {'Gap':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_gap = rf_results['train_accuracy'].values[0] - rf_results['test_accuracy'].values[0]\n",
    "    print(f\"{'Random Forest':<20} {rf_results['train_accuracy'].values[0]:<12.4f} \"\n",
    "          f\"{rf_results['test_accuracy'].values[0]:<12.4f} \"\n",
    "          f\"{rf_results['f1_macro'].values[0]:<12.4f} {rf_gap:<10.4f}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_gap = xgb_results['train_accuracy'].values[0] - xgb_results['test_accuracy'].values[0]\n",
    "    print(f\"{'XGBoost':<20} {xgb_results['train_accuracy'].values[0]:<12.4f} \"\n",
    "          f\"{xgb_results['test_accuracy'].values[0]:<12.4f} \"\n",
    "          f\"{xgb_results['f1_macro'].values[0]:<12.4f} {xgb_gap:<10.4f}\")\n",
    "    \n",
    "    # SVM RBF\n",
    "    rbf_gap = train_acc_rbf - test_acc_rbf\n",
    "    print(f\"{'SVM (RBF)':<20} {train_acc_rbf:<12.4f} {test_acc_rbf:<12.4f} \"\n",
    "          f\"{test_f1_rbf:<12.4f} {rbf_gap:<10.4f}\")\n",
    "    \n",
    "    # SVM Linear\n",
    "    linear_gap = train_acc_linear - test_acc_linear\n",
    "    print(f\"{'SVM (Linear)':<20} {train_acc_linear:<12.4f} {test_acc_linear:<12.4f} \"\n",
    "          f\"{test_f1_linear:<12.4f} {linear_gap:<10.4f}\")\n",
    "    \n",
    "    # Find best model\n",
    "    models = {\n",
    "        'Random Forest': rf_results['test_accuracy'].values[0],\n",
    "        'XGBoost': xgb_results['test_accuracy'].values[0],\n",
    "        'SVM (RBF)': test_acc_rbf,\n",
    "        'SVM (Linear)': test_acc_linear\n",
    "    }\n",
    "    \n",
    "    best_model = max(models, key=models.get)\n",
    "    best_acc = models[best_model]\n",
    "    \n",
    "    print(\"\\n WINNER:\")\n",
    "    print(f\"  {best_model} with {best_acc:.4f} test accuracy\")\n",
    "    \n",
    "    # Performance ranking\n",
    "    print(\"\\n Ranking by Test Accuracy:\")\n",
    "    sorted_models = sorted(models.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (model, acc) in enumerate(sorted_models, 1):\n",
    "        print(f\"  {i}. {model:<20} {acc:.4f}\")\n",
    "    \n",
    "    # Generalization ranking (smallest gap)\n",
    "    gaps = {\n",
    "        'Random Forest': rf_gap,\n",
    "        'XGBoost': xgb_gap,\n",
    "        'SVM (RBF)': rbf_gap,\n",
    "        'SVM (Linear)': linear_gap\n",
    "    }\n",
    "\n",
    "    print(\"\\n Ranking by Generalization (smallest train-test gap):\")\n",
    "    sorted_gaps = sorted(gaps.items(), key=lambda x: abs(x[1]))\n",
    "    for i, (model, gap) in enumerate(sorted_gaps, 1):\n",
    "        print(f\"  {i}. {model:<20} {abs(gap):.4f} gap\")\n",
    "    \n",
    "    # Final recommendation\n",
    "    if best_model.startswith('SVM'):\n",
    "        print(f\"Use {best_model} for deployment\")\n",
    "        print(\"Reasons:\")\n",
    "        print(\"- Highest test accuracy\")\n",
    "        print(\"- Good generalization\")\n",
    "        if best_model == 'SVM (Linear)':\n",
    "            print(\"- Fast training and prediction\")\n",
    "        print(\"Trade-offs:\")\n",
    "        print(\"- No feature importance (less interpretable)\")\n",
    "        if best_model == 'SVM (RBF)':\n",
    "            print(\"-Slower training time\")\n",
    "    else:\n",
    "        print(f\" Use {best_model} for deployment\")\n",
    "        print(\"Reasons:\")\n",
    "        print(\"- Highest test accuracy\")\n",
    "        print(\"- Provides feature importance (interpretable)\")\n",
    "        print(\"- Fast prediction speed\")\n",
    "       \n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(\"\\n Could not load tree model results.\")\n",
    "    print(\"Please run Task 2.2 (Random Forest) and Task 2.3 (XGBoost) first.\")\n",
    "    print(f\"Missing file: {e.filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 8: Save Results and Models\n",
    "\n",
    "Save all SVM results, predictions, and trained models for future use and comparison.\n",
    "\n",
    "### Files Saved:\n",
    "\n",
    "**1. Models:**\n",
    "- `svm_rbf_model.pkl` - Trained RBF SVM model\n",
    "- `svm_linear_model.pkl` - Trained Linear SVM model\n",
    "- Can be loaded later for predictions without retraining\n",
    "\n",
    "**2. Results:**\n",
    "- `svm_rbf_results.csv` - RBF SVM performance metrics\n",
    "- `svm_linear_results.csv` - Linear SVM performance metrics\n",
    "- Easy comparison with other models\n",
    "\n",
    "**3. Predictions:**\n",
    "- `svm_rbf_predictions.csv` - RBF SVM predictions on test set\n",
    "- `svm_linear_predictions.csv` - Linear SVM predictions on test set\n",
    "- Useful for error analysis and ensemble methods\n",
    "\n",
    "**4. Comparison:**\n",
    "- `svm_kernel_comparison.csv` - Direct comparison of both kernels\n",
    "- Helps decide which kernel to use\n",
    "\n",
    "### Why Save These Files:\n",
    "\n",
    "1. **Reproducibility:** Can recreate results without retraining\n",
    "2. **Model Selection:** Easy to compare all models (LR, RF, XGB, SVM)\n",
    "3. **Deployment:** Saved models ready for production use\n",
    "4. **Documentation:** Complete record for final report\n",
    "5. **Ensemble Methods:** Can combine predictions from multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RBF SVM results\n",
    "rbf_results_df = pd.DataFrame({\n",
    "    'model': ['SVM_RBF'],\n",
    "    'kernel': ['rbf'],\n",
    "    'C': [10],\n",
    "    'gamma': ['scale'],\n",
    "    'train_accuracy': [train_acc_rbf],\n",
    "    'test_accuracy': [test_acc_rbf],\n",
    "    'precision_macro': [test_precision_rbf],\n",
    "    'recall_macro': [test_recall_rbf],\n",
    "    'f1_macro': [test_f1_rbf],\n",
    "    'training_time': [rbf_training_time],\n",
    "    'n_support_vectors': [len(svm_rbf.support_)]\n",
    "})\n",
    "rbf_results_df.to_csv('../../data/processed/svm_rbf_results.csv', index=False)\n",
    "\n",
    "# Save Linear SVM results\n",
    "linear_results_df = pd.DataFrame({\n",
    "    'model': ['SVM_Linear'],\n",
    "    'kernel': ['linear'],\n",
    "    'C': [1.0],\n",
    "    'gamma': ['N/A'],\n",
    "    'train_accuracy': [train_acc_linear],\n",
    "    'test_accuracy': [test_acc_linear],\n",
    "    'precision_macro': [test_precision_linear],\n",
    "    'recall_macro': [test_recall_linear],\n",
    "    'f1_macro': [test_f1_linear],\n",
    "    'training_time': [linear_training_time],\n",
    "    'n_support_vectors': [len(svm_linear.support_)]\n",
    "})\n",
    "linear_results_df.to_csv('../../data/processed/svm_linear_results.csv', index=False)\n",
    "\n",
    "# Save kernel comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'metric': ['Test Accuracy', 'F1-Score', 'Training Time (s)', 'Support Vectors'],\n",
    "    'RBF': [test_acc_rbf, test_f1_rbf, rbf_training_time, len(svm_rbf.support_)],\n",
    "    'Linear': [test_acc_linear, test_f1_linear, linear_training_time, len(svm_linear.support_)],\n",
    "    'Difference': [\n",
    "        test_acc_rbf - test_acc_linear,\n",
    "        test_f1_rbf - test_f1_linear,\n",
    "        rbf_training_time - linear_training_time,\n",
    "        len(svm_rbf.support_) - len(svm_linear.support_)\n",
    "    ]\n",
    "})\n",
    "comparison_df.to_csv('../../data/processed/svm_kernel_comparison.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "rbf_predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred_rbf\n",
    "})\n",
    "rbf_predictions_df.to_csv('../../data/processed/svm_rbf_predictions.csv', index=False)\n",
    "\n",
    "linear_predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred_linear\n",
    "})\n",
    "linear_predictions_df.to_csv('../../data/processed/svm_linear_predictions.csv', index=False)\n",
    "\n",
    "# Save models\n",
    "with open('../../models/svm_rbf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_rbf, f)\n",
    "\n",
    "with open('../../models/svm_linear_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_linear, f)\n",
    "\n",
    "\n",
    "print(\"\\n Models:\")\n",
    "print(\"  - ../../models/svm_rbf_model.pkl\")\n",
    "print(\"  - ../../models/svm_linear_model.pkl\")\n",
    "print(\"\\n Results:\")\n",
    "print(\"  - ../../data/processed/svm_rbf_results.csv\")\n",
    "print(\"  - ../../data/processed/svm_linear_results.csv\")\n",
    "print(\"  - ../../data/processed/svm_kernel_comparison.csv\")\n",
    "print(\"\\n  Predictions:\")\n",
    "print(\"  - ../../data/processed/svm_rbf_predictions.csv\")\n",
    "print(\"  - ../../data/processed/svm_linear_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "### Summary of SVM Implementation:\n",
    "\n",
    "We successfully implemented and compared two SVM variants:\n",
    "1. **SVM with RBF Kernel** - For non-linear patterns\n",
    "2. **SVM with Linear Kernel** - For linearly separable data\n",
    "\n",
    "### Key Learnings:\n",
    "\n",
    "**1. Feature Scaling is Critical:**\n",
    "- SVM is distance-based, so feature scaling is mandatory\n",
    "- Tree-based models don't need scaling\n",
    "- Always use StandardScaler or MinMaxScaler for SVM\n",
    "\n",
    "**2. Kernel Selection Matters:**\n",
    "- RBF kernel: More flexible, handles non-linear patterns\n",
    "- Linear kernel: Faster, simpler, works for linear data\n",
    "- Compare both to find the best fit\n",
    "\n",
    "**3. Hyperparameter Tuning:**\n",
    "- **C parameter:** Controls regularization (margin vs accuracy)\n",
    "- **gamma parameter:** Controls kernel influence (RBF only)\n",
    "- Start with defaults, then tune if needed\n",
    "\n",
    "**4. SVM vs Tree Models:**\n",
    "- SVM: Better for high-dimensional data, smooth boundaries\n",
    "- Trees: Better for feature interactions, provide interpretability\n",
    "- No guaranteed winner - depends on the data\n",
    "\n",
    "### Hyperparameter Tuning Tips:\n",
    "\n",
    "If you want to improve SVM performance further:\n",
    "\n",
    "**For RBF Kernel:**\n",
    "- Try C values: [0.1, 1, 10, 100]\n",
    "- Try gamma values: ['scale', 'auto', 0.001, 0.01, 0.1]\n",
    "- Use GridSearchCV for systematic tuning\n",
    "\n",
    "**For Linear Kernel:**\n",
    "- Try C values: [0.01, 0.1, 1, 10, 100]\n",
    "- Linear is less sensitive to hyperparameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
