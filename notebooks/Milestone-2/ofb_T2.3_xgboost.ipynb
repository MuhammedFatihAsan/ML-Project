{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.3: Advanced Model - XGBoost Classifier Implementation\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to implement an XGBoost (Extreme Gradient Boosting) Classifier with hyperparameter tuning. XGBoost is a powerful gradient boosting algorithm that often outperforms Random Forest by sequentially building trees that correct errors from previous trees. We will compare its performance with both the baseline Logistic Regression and Random Forest models.\n",
    "\n",
    "## Understanding Key Metrics\n",
    "\n",
    "Since this is a supervised classification task, we use specific metrics to evaluate how well our model performs:\n",
    "\n",
    "1. **Accuracy:** - Measures the overall correctness of predictions. Range is 0 to 1, where 1 indicates perfect classification.\n",
    "2. **Precision (Macro):** - Average precision across all classes, measuring the proportion of correct positive predictions.\n",
    "3. **Recall (Macro):** - Average recall across all classes, measuring the proportion of actual positives correctly identified.\n",
    "4. **F1-Score (Macro):** - Harmonic mean of precision and recall, providing a balanced measure of model performance. Range is 0 to 1.\n",
    "\n",
    "## Why XGBoost?\n",
    "\n",
    "XGBoost offers several advantages over Random Forest and other algorithms:\n",
    "\n",
    "1. **Sequential Learning:** Unlike Random Forest (parallel trees), XGBoost builds trees sequentially, where each new tree corrects errors from previous ones.\n",
    "2. **Gradient Boosting:** Uses gradient descent to minimize loss, leading to more accurate predictions.\n",
    "3. **Regularization:** Built-in L1 and L2 regularization prevents overfitting better than Random Forest.\n",
    "4. **Handling Missing Values:** Automatically learns the best direction for missing values.\n",
    "5. **Speed and Performance:** Optimized implementation with parallel processing and tree pruning.\n",
    "6. **Feature Importance:** Provides multiple ways to measure feature importance (gain, cover, frequency).\n",
    "\n",
    "## XGBoost vs Random Forest:\n",
    "\n",
    "| Aspect | Random Forest | XGBoost |\n",
    "|--------|--------------|----------|\n",
    "| Tree Building | Parallel (independent) | Sequential (corrective) |\n",
    "| Learning Method | Bagging (averaging) | Boosting (error correction) |\n",
    "| Overfitting Risk | Lower | Higher (needs tuning) |\n",
    "| Training Speed | Faster | Slower |\n",
    "| Prediction Accuracy | Good | Often Better |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Discovery\n",
    "\n",
    "In this step, we import the required libraries and load the preprocessed dataset from the project directory.\n",
    "\n",
    "### Libraries Used:\n",
    "- **pandas & numpy:** For data manipulation and numerical operations\n",
    "- **XGBClassifier:** The main algorithm from xgboost library\n",
    "- **sklearn.metrics:** For evaluating model performance\n",
    "- **LabelEncoder:** To convert categorical target labels to numeric format\n",
    "- **pickle:** For saving the trained model\n",
    "\n",
    "### Data Loading:\n",
    "We load the scaled training and testing sets that were prepared in previous tasks. XGBoost can work with unscaled data, but using scaled features ensures consistency with previous models and can improve convergence speed.\n",
    "\n",
    "### Important Note:\n",
    "XGBoost requires target labels to be in the range [0, num_classes-1]. Our LabelEncoder ensures this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train['value_category'])\n",
    "y_test_encoded = label_encoder.transform(y_test['value_category'])\n",
    "\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    category = label_encoder.classes_[val]\n",
    "    print(f\"  Class {val} ({category}): {count} samples ({count/len(y_train_encoded)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Removing ID Columns from Features\n",
    "\n",
    "**Why we remove ID columns:**\n",
    "\n",
    "1. **IDs are unique identifiers, not predictive features** - They don't contain information about value categories\n",
    "2. **Including IDs would cause overfitting** - The model would memorize specific listings instead of learning patterns\n",
    "3. **IDs have no relationship with value categories** - A listing's ID number doesn't determine its value\n",
    "\n",
    "This is a critical preprocessing step to ensure our model learns meaningful patterns rather than memorizing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Model Training with Hyperparameter Tuning\n",
    "\n",
    "We train an XGBoost Classifier with carefully tuned hyperparameters. These parameters were selected to balance model performance and prevent overfitting.\n",
    "\n",
    "### Hyperparameter Explanation:\n",
    "\n",
    "1. **learning_rate=0.1** (also called eta)\n",
    "   - Controls how much each tree contributes to the final prediction\n",
    "   - Lower values (0.01-0.1) make the model more robust but require more trees\n",
    "   - 0.1 is a good balance between training speed and accuracy\n",
    "   - Think of it as \"step size\" - smaller steps are more careful but slower\n",
    "\n",
    "2. **max_depth=6**\n",
    "   - Maximum depth of each tree\n",
    "   - Deeper trees can capture more complex patterns but risk overfitting\n",
    "   - 6 is a common default that works well for most problems\n",
    "   - Shallower than Random Forest (20) because boosting is more powerful\n",
    "\n",
    "3. **n_estimators=200**\n",
    "   - Number of boosting rounds (trees to build)\n",
    "   - More trees generally improve performance but increase training time\n",
    "   - 200 is higher than Random Forest (100) because each tree is simpler\n",
    "   - With learning_rate=0.1, 200 trees provides good convergence\n",
    "\n",
    "4. **subsample=0.8**\n",
    "   - Fraction of training samples used for each tree (80%)\n",
    "   - Randomly samples 80% of data for each tree, preventing overfitting\n",
    "   - Similar to Random Forest's bootstrap sampling\n",
    "   - Values between 0.5-0.9 work well; 0.8 is a sweet spot\n",
    "\n",
    "5. **colsample_bytree=0.8**\n",
    "   - Fraction of features used for each tree (80%)\n",
    "   - Randomly samples 80% of features, adding diversity to trees\n",
    "   - Helps prevent overfitting and speeds up training\n",
    "   - Similar to Random Forest's feature sampling\n",
    "\n",
    "6. **objective='multi:softmax'**\n",
    "   - Loss function for multi-class classification\n",
    "   - Returns class labels directly (0, 1, 2)\n",
    "   - Alternative: 'multi:softprob' returns probabilities\n",
    "\n",
    "7. **eval_metric='mlogloss'**\n",
    "   - Evaluation metric: multi-class log loss\n",
    "   - Measures how well predicted probabilities match true labels\n",
    "   - Lower values indicate better performance\n",
    "\n",
    "8. **random_state=42**\n",
    "   - Ensures reproducibility across runs\n",
    "\n",
    "9. **n_jobs=-1**\n",
    "   - Uses all available CPU cores for parallel processing\n",
    "\n",
    "### How XGBoost Works:\n",
    "\n",
    "1. **Start:** Build a simple tree to predict the target\n",
    "2. **Calculate Errors:** Find where the first tree made mistakes\n",
    "3. **Build Next Tree:** Focus on correcting those errors\n",
    "4. **Repeat:** Each new tree corrects errors from previous trees\n",
    "5. **Final Prediction:** Weighted sum of all tree predictions\n",
    "\n",
    "This sequential error-correction approach often leads to better accuracy than Random Forest's parallel approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with hyperparameter tuning\n",
    "xgb_model = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softmax',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation\n",
    "\n",
    "Evaluating model performance on both training and testing sets.\n",
    "\n",
    "### Understanding the Metrics:\n",
    "\n",
    "**Training vs Testing Accuracy:**\n",
    "- **Training Accuracy:** How well the model performs on data it has seen during training\n",
    "- **Testing Accuracy:** How well the model generalizes to new, unseen data\n",
    "- **Gap between them:** A large gap indicates overfitting (model memorized training data)\n",
    "- **XGBoost Note:** Due to boosting, training accuracy may be very high; focus on test accuracy\n",
    "\n",
    "**Macro-Averaged Metrics:**\n",
    "- Calculate metric for each class separately, then average them\n",
    "- Treats all classes equally, regardless of their size\n",
    "- Important for imbalanced datasets to ensure all classes are considered\n",
    "\n",
    "**What to Look For:**\n",
    "- Testing accuracy should be higher than Random Forest and Logistic Regression\n",
    "- Training accuracy may be higher than Random Forest (boosting is more powerful)\n",
    "- F1-Score provides the best overall measure of model quality\n",
    "- Compare with previous models to see if XGBoost provides improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
    "test_acc = accuracy_score(y_test_encoded, y_test_pred)\n",
    "test_precision = precision_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test_encoded, y_test_pred, average='macro')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision (Macro): {test_precision:.4f}\")\n",
    "print(f\"  Recall (Macro): {test_recall:.4f}\")\n",
    "print(f\"  F1-Score (Macro): {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Detailed Classification Report\n",
    "\n",
    "The classification report provides per-class performance metrics, helping us understand which value categories the model predicts well and which ones it struggles with.\n",
    "\n",
    "### Reading the Report:\n",
    "\n",
    "**For Each Class:**\n",
    "- **Precision:** Of all listings predicted as this class, what percentage were correct?\n",
    "- **Recall:** Of all actual listings in this class, what percentage did we correctly identify?\n",
    "- **F1-Score:** Harmonic mean of precision and recall for this class\n",
    "- **Support:** Number of actual samples in this class\n",
    "\n",
    "**Overall Metrics:**\n",
    "- **Accuracy:** Overall correctness across all classes\n",
    "- **Macro avg:** Simple average of metrics across all classes (treats each class equally)\n",
    "- **Weighted avg:** Average weighted by the number of samples in each class\n",
    "\n",
    "### What to Look For:\n",
    "- Compare with Random Forest: Is XGBoost better at predicting all classes or just some?\n",
    "- Are all three classes performing similarly, or is one class much harder to predict?\n",
    "- Low recall for a class means we're missing many actual instances of that class\n",
    "- Low precision for a class means we're incorrectly labeling other classes as this one\n",
    "\n",
    "### Expected Improvement:\n",
    "XGBoost should show more balanced performance across all three value categories compared to previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_encoded, y_test_pred, \n",
    "                    target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Comparison with Random Forest\n",
    "\n",
    "Let's load the Random Forest results and compare them with XGBoost to see which model performs better.\n",
    "\n",
    "### Why Compare?\n",
    "\n",
    "1. **Validate Improvement:** Confirm that XGBoost's complexity is justified by better performance\n",
    "2. **Understand Trade-offs:** XGBoost may be slower to train but more accurate\n",
    "3. **Model Selection:** Choose the best model for deployment based on performance and requirements\n",
    "4. **Learning Insights:** Understand which algorithm works better for this specific problem\n",
    "\n",
    "### Key Comparison Points:\n",
    "\n",
    "- **Test Accuracy:** Which model generalizes better to unseen data?\n",
    "- **F1-Score:** Which model has better overall balance of precision and recall?\n",
    "- **Training Time:** Is the performance gain worth the extra training time?\n",
    "- **Overfitting:** Which model has a smaller gap between training and testing accuracy?\n",
    "\n",
    "### Expected Outcome:\n",
    "\n",
    "XGBoost typically outperforms Random Forest on structured/tabular data like our Airbnb dataset, but the improvement may be modest (1-5% accuracy gain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Random Forest results for comparison\n",
    "try:\n",
    "    rf_results = pd.read_csv('../../data/processed/random_forest_results.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON: XGBoost vs Random Forest\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nRandom Forest Performance:\")\n",
    "    print(f\"  Training Accuracy: {rf_results['train_accuracy'].values[0]:.4f}\")\n",
    "    print(f\"  Testing Accuracy:  {rf_results['test_accuracy'].values[0]:.4f}\")\n",
    "    print(f\"  Precision (Macro): {rf_results['precision_macro'].values[0]:.4f}\")\n",
    "    print(f\"  Recall (Macro):    {rf_results['recall_macro'].values[0]:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):  {rf_results['f1_macro'].values[0]:.4f}\")\n",
    "    \n",
    "    print(\"\\nXGBoost Performance:\")\n",
    "    print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Testing Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Precision (Macro): {test_precision:.4f}\")\n",
    "    print(f\"  Recall (Macro):    {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):  {test_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nImprovement (XGBoost - Random Forest):\")\n",
    "    print(f\"  Testing Accuracy:  {(test_acc - rf_results['test_accuracy'].values[0]):.4f} ({((test_acc - rf_results['test_accuracy'].values[0])/rf_results['test_accuracy'].values[0]*100):.2f}%)\")\n",
    "    print(f\"  F1-Score (Macro):  {(test_f1 - rf_results['f1_macro'].values[0]):.4f} ({((test_f1 - rf_results['f1_macro'].values[0])/rf_results['f1_macro'].values[0]*100):.2f}%)\")\n",
    "    \n",
    "    if test_acc > rf_results['test_accuracy'].values[0]:\n",
    "        print(\"\\n XGBoost outperforms Random Forest!\")\n",
    "    elif test_acc == rf_results['test_accuracy'].values[0]:\n",
    "        print(\"\\n XGBoost and Random Forest perform equally.\")\n",
    "    else:\n",
    "        print(\"\\n Random Forest performs better than XGBoost.\")\n",
    "        print(\"   Consider: 1) Different hyperparameters, 2) More training data, 3) Feature engineering\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n  Random Forest results not found. Please run Task 2.2 first.\")\n",
    "    print(\"   Comparison will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance Analysis\n",
    "\n",
    "XGBoost provides feature importance scores that indicate which features contribute most to the model's predictions.\n",
    "\n",
    "### How XGBoost Feature Importance Works:\n",
    "\n",
    "XGBoost offers three types of feature importance:\n",
    "\n",
    "1. **Gain (default):** Average gain of splits using this feature\n",
    "   - Measures the improvement in accuracy brought by a feature\n",
    "   - Higher gain = more important for making correct predictions\n",
    "\n",
    "2. **Cover:** Average coverage of splits using this feature\n",
    "   - Number of samples affected by splits on this feature\n",
    "   - Shows how broadly a feature is used\n",
    "\n",
    "3. **Frequency (Weight):** Number of times a feature is used in splits\n",
    "   - How often the feature appears in trees\n",
    "   - High frequency doesn't always mean high importance\n",
    "\n",
    "We use **Gain** as it best represents true predictive importance.\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Model Interpretability:** Understand what drives the model's decisions\n",
    "- **Feature Selection:** Identify which features could potentially be removed\n",
    "- **Business Insights:** Learn which listing characteristics most affect value perception\n",
    "- **Validation:** Ensure the model is using sensible features (not just noise)\n",
    "- **Comparison with Random Forest:** See if both models agree on important features\n",
    "\n",
    "### Expected Important Features:\n",
    "\n",
    "We expect features like review scores, price-related features, location, and amenities to be highly important, as these directly relate to a listing's value proposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances (using 'gain' as importance type)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (XGBoost):\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('../../data/processed/xgboost_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to: ../../data/processed/xgboost_feature_importance.csv\")\n",
    "\n",
    "# Compare with Random Forest feature importance if available\n",
    "try:\n",
    "    rf_importance = pd.read_csv('../../data/processed/random_forest_feature_importance.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTop 5 Features - Random Forest:\")\n",
    "    print(rf_importance.head(5)['feature'].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop 5 Features - XGBoost:\")\n",
    "    print(feature_importance.head(5)['feature'].to_string(index=False))\n",
    "    \n",
    "    # Find common important features\n",
    "    rf_top10 = set(rf_importance.head(10)['feature'])\n",
    "    xgb_top10 = set(feature_importance.head(10)['feature'])\n",
    "    common_features = rf_top10.intersection(xgb_top10)\n",
    "    \n",
    "    print(f\"\\nCommon features in both models' top 10: {len(common_features)}\")\n",
    "    if common_features:\n",
    "        print(\"Features:\", ', '.join(common_features))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"\\n Random Forest feature importance not found. Comparison skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 7: Saving Results\n",
    "\n",
    "We save all important outputs for future reference and comparison with other models.\n",
    "\n",
    "### Files Saved:\n",
    "\n",
    "1. **xgboost_model.pkl** - The trained model object\n",
    "   - Can be loaded later for making predictions without retraining\n",
    "   - Preserves all learned parameters and tree structures\n",
    "   - Includes all hyperparameter settings\n",
    "\n",
    "2. **xgboost_results.csv** - Summary of model performance metrics\n",
    "   - Allows easy comparison with other models (Logistic Regression, Random Forest)\n",
    "   - Contains all key metrics in one place\n",
    "   - Essential for final model selection\n",
    "\n",
    "3. **xgboost_predictions.csv** - Actual vs predicted values\n",
    "   - Useful for error analysis and understanding misclassifications\n",
    "   - Can be used to create confusion matrices\n",
    "   - Helps identify which listings are hardest to classify\n",
    "\n",
    "4. **xgboost_feature_importance.csv** - Feature importance rankings\n",
    "   - Documents which features the model relies on most\n",
    "   - Useful for feature selection and model interpretation\n",
    "   - Can be compared with Random Forest importance\n",
    "\n",
    "### Why Save These Files:\n",
    "\n",
    "- **Reproducibility:** Can recreate results without rerunning the entire notebook\n",
    "- **Comparison:** Easy to compare XGBoost with Random Forest and Logistic Regression\n",
    "- **Documentation:** Provides a record of model performance for reports\n",
    "- **Deployment:** The saved model can be used in production applications\n",
    "- **Model Selection:** Having all results saved makes final model selection easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'model': ['XGBoost'],\n",
    "    'train_accuracy': [train_acc],\n",
    "    'test_accuracy': [test_acc],\n",
    "    'precision_macro': [test_precision],\n",
    "    'recall_macro': [test_recall],\n",
    "    'f1_macro': [test_f1]\n",
    "})\n",
    "results_df.to_csv('../../data/processed/xgboost_results.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred\n",
    "})\n",
    "predictions_df.to_csv('../../data/processed/xgboost_predictions.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "with open('../../models/xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(\"\\nFiles saved successfully:\")\n",
    "print(\"  - ../../models/xgboost_model.pkl\")\n",
    "print(\"  - ../../data/processed/xgboost_results.csv\")\n",
    "print(\"  - ../../data/processed/xgboost_predictions.csv\")\n",
    "print(\"  - ../../data/processed/xgboost_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Expected Improvements Over Random Forest:\n",
    "\n",
    "XGBoost should outperform Random Forest because:\n",
    "1. **Sequential Error Correction:** Each tree learns from previous mistakes, leading to better accuracy\n",
    "2. **Gradient Optimization:** Uses gradient descent to minimize loss more effectively\n",
    "3. **Built-in Regularization:** Better prevents overfitting through L1/L2 regularization\n",
    "4. **Optimized Splits:** More sophisticated tree-building algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
