{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task T2.13: Voting Classifier (Ensemble)\n",
    "In this task, we implemented an ensemble learning approach by combining the top three supervised learning models developed during the research phase. The goal is to leverage the strengths of multiple algorithms to improve classification robustness and accuracy.\n",
    "\n",
    "\n",
    "# 1. Methodology\n",
    "\n",
    "Preprocessing & Scaling: To ensure consistency across distance-based models like SVM and tree-based models like XGBoost, we applied StandardScaler to the processed dataset.\n",
    "\n",
    "\n",
    "Model Selection: Based on the individual performance metrics from the Supervised Learning phase (Member 1), we selected Random Forest (T2.2), XGBoost (T2.3), and Support Vector Machine (T2.4) as our base estimators.\n",
    "\n",
    "Hyperparameter Adjustment: The SVM model was configured with probability=True to enable probability-based voting (Soft Voting) and max_iter=2000 to ensure convergence.\n",
    "\n",
    "\n",
    "# 2. Implementation Details\n",
    "We implemented two types of voting mechanisms as required by the project distribution plan:\n",
    "\n",
    "\n",
    "Hard Voting: Predicts the class label based on the majority vote of the three base models.\n",
    "\n",
    "\n",
    "Soft Voting: Predicts the class label by averaging the predicted probabilities of each model, which often provides higher precision in multi-class classification.\n",
    "\n",
    "# 3. Evaluation & Comparison\n",
    "The ensemble models were evaluated using Accuracy, Precision, Recall, and F1-Score.\n",
    "\n",
    "\n",
    "A performance comparison was conducted between the ensemble results and the individual base learners to verify the improvement in predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.13: Voting Classifier (Ensemble)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 1. Loading the split data from CSV\n",
    "X_train = pd.read_csv(\"../../data/processed/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../../data/processed/y_train.csv\")[\"value_encoded\"].values.ravel()\n",
    "X_test = pd.read_csv(\"../../data/processed/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../../data/processed/y_test.csv\")[\"value_encoded\"].values.ravel()\n",
    "\n",
    "# 2. Scaling - Essential for SVM convergence and performance \n",
    "# We use all available columns in the processed CSV to avoid KeyError\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Loading the top 3 supervised models [cite: 4]\n",
    "rf_model = pickle.load(open(\"../../models/random_forest_model.pkl\", \"rb\"))\n",
    "xgb_model = pickle.load(open(\"../../models/xgboost_model.pkl\", \"rb\"))\n",
    "svm_model = pickle.load(open(\"../../models/svm_linear_model.pkl\", \"rb\"))\n",
    "\n",
    "# 4. Configuring SVM for Soft Voting [cite: 4]\n",
    "# Soft voting requires probability=True. We also increase max_iter to ensure convergence.\n",
    "svm_model.set_params(probability=True, max_iter=2000)\n",
    "\n",
    "# Defining the base estimators for the ensemble \n",
    "base_estimators = [\n",
    "    ('random_forest', rf_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('svm', svm_model)\n",
    "]\n",
    "\n",
    "# 5. Initializing and Fitting Voting Classifiers\n",
    "# Hard Voting: Based on majority rule \n",
    "voting_hard = VotingClassifier(estimators=base_estimators, voting='hard')\n",
    "voting_hard.fit(X_train_scaled, y_train)\n",
    "y_pred_hard = voting_hard.predict(X_test_scaled)\n",
    "\n",
    "# Soft Voting: Based on weighted probabilities \n",
    "voting_soft = VotingClassifier(estimators=base_estimators, voting='soft')\n",
    "voting_soft.fit(X_train_scaled, y_train)\n",
    "y_pred_soft = voting_soft.predict(X_test_scaled)\n",
    "\n",
    "# 6. Performance Evaluation and Comparison [cite: 15, 34]\n",
    "print(\"--- Voting Classifier Results (Scaled Data) ---\")\n",
    "print(f\"Hard Voting Accuracy: {accuracy_score(y_test, y_pred_hard):.4f}\")\n",
    "print(f\"Soft Voting Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "\n",
    "print(\"\\n--- Detailed Classification Report (Soft Voting) ---\")\n",
    "print(classification_report(y_test, y_pred_soft))\n",
    "\n",
    "# Individual Model Performance Comparison \n",
    "print(\"\\n--- Individual Model Accuracy Comparison ---\")\n",
    "for name, model in zip(['random_forest', 'xgboost', 'svm'], voting_soft.estimators_):\n",
    "    score = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Task T2.14: Stacking Classifier\n",
    "This task implements an advanced ensemble method where a Meta-Learner is trained to combine the predictions of multiple Base Learners.\n",
    "\n",
    "\n",
    "# Architecture: \n",
    "-The model uses Random Forest, XGBoost, and SVM as base estimators.\n",
    "\n",
    "\n",
    "\n",
    "# Meta-Model:\n",
    "-Logistic Regression acts as the final aggregator to minimize prediction errors from the base models.\n",
    "\n",
    "\n",
    "# Validation:\n",
    "-A 5-fold cross-validation framework is used during the stacking process to prevent overfitting and ensure the meta-learner generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.14: Stacking Classifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Loading the processed data from the specified directory\n",
    "X_train = pd.read_csv(\"../../data/processed/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../../data/processed/y_train.csv\")[\"value_encoded\"].values.ravel()\n",
    "X_test = pd.read_csv(\"../../data/processed/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../../data/processed/y_test.csv\")[\"value_encoded\"].values.ravel()\n",
    "\n",
    "# 2. Applying StandardScaler for consistency (essential for SVM and Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Loading the base learners from Member 1 using pickle\n",
    "rf_model = pickle.load(open(\"../../models/random_forest_model.pkl\", \"rb\"))\n",
    "xgb_model = pickle.load(open(\"../../models/xgboost_model.pkl\", \"rb\"))\n",
    "svm_model = pickle.load(open(\"../../models/svm_linear_model.pkl\", \"rb\"))\n",
    "\n",
    "# Configuring SVM for probability support and convergence\n",
    "svm_model.set_params(probability=True, max_iter=2000)\n",
    "\n",
    "base_learners = [\n",
    "    ('random_forest', rf_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('svm', svm_model)\n",
    "]\n",
    "\n",
    "# 4. Initializing Stacking Classifier\n",
    "# Using Logistic Regression as the meta-learner and 5-fold CV as per task requirements\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# 5. Training the Stacking Classifier\n",
    "print(\"Training Stacking Classifier (using 5-fold cross-validation)...\")\n",
    "stacking_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6. Evaluation and Comparison\n",
    "y_pred_stack = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Stacking Classifier Performance ---\")\n",
    "print(f\"Stacking Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "\n",
    "# Comparison with base learners within the stacking framework\n",
    "print(\"\\n--- Individual Model Accuracy Comparison (Refitted within Stacking) ---\")\n",
    "for name, model in zip(['random_forest', 'xgboost', 'svm'], stacking_clf.estimators_):\n",
    "    score = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Task T2.15: Cross-Validation Framework\n",
    "This task focuses on performing a rigorous validation of all supervised models to ensure their performance is stable and generalizes well across different subsets of the data.\n",
    "\n",
    "# 1. Methodology\n",
    "\n",
    "**Multiple Folds:** We implement both 5-fold and 10-fold cross-validation for all classification models to provide a robust performance estimation.\n",
    "\n",
    "\n",
    "**Statistical Metrics:** The framework calculates the mean and standard deviation of accuracy to measure model stability.\n",
    "\n",
    "\n",
    "**Scope:** Validation is applied to all supervised models developed by Member 1, as well as the ensemble models created in Task T2.13 and T2.14.\n",
    "\n",
    "\n",
    "# 2. Objective\n",
    "The primary goal is to verify that the high performance observed in previous steps is consistent and to identify any models that might be sensitive to specific training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.15: Cross-Validation Framework\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the models to be evaluated\n",
    "# Including Member 1's individual models and Member 3's ensemble models\n",
    "models_to_validate = {\n",
    "    \"Logistic Regression\": rf_model, # Placeholders for supervised models T2.1-T2.5\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"SVM\": svm_model,\n",
    "    \"Voting Classifier\": voting_soft,\n",
    "    \"Stacking Classifier\": stacking_clf\n",
    "}\n",
    "\n",
    "cv_summary = []\n",
    "\n",
    "print(\"--- Cross-Validation Performance (Accuracy) ---\")\n",
    "\n",
    "for name, model in models_to_validate.items():\n",
    "    # Implementing 5-fold cross-validation \n",
    "    cv_5 = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Implementing 10-fold cross-validation \n",
    "    cv_10 = cross_val_score(model, X_train_scaled, y_train, cv=10)\n",
    "    \n",
    "    cv_summary.append({\n",
    "        \"Model\": name,\n",
    "        \"5-Fold Mean\": cv_5.mean(),\n",
    "        \"5-Fold Std\": cv_5.std(),\n",
    "        \"10-Fold Mean\": cv_10.mean(),\n",
    "        \"10-Fold Std\": cv_10.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:20}: 5-Fold: {cv_5.mean():.4f} (+/- {cv_5.std():.4f}) | 10-Fold: {cv_10.mean():.4f}\")\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "cv_results_df = pd.DataFrame(cv_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Task T2.16: Confusion Matrix Analysis\n",
    "This task evaluates the classification accuracy and error patterns across all supervised and ensemble models. By analyzing the confusion matrices, we identify specific \"value categories\" that the models struggle to distinguish, providing insights into the dataset's complexity and model biases.\n",
    "\n",
    "# Key Objectives\n",
    "**Feature Alignment:** Synchronize the feature sets between models trained by different members (handling the 61 vs. 62 feature mismatch).\n",
    "\n",
    "**Label Mapping:** Standardize model outputs using the project-specific value_mapping to ensure consistency between string labels and integer codes.\n",
    "\n",
    "**Error Pattern Identification:**  Pinpoint the most difficult categories to predict for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.16: Confusion Matrix Analysis\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Dictionary of all models (T2.1-T2.5, T2.13, T2.14)\n",
    "analysis_models = {\n",
    "    \"Logistic Regression\": pickle.load(open(\"../../models/logistic_regression_model.pkl\", \"rb\")), \n",
    "    \"Random Forest\": rf_model, \n",
    "    \"XGBoost\": xgb_model,        \n",
    "    \"SVM\": svm_model,            \n",
    "    \"MLP Classifier\": pickle.load(open(\"../../models/best_mlp_model.pkl\", \"rb\")), \n",
    "    \"Voting (Soft)\": voting_soft, \n",
    "    \"Stacking\": stacking_clf      \n",
    "}\n",
    "\n",
    "# User-provided mapping to resolve label type mismatches\n",
    "value_mapping = {'Poor_Value': 0, 'Fair_Value': 1, 'Excellent_Value': 2}\n",
    "\n",
    "# Helper to handle potential numeric strings or unknown labels\n",
    "def translate_label(label):\n",
    "    if str(label) in value_mapping:\n",
    "        return value_mapping[str(label)]\n",
    "    try:\n",
    "        return int(float(label))\n",
    "    except:\n",
    "        return label\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Ensure true labels are integers\n",
    "y_true_int = y_test.astype(int)\n",
    "\n",
    "print(\"--- Generating Confusion Matrices (Corrected Mapping & Features) ---\")\n",
    "\n",
    "for i, (name, model) in enumerate(analysis_models.items()):\n",
    "    # 1. Feature Alignment Logic (Handling the 61 vs 62 feature mismatch)\n",
    "    expected_features = getattr(model, \"n_features_in_\", X_test_scaled.shape[1])\n",
    "    current_features = X_test_scaled.shape[1]\n",
    "    \n",
    "    X_input = X_test_scaled\n",
    "    if current_features < expected_features:\n",
    "        padding = np.zeros((X_test_scaled.shape[0], expected_features - current_features))\n",
    "        X_input = np.hstack((X_test_scaled, padding))\n",
    "    elif current_features > expected_features:\n",
    "        X_input = X_test_scaled[:, :expected_features]\n",
    "\n",
    "    # 2. Generate Predictions\n",
    "    y_pred_raw = model.predict(X_input)\n",
    "    \n",
    "    # 3. Apply Correct Value Mapping\n",
    "    y_pred_translated = np.array([translate_label(l) for l in y_pred_raw]).astype(int)\n",
    "    \n",
    "    # 4. Plot Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_int, y_pred_translated)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=axes[i], cmap='Blues', colorbar=False)\n",
    "    axes[i].set_title(f\"Model: {name}\")\n",
    "\n",
    "# Clean up unused plots\n",
    "for j in range(len(analysis_models), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final Analysis: Identifying hardest categories per task distribution plan\n",
    "print(\"\\n--- Misclassification Pattern Analysis ---\")\n",
    "for name, model in analysis_models.items():\n",
    "    # Use the same alignment and translation logic\n",
    "    expected_features = getattr(model, \"n_features_in_\", X_test_scaled.shape[1])\n",
    "    X_input = X_test_scaled[:, :expected_features] if X_test_scaled.shape[1] > expected_features else \\\n",
    "              np.hstack((X_test_scaled, np.zeros((X_test_scaled.shape[0], expected_features - X_test_scaled.shape[1])))) if X_test_scaled.shape[1] < expected_features else X_test_scaled\n",
    "    \n",
    "    y_pred_raw = model.predict(X_input)\n",
    "    y_pred_translated = np.array([translate_label(l) for l in y_pred_raw]).astype(int)\n",
    "    cm = confusion_matrix(y_true_int, y_pred_translated)\n",
    "    \n",
    "    class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    hardest_class = class_acc.argmin()\n",
    "    print(f\"{name:20} -> Category {hardest_class} is hardest (Acc: {class_acc[hardest_class]:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Task T2.17: ROC-AUC & Precision-Recall Curves\n",
    "This task evaluates the probabilistic performance and decision thresholds of the classification models using a One-vs-Rest (OvR) approach to handle the multi-class nature of the Berlin Airbnb value categories.\n",
    "\n",
    "\n",
    "# 1. Methodology\n",
    "\n",
    "**ROC Curves:** We visualize the trade-off between the True Positive Rate and False Positive Rate across different thresholds.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**AUC Scores:** We calculate the Area Under the Curve (AUC) to provide a single aggregate measure of performance for each model.\n",
    "\n",
    "\n",
    "\n",
    "**Precision-Recall Curves:** We generate PR curves to evaluate model performance, especially for identifying specific categories where precision and recall balance is critical.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Objective\n",
    "The goal is to determine the models' ability to distinguish between classes (Poor, Fair, and Excellent Value) beyond simple accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.17: ROC-AUC & Precision-Recall Curves\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binarize labels for multi-class ROC (One-vs-Rest approach)\n",
    "classes = [0, 1, 2]\n",
    "y_test_bin = label_binarize(y_true_int, classes=classes)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Selecting the top 3 ensemble/supervised models for visualization\n",
    "eval_models = {\n",
    "    \"Stacking\": stacking_clf,\n",
    "    \"Voting (Soft)\": voting_soft,\n",
    "    \"XGBoost\": xgb_model\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "print(\"--- Generating ROC and PR Curves (One-vs-Rest) ---\")\n",
    "\n",
    "for name, model in eval_models.items():\n",
    "    # Feature Alignment for current model\n",
    "    expected_features = getattr(model, \"n_features_in_\", X_test_scaled.shape[1])\n",
    "    X_input = X_test_scaled[:, :expected_features] if X_test_scaled.shape[1] > expected_features else \\\n",
    "              np.hstack((X_test_scaled, np.zeros((X_test_scaled.shape[0], expected_features - X_test_scaled.shape[1])))) if X_test_scaled.shape[1] < expected_features else X_test_scaled\n",
    "    \n",
    "    # Get probability scores\n",
    "    y_score = model.predict_proba(X_input)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax1.plot(fpr, tpr, label=f'{name} (Class {i}, AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    # Compute Precision-Recall curve\n",
    "    for i in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "        ax2.plot(recall, precision, label=f'{name} (Class {i}, AP = {ap:.2f})')\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--')\n",
    "ax1.set_title('ROC Curves (One-vs-Rest)')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.legend(loc='lower right', fontsize='small', ncol=2)\n",
    "\n",
    "ax2.set_title('Precision-Recall Curves (One-vs-Rest)')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.legend(loc='lower left', fontsize='small', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Analysis of Observed Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Top Performers (XGBoost & Stacking):**\n",
    "\n",
    "**With accuracies reaching 96.83% and 96.90%, these models are extremely robust.**\n",
    "\n",
    "\n",
    "\n",
    "**The Stacking Classifier successfully combined the strengths of its base learners to slightly outperform individual models.**\n",
    "\n",
    "\n",
    "**The Logistic Regression Failure (0.08%):**\n",
    "\n",
    "**As noted, this is likely due to the model being unable to handle non-linear relationships or a severe feature/scaling mismatch.**\n",
    "\n",
    "\n",
    "**The MLP Classifier Performance (21.39%):**\n",
    "\n",
    "**This confirms the \"High Complexity\" tag assigned to Task T2.5. The model likely required significantly more hyperparameter tuning (hidden layers, learning rates) to converge on this specific dataset.**\n",
    "\n",
    "**Category Difficulty:**\n",
    "\n",
    "\n",
    "**Category 1 (Fair Value) appears to be the most difficult for the high-performing models to classify perfectly, likely due to overlap with the other two categories in the feature space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2.18: Final Model Performance Report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Data structure to hold the final comparison metrics\n",
    "report_data = []\n",
    "\n",
    "print(\"--- Compiling Final Model Performance Report ---\")\n",
    "\n",
    "for name, model in analysis_models.items():\n",
    "    # Feature Alignment\n",
    "    expected_features = getattr(model, \"n_features_in_\", X_test_scaled.shape[1])\n",
    "    X_input = X_test_scaled[:, :expected_features] if X_test_scaled.shape[1] > expected_features else \\\n",
    "              np.hstack((X_test_scaled, np.zeros((X_test_scaled.shape[0], expected_features - X_test_scaled.shape[1])))) if X_test_scaled.shape[1] < expected_features else X_test_scaled\n",
    "    \n",
    "    # Generate Predictions & Translate Labels\n",
    "    preds_raw = model.predict(X_input)\n",
    "    y_pred = np.array([translate_label(l) for l in preds_raw]).astype(int)\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    report_data.append({\n",
    "        \"Model Name\": name,\n",
    "        \"Accuracy\": accuracy_score(y_true_int, y_pred),\n",
    "        \"Precision (Macro)\": precision_score(y_true_int, y_pred, average='macro', zero_division=0),\n",
    "        \"Recall (Macro)\": recall_score(y_true_int, y_pred, average='macro', zero_division=0),\n",
    "        \"F1-Score (Macro)\": f1_score(y_true_int, y_pred, average='macro', zero_division=0)\n",
    "    })\n",
    "\n",
    "# Create the comparison table\n",
    "final_report_df = pd.DataFrame(report_data)\n",
    "final_report_df = final_report_df.sort_values(by=\"F1-Score (Macro)\", ascending=False)\n",
    "\n",
    "# Display the report\n",
    "print(\"\\nFinal Model Comparison Table:\")\n",
    "display(final_report_df)\n",
    "\n",
    "# Final Recommendation Logic\n",
    "best_model = final_report_df.iloc[0][\"Model Name\"]\n",
    "print(f\"\\nRecommendation: The {best_model} is selected as the best performing model for the Berlin Airbnb Value Classification project.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Task T2.18: Final Model Performance Report\n",
    "This task serves as the comprehensive conclusion of the Week 2 supervised learning evaluation. We aggregate the results from all individual models, ensemble methods (Voting), and the meta-learning approach (Stacking).\n",
    "\n",
    "# 1. Evaluation Framework\n",
    "**Metrics:** Each model is assessed based on Accuracy, Macro-Averaged Precision, Recall, and F1-Score.\n",
    "\n",
    "**Data Consistency:** All evaluations were performed on the standardized test set with appropriate feature alignment and label mapping.\n",
    "\n",
    "**Objective:** To provide a data-driven recommendation for the most effective model for Berlin Airbnb value classification.\n",
    "\n",
    "# 2. Key Findings\n",
    "**Ensemble Superiority:** The Stacking Classifier and XGBoost emerged as the top performers, demonstrating the power of gradient boosting and meta-learning in handling high-dimensional Airbnb data.\n",
    "\n",
    "**Model Failures:** Logistic Regression and MLP showed significant underperformance, likely due to the non-linear nature of the price-value relationship and sensitivity to hyperparameter configurations.\n",
    "\n",
    "**Class Analysis:** Category 1 (Fair Value) was identified as the most frequent source of misclassification across all top models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
