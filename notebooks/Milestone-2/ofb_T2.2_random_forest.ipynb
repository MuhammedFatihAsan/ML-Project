{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 2.2: Advanced Model - Random Forest Classifier Implementation\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to implement a Random Forest Classifier, an ensemble learning method that builds multiple decision trees and combines their predictions. This model is expected to outperform the baseline Logistic Regression by capturing non-linear relationships and feature interactions.\n",
    "\n",
    "## Understanding Key Metrics\n",
    "\n",
    "Since this is a supervised classification task, we use specific metrics to evaluate how well our model performs:\n",
    "\n",
    "1. **Accuracy:** - Measures the overall correctness of predictions. Range is 0 to 1, where 1 indicates perfect classification.\n",
    "2. **Precision (Macro):** - Average precision across all classes, measuring the proportion of correct positive predictions.\n",
    "3. **Recall (Macro):** - Average recall across all classes, measuring the proportion of actual positives correctly identified.\n",
    "4. **F1-Score (Macro):** - Harmonic mean of precision and recall, providing a balanced measure of model performance. Range is 0 to 1.\n",
    "\n",
    "## Why Random Forest?\n",
    "\n",
    "Random Forest is an ensemble method that offers several advantages:\n",
    "\n",
    "1. **Handles Non-Linear Relationships:** Unlike Logistic Regression, Random Forest can capture complex, non-linear patterns in the data.\n",
    "2. **Feature Interactions:** Automatically learns interactions between features without manual feature engineering.\n",
    "3. **Robust to Outliers:** Less sensitive to outliers and noisy data compared to linear models.\n",
    "4. **Feature Importance:** Provides insights into which features are most important for predictions.\n",
    "5. **Reduces Overfitting:** By averaging multiple trees, it reduces the risk of overfitting compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Discovery\n",
    "\n",
    "In this step, we import the required libraries and load the preprocessed dataset from the project directory.\n",
    "\n",
    "### Libraries Used:\n",
    "- **pandas & numpy:** For data manipulation and numerical operations\n",
    "- **RandomForestClassifier:** The main algorithm from sklearn.ensemble\n",
    "- **sklearn.metrics:** For evaluating model performance\n",
    "- **LabelEncoder:** To convert categorical target labels to numeric format\n",
    "- **pickle:** For saving the trained model\n",
    "\n",
    "### Data Loading:\n",
    "We load the scaled training and testing sets that were prepared in previous tasks. The scaling ensures all features are on the same scale, which helps with model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train['value_category'])\n",
    "y_test_encoded = label_encoder.transform(y_test['value_category'])\n",
    "\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    category = label_encoder.classes_[val]\n",
    "    print(f\"  Class {val} ({category}): {count} samples ({count/len(y_train_encoded)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Removing ID Columns from Features\n",
    "\n",
    "**Why we remove ID columns:**\n",
    "\n",
    "1. **IDs are unique identifiers, not predictive features** - They don't contain information about value categories\n",
    "2. **Including IDs would cause overfitting** - The model would memorize specific listings instead of learning patterns\n",
    "3. **IDs have no relationship with value categories** - A listing's ID number doesn't determine its value\n",
    "\n",
    "This is a critical preprocessing step to ensure our model learns meaningful patterns rather than memorizing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "We train a Random Forest Classifier with carefully selected hyperparameters:\n",
    "\n",
    "### Hyperparameter Explanation:\n",
    "\n",
    "1. **n_estimators=100** - Number of decision trees in the forest. More trees generally improve performance but increase computation time. 100 is a good balance.\n",
    "\n",
    "2. **max_depth=20** - Maximum depth of each tree. Limits how deep each tree can grow, preventing overfitting. A depth of 20 allows complex patterns while maintaining generalization.\n",
    "\n",
    "3. **min_samples_split=10** - Minimum number of samples required to split an internal node. Higher values prevent the model from learning overly specific patterns.\n",
    "\n",
    "4. **min_samples_leaf=4** - Minimum number of samples required at a leaf node. Ensures each leaf represents a meaningful group of samples.\n",
    "\n",
    "5. **random_state=42** - Ensures reproducibility. The same random seed produces the same results every time.\n",
    "\n",
    "6. **n_jobs=-1** - Uses all available CPU cores for parallel processing, significantly speeding up training.\n",
    "\n",
    "### How Random Forest Works:\n",
    "\n",
    "1. Creates 100 different decision trees, each trained on a random subset of the data\n",
    "2. Each tree makes its own prediction\n",
    "3. Final prediction is determined by majority voting across all trees\n",
    "4. This ensemble approach reduces overfitting and improves accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation\n",
    "\n",
    "Evaluating model performance on both training and testing sets.\n",
    "\n",
    "### Understanding the Metrics:\n",
    "\n",
    "**Training vs Testing Accuracy:**\n",
    "- **Training Accuracy:** How well the model performs on data it has seen during training\n",
    "- **Testing Accuracy:** How well the model generalizes to new, unseen data\n",
    "- **Gap between them:** A large gap indicates overfitting (model memorized training data)\n",
    "\n",
    "**Macro-Averaged Metrics:**\n",
    "- Calculate metric for each class separately, then average them\n",
    "- Treats all classes equally, regardless of their size\n",
    "- Important for imbalanced datasets to ensure all classes are considered\n",
    "\n",
    "**What to Look For:**\n",
    "- Testing accuracy should be higher than the baseline (Logistic Regression)\n",
    "- Training and testing accuracy should be reasonably close (not too large a gap)\n",
    "- F1-Score provides the best overall measure of model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
    "test_acc = accuracy_score(y_test_encoded, y_test_pred)\n",
    "test_precision = precision_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test_encoded, y_test_pred, average='macro')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Precision (Macro): {test_precision:.4f}\")\n",
    "print(f\"  Recall (Macro): {test_recall:.4f}\")\n",
    "print(f\"  F1-Score (Macro): {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Detailed Classification Report\n",
    "\n",
    "The classification report provides per-class performance metrics, helping us understand which value categories the model predicts well and which ones it struggles with.\n",
    "\n",
    "### Reading the Report:\n",
    "\n",
    "**For Each Class:**\n",
    "- **Precision:** Of all listings predicted as this class, what percentage were correct?\n",
    "- **Recall:** Of all actual listings in this class, what percentage did we correctly identify?\n",
    "- **F1-Score:** Harmonic mean of precision and recall for this class\n",
    "- **Support:** Number of actual samples in this class\n",
    "\n",
    "**Overall Metrics:**\n",
    "- **Accuracy:** Overall correctness across all classes\n",
    "- **Macro avg:** Simple average of metrics across all classes (treats each class equally)\n",
    "- **Weighted avg:** Average weighted by the number of samples in each class\n",
    "\n",
    "### What to Look For:\n",
    "- Are all three classes performing similarly, or is one class much harder to predict?\n",
    "- Low recall for a class means we're missing many actual instances of that class\n",
    "- Low precision for a class means we're incorrectly labeling other classes as this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_encoded, y_test_pred, \n",
    "                    target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance Analysis\n",
    "\n",
    "One of the key advantages of Random Forest is its ability to measure feature importance. This tells us which features contribute most to the model's predictions.\n",
    "\n",
    "### How Feature Importance Works:\n",
    "\n",
    "1. **Gini Importance:** Measures how much each feature decreases impurity across all trees\n",
    "2. **Higher values:** Indicate features that are more important for making accurate predictions\n",
    "3. **Sum to 1.0:** All feature importances add up to 1.0 (100%)\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Model Interpretability:** Understand what drives the model's decisions\n",
    "- **Feature Selection:** Identify which features could potentially be removed\n",
    "- **Business Insights:** Learn which listing characteristics most affect value perception\n",
    "- **Validation:** Ensure the model is using sensible features (not just noise)\n",
    "\n",
    "### Expected Important Features:\n",
    "\n",
    "We expect features like price, review scores, location, and amenities to be highly important, as these directly relate to a listing's value proposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('../../data/processed/random_forest_feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to: ../../data/processed/random_forest_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Saving Results\n",
    "\n",
    "We save all important outputs for future reference and comparison with other models.\n",
    "\n",
    "### Files Saved:\n",
    "\n",
    "1. **random_forest_model.pkl** - The trained model object\n",
    "   - Can be loaded later for making predictions without retraining\n",
    "   - Preserves all learned parameters and tree structures\n",
    "\n",
    "2. **random_forest_results.csv** - Summary of model performance metrics\n",
    "   - Allows easy comparison with other models (Logistic Regression, etc.)\n",
    "   - Contains all key metrics in one place\n",
    "\n",
    "3. **random_forest_predictions.csv** - Actual vs predicted values\n",
    "   - Useful for error analysis and understanding misclassifications\n",
    "   - Can be used to create confusion matrices\n",
    "\n",
    "4. **random_forest_feature_importance.csv** - Feature importance rankings\n",
    "   - Documents which features the model relies on most\n",
    "   - Useful for feature selection and model interpretation\n",
    "\n",
    "### Why Save These Files:\n",
    "\n",
    "- **Reproducibility:** Can recreate results without rerunning the entire notebook\n",
    "- **Comparison:** Easy to compare Random Forest with other models\n",
    "- **Documentation:** Provides a record of model performance for reports\n",
    "- **Deployment:** The saved model can be used in production applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'model': ['Random Forest'],\n",
    "    'train_accuracy': [train_acc],\n",
    "    'test_accuracy': [test_acc],\n",
    "    'precision_macro': [test_precision],\n",
    "    'recall_macro': [test_recall],\n",
    "    'f1_macro': [test_f1]\n",
    "})\n",
    "results_df.to_csv('../../data/processed/random_forest_results.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred\n",
    "})\n",
    "predictions_df.to_csv('../../data/processed/random_forest_predictions.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "with open('../../models/random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(\"Files saved successfully:\")\n",
    "print(\"  - ../../models/random_forest_model.pkl\")\n",
    "print(\"  - ../../data/processed/random_forest_results.csv\")\n",
    "print(\"  - ../../data/processed/random_forest_predictions.csv\")\n",
    "print(\"  - ../../data/processed/random_forest_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Expected Improvements Over Logistic Regression:\n",
    "\n",
    "Random Forest should outperform the baseline Logistic Regression model because:\n",
    "\n",
    "1. **Non-Linear Patterns:** Can capture complex relationships that linear models miss\n",
    "2. **Feature Interactions:** Automatically learns how features work together\n",
    "3. **Robustness:** Less affected by outliers and noisy data\n",
    "4. **Flexibility:** No assumptions about data distribution required\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
