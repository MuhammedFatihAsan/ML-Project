{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Milestone 3: Advanced NLP & Feature Engineering\n",
    "\n",
    "**Objective**\n",
    "In this notebook, we will use advanced Deep Learning techniques to understand the listing descriptions and compare them with our earlier work.\n",
    "1. **BERT Analysis:** Use pre-trained transformers to capture the context of sentences.\n",
    "2. **Word Embeddings:** Train a custom Word2Vec model to learn Airbnb-specific vocabulary.\n",
    "3. **Comparative Analysis:** Perform a comprehensive comparison between **BERT**, **Word2Vec**, and our **Baseline** (statistics from Milestone 1) to evaluate their performance relative to each other.\n",
    "\n",
    "**Final Goal: Meta-Feature Generation (Stacking)**\n",
    "We will not just choose one model. We will combine their intelligence.\n",
    "* We will calculate a **\"Confidence Score\"** (from -1 to +1) for each model.\n",
    "* We will save these scores into a new file (`nlp_scores.csv`).\n",
    "* This file will be the input for our final Hybrid Deep Learning Model in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "**What will we do?**\n",
    "In this step, we will prepare our environment and load the data.\n",
    "\n",
    "1.  **Import Libraries:** We will import Pandas, NumPy, and the Transformers library for BERT.\n",
    "2.  **Load Text Data (`listings_text_cleaned.csv`):**\n",
    "    * We created this file in milestone 1.\n",
    "    * It contains **two versions** of the text for different future tasks:\n",
    "        * **Raw Text:** The original text with punctuation. We need this for **BERT** to understand the context.\n",
    "        * **Clean Text:** The processed text without stopwords. We used this for TF-IDF before.\n",
    "3.  **Load Baseline Features (`nlp_master_features.csv`):**\n",
    "    * This file contains our old VADER sentiment scores.\n",
    "    * We will keep these scores to compare them with our new Deep Learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# We ignore warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import Transformers (for BERT)\n",
    "try:\n",
    "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "    print(\"Transformers library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Transformers library is not found. Please install it.\")\n",
    "\n",
    "# 2. Define File Paths\n",
    "# We assume the data is in the processed folder\n",
    "DATA_DIR = \"../../data/processed/\"\n",
    "TEXT_FILE = os.path.join(DATA_DIR, \"listings_text_cleaned.csv\")\n",
    "FEATURES_FILE = os.path.join(DATA_DIR, \"nlp_master_features.csv\")\n",
    "\n",
    "# 3. Load Data\n",
    "if os.path.exists(TEXT_FILE) and os.path.exists(FEATURES_FILE):\n",
    "    # Load text data (contains 'description' and 'description_clean')\n",
    "    df_text = pd.read_csv(TEXT_FILE)\n",
    "    \n",
    "    # Load old features (contains VADER scores)\n",
    "    df_features = pd.read_csv(FEATURES_FILE)\n",
    "    \n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Text Data Shape: {df_text.shape}\")\n",
    "    print(f\"Features Data Shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Please check your paths.\")\n",
    "\n",
    "# 4. Prepare Baseline Features\n",
    "# Strategy: We keep VADER scores and Counts. We DROP old TF-IDF columns.\n",
    "# We will use these to compare with our new Deep Learning model later.\n",
    "keep_columns = [\n",
    "    'id', \n",
    "    'description_sentiment', \n",
    "    'host_about_sentiment', \n",
    "    'desc_word_count', \n",
    "    'desc_length', \n",
    "    'name_length'\n",
    "]\n",
    "\n",
    "# Create the baseline dataframe\n",
    "if 'df_features' in locals():\n",
    "    df_baseline = df_features[keep_columns].copy()\n",
    "    print(\"\\nBaseline Features Selected (VADER + Structure):\")\n",
    "    print(df_baseline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Interpretation of Step 1\n",
    "\n",
    "We successfully loaded the data and created three dataframes:\n",
    "\n",
    "1.  **df_text**:\n",
    "    * **Source:** `listings_text_cleaned.csv`\n",
    "    * **Content:** Contains the raw `description` text.\n",
    "    * **Why?** We will feed this raw text into the BERT model to understand the context.<br><br>\n",
    "\n",
    "2.  **df_features**:\n",
    "    * **Source:** `nlp_master_features.csv`\n",
    "    * **Content:** Contains all 107 NLP features from Milestone 1 (including VADER scores and TF-IDF).\n",
    "    * **Why?** We loaded this to extract the specific columns we need.<br><br>\n",
    "\n",
    "3.  **df_baseline**:\n",
    "    * **Source:** Selected columns from `df_features`.\n",
    "    * **Content:** Contains only `id`, VADER sentiment scores, and word counts.\n",
    "    * **Why?** These are our \"Baseline\" features. Later, we will compare these old scores with the new BERT scores to see if Deep Learning is better.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Initialize BERT Tokenizer and Model\n",
    "\n",
    "**What will we do?**\n",
    "Computers cannot read words. They only understand numbers.\n",
    "1.  **Tokenizer:** We will load a tool that converts our text into numbers (tokens).\n",
    "2.  **Model:** We will download the **DistilBERT** model.\n",
    "    * **Why DistilBERT?** It is a smaller, faster, and lighter version of BERT. It gives 95% of the performance but runs 60% faster.\n",
    "    * **Pre-trained:** The model already \"knows\" English because it was trained on Wikipedia and Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import PyTorch and Transformers\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Check device (Use GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Tokenizer\n",
    "# We use 'distilbert-base-uncased' (Standard English model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 3. Initialize Model (PyTorch Version)\n",
    "# We switched to PyTorch (DistilBertModel) to avoid Keras errors.\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move the model to the active device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "print(\"DistilBERT Model (PyTorch) loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Interpretation of Step 2\n",
    "\n",
    "We successfully loaded the model.\n",
    "\n",
    "**Model Status:** The DistilBERT model is ready to process our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Generating BERT Embeddings\n",
    "\n",
    "**What will we do?**\n",
    "We will now convert the listing descriptions into numbers (vectors).\n",
    "\n",
    "**How will we do it?**\n",
    "1.  **Batch Processing:** We cannot process all 20,000 listings at once. It would crash the computer's memory (RAM).\n",
    "2.  **Loop:** We will take small groups (e.g., 32 listings at a time).\n",
    "3.  **Tokenize & Encode:**\n",
    "    * First, we convert words to tokens.\n",
    "    * Then, we feed them into the DistilBERT model.\n",
    "    * The model gives us a **vector of size 768** for each listing. This vector represents the \"meaning\" of the description.\n",
    "\n",
    "**Note:** This process might take some time (10-20 minutes on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data\n",
    "# We fill empty descriptions with \" \" to avoid errors.\n",
    "descriptions = df_text['description'].fillna(\"\").tolist()\n",
    "\n",
    "# 2. Parameters\n",
    "BATCH_SIZE = 32 # We process 32 listings at a time\n",
    "embeddings_list = []\n",
    "\n",
    "print(f\"Starting BERT embedding generation for {len(descriptions)} listings...\")\n",
    "\n",
    "# 3. Loop through data in batches\n",
    "# range(start, stop, step)\n",
    "for i in range(0, len(descriptions), BATCH_SIZE):\n",
    "    # Select the batch\n",
    "    batch_texts = descriptions[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # Tokenize\n",
    "    # padding=True: pad to the longest sentence in the batch\n",
    "    # truncation=True: cut texts longer than 128 tokens (saves memory)\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                      max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the device (CPU or GPU)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Generate Embeddings\n",
    "    with torch.no_grad(): # We do not need gradients for inference (saves RAM)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token (The vector representing the whole sentence)\n",
    "    # It is the first token (index 0) of the last hidden state\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Add to our list\n",
    "    embeddings_list.extend(batch_embeddings)\n",
    "    \n",
    "    # Print progress every 100 batches (approx. every 3200 listings)\n",
    "    if (i // BATCH_SIZE) % 100 == 0:\n",
    "        print(f\"Processed {i} / {len(descriptions)} listings...\")\n",
    "\n",
    "print(\"Embedding generation complete!\")\n",
    "\n",
    "# 4. Create DataFrame\n",
    "# Convert the list of arrays into a Pandas DataFrame\n",
    "df_bert = pd.DataFrame(embeddings_list)\n",
    "\n",
    "# Rename columns to 'bert_0', 'bert_1', ... 'bert_767'\n",
    "df_bert.columns = [f'bert_{i}' for i in range(df_bert.shape[1])]\n",
    "\n",
    "# Add the ID column for merging later\n",
    "df_bert['id'] = df_text['id'].values\n",
    "\n",
    "print(f\"BERT DataFrame Shape: {df_bert.shape}\")\n",
    "print(df_bert.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Interpretation of Step 3 (BERT Embeddings)\n",
    "\n",
    "We have successfully converted 20,942 listing descriptions into high-dimensional vectors.\n",
    "\n",
    "**Understanding the Output (`df_bert`):**\n",
    "* **Rows (20,942):** Each row represents one Airbnb listing.\n",
    "* **Columns (769):**\n",
    "    * **`id`:** The key to match these numbers back to the original house.\n",
    "    * **`bert_0` ... `bert_767`:** These **768 numbers** are the \"Deep Learning features.\"\n",
    "    * Unlike VADER (which gave us just 1 score: Positive/Negative), BERT gives us **768 dimensions** of meaning (e.g., one number might represent \"luxury,\" another \"location,\" another \"coziness\").\n",
    "\n",
    "**Next Step:**\n",
    "Now that we have these valuable numbers, we must **save** them immediately so we don't have to wait a long time again. Then, we will merge them with our VADER scores to prepare for the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Save and Merge Data\n",
    "\n",
    "**What will we do?**\n",
    "1.  **Save to CSV:** We will save the new BERT features to a file (`bert_embeddings.csv`).\n",
    "    * **Why?** Generating these numbers took a long time. We save them immediately to avoid doing it again if the computer crashes.\n",
    "2.  **Merge:** We will combine the **BERT features** (768 columns) with our **Baseline Features** (VADER scores + Word Counts).\n",
    "    * **Goal:** Create a single \"Dataset\" that allows us to compare the old method vs. the new method side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Paths\n",
    "BERT_FILE = os.path.join(DATA_DIR, \"bert_embeddings.csv\")\n",
    "FINAL_TASK_FILE = os.path.join(DATA_DIR, \"bert_prepared.csv\")\n",
    "\n",
    "# 2. Save BERT Embeddings (Checkpoint)\n",
    "# We save this immediately so we don't lose the calculated data.\n",
    "if 'df_bert' in locals():\n",
    "    df_bert.to_csv(BERT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved: {BERT_FILE}\")\n",
    "else:\n",
    "    print(\"Warning: df_bert not found. Make sure Step 3 ran successfully.\")\n",
    "\n",
    "# 3. Merge with Baseline Features\n",
    "# We combine:\n",
    "# - df_baseline: ID + VADER Scores + Word Counts (Old features)\n",
    "# - df_bert: ID + 768 BERT Vectors (New Deep Learning features)\n",
    "if 'df_baseline' in locals() and 'df_bert' in locals():\n",
    "    # Merge on 'id'\n",
    "    df_task3_1 = pd.merge(df_baseline, df_bert, on='id', how='inner')\n",
    "    \n",
    "    # 4. Save Final Dataset\n",
    "    df_task3_1.to_csv(FINAL_TASK_FILE, index=False)\n",
    "    \n",
    "    print(\"\\nMerge Complete!\")\n",
    "    print(f\"Final Dataset Shape: {df_task3_1.shape}\")\n",
    "    print(f\"Saved to: {FINAL_TASK_FILE}\")\n",
    "    \n",
    "    # Display first few rows to confirm\n",
    "    print(df_task3_1.head(3))\n",
    "else:\n",
    "    print(\"Error: Could not merge. Check if df_baseline and df_bert exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Word Embeddings (Word2Vec)\n",
    "\n",
    "**What will we do?**\n",
    "We will train a **Word2Vec** model specifically on our Airbnb descriptions.\n",
    "* **BERT vs Word2Vec:**\n",
    "    * **BERT** is pre-trained on Wikipedia. It knows general English perfectly.\n",
    "    * **Word2Vec** will be trained *only* on our data. It will learn the specific jargon of Airbnb (e.g., that \"Ocean\" and \"Beach\" are mathematically very close).\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Input:** We will use the **Cleaned Text** (`description_clean`) this time.\n",
    "    * *Why?* Word2Vec doesn't need punctuation or stopwords. It just needs the core words.\n",
    "2.  **Training:** We will create a model that represents every word as a vector.\n",
    "3.  **Averaging:** Since a house has many words, we will take the **average** of all word vectors to get a single \"House Vector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Library\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    print(\"Gensim library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Gensim not found. Please install it using: !pip install gensim\")\n",
    "\n",
    "# 2. Prepare Data\n",
    "# Word2Vec expects a list of words, not a full sentence string.\n",
    "# We use 'description_clean' because stopwords/punctuation are already removed.\n",
    "# We convert \"sunny room wifi\" -> ['sunny', 'room', 'wifi']\n",
    "sentences = df_text['description_clean'].fillna(\"\").apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "print(f\"Training Word2Vec model on {len(sentences)} listings...\")\n",
    "\n",
    "# 3. Train Word2Vec Model\n",
    "# vector_size=100: Each word/house will be represented by 100 numbers.\n",
    "# window=5: The model looks at 5 words before and after the target word.\n",
    "# min_count=5: We ignore rare words (words that appear less than 5 times).\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "print(\"Word2Vec Model trained successfully.\")\n",
    "\n",
    "# 4. Generate Document Vectors (Averaging)\n",
    "# Since a house description has many words, we take the AVERAGE of all word vectors\n",
    "# to get a single vector representing the house.\n",
    "\n",
    "def get_mean_vector(word_list):\n",
    "    # Filter words that exist in our trained model\n",
    "    valid_words = [word for word in word_list if word in w2v_model.wv]\n",
    "    \n",
    "    if len(valid_words) > 0:\n",
    "        # Calculate mean (average)\n",
    "        return np.mean(w2v_model.wv[valid_words], axis=0)\n",
    "    else:\n",
    "        # If no valid words, return a list of zeros\n",
    "        return np.zeros(100)\n",
    "\n",
    "# Apply the function to all listings\n",
    "w2v_vectors = [get_mean_vector(doc) for doc in sentences]\n",
    "\n",
    "# 5. Create DataFrame\n",
    "df_w2v = pd.DataFrame(w2v_vectors)\n",
    "\n",
    "# Rename columns to w2v_0, w2v_1, ... w2v_99\n",
    "df_w2v.columns = [f'w2v_{i}' for i in range(df_w2v.shape[1])]\n",
    "\n",
    "# Add ID for merging\n",
    "df_w2v['id'] = df_text['id'].values\n",
    "\n",
    "print(f\"Word2Vec DataFrame Shape: {df_w2v.shape}\")\n",
    "print(df_w2v.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Interpretation of Step 5 (Word2Vec)\n",
    "\n",
    "We successfully trained a custom Word2Vec model on our data.\n",
    "* **Warning Note:** The `Exception ignored` message in the output is a harmless warning related to the library's internal threads. It did not stop the process.\n",
    "* **Result:** We now have a dataframe (`df_w2v`) with **100 new columns** (`w2v_0` to `w2v_99`)(and +1 id for represent).\n",
    "* **Meaning:** Each row represents the \"average meaning\" of a house description, distilled into 100 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Save Word2Vec Data\n",
    "\n",
    "**What will we do?**\n",
    "We will save these new features to a CSV file (`word2vec_embeddings.csv`).\n",
    "\n",
    "**Why?**\n",
    "1.  **Safety:** Just like BERT, we want to save our work so we don't have to retrain the model later.\n",
    "2.  **Usage:** Later, we will use this file to train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Path\n",
    "W2V_FILE = os.path.join(DATA_DIR, \"word2vec_embeddings.csv\")\n",
    "\n",
    "# 2. Save Word2Vec Features\n",
    "if 'df_w2v' in locals():\n",
    "    df_w2v.to_csv(W2V_FILE, index=False)\n",
    "    \n",
    "    print(\"Save Complete!\")\n",
    "    print(f\"File saved to: {W2V_FILE}\")\n",
    "else:\n",
    "    print(\"Error: df_w2v not found. Please check Step 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Target Variable\n",
    "\n",
    "**What will we do?**\n",
    "Before training the model, we need to prepare the \"Answer Key\" (Target Variable).\n",
    "\n",
    "1.  **Load Data:** We will load the file `listings_cleaned_with_target.csv` which contains our calculated value categories.\n",
    "2.  **Filter and Encode:**\n",
    "    * We will select only the `id` and `value_category` columns.\n",
    "    * We will convert the text categories into numbers (Label Encoding) so the Neural Network can understand them:\n",
    "        * `Poor_Value` -> **0**\n",
    "        * `Fair_Value` -> **1**\n",
    "        * `Excellent_Value` -> **2**\n",
    "3.  **Save:** We will save this ready-to-use table (with columns: `id`, `value_category`, `target_encoded`) as `target_labels.csv`. This ensures we don't have to repeat this step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define File Paths\n",
    "TARGET_SOURCE_FILE = os.path.join(DATA_DIR, \"listings_cleaned_with_target.csv\")\n",
    "TARGET_OUTPUT_FILE = os.path.join(DATA_DIR, \"target_labels.csv\")\n",
    "\n",
    "# 2. Load Source Data\n",
    "if os.path.exists(TARGET_SOURCE_FILE):\n",
    "    df_target_source = pd.read_csv(TARGET_SOURCE_FILE)\n",
    "    \n",
    "    # 3. Filter and Encode\n",
    "    # We only need ID and the Category\n",
    "    df_labels = df_target_source[['id', 'value_category']].copy()\n",
    "    \n",
    "    # Define the mapping (Encoding)\n",
    "    # 0: Poor, 1: Fair, 2: Excellent\n",
    "    label_mapping = {\n",
    "        'Poor_Value': 0,\n",
    "        'Fair_Value': 1,\n",
    "        'Excellent_Value': 2\n",
    "    }\n",
    "    \n",
    "    # Apply mapping\n",
    "    df_labels['target_encoded'] = df_labels['value_category'].map(label_mapping)\n",
    "    \n",
    "    # Check for unmapped values (NaN)\n",
    "    if df_labels['target_encoded'].isnull().sum() > 0:\n",
    "        print(\"Warning: Some categories were not found in the map and are set to NaN.\")\n",
    "        # Drop NaNs if any (to be safe)\n",
    "        df_labels = df_labels.dropna(subset=['target_encoded'])\n",
    "        \n",
    "    # Convert to integer\n",
    "    df_labels['target_encoded'] = df_labels['target_encoded'].astype(int)\n",
    "\n",
    "    # 4. Save to CSV\n",
    "    df_labels.to_csv(TARGET_OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"Target Labels Processed and Saved!\")\n",
    "    print(f\"File saved to: {TARGET_OUTPUT_FILE}\")\n",
    "    print(f\"Shape: {df_labels.shape}\")\n",
    "    print(df_labels.head(5))\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Source file not found at {TARGET_SOURCE_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Step 8: Comparative Analysis (Cross-Validation)\n",
    "\n",
    "**What will we do?**\n",
    "We will conduct a rigorous experiment to decide which NLP technique is the best for predicting value.\n",
    "Instead of a single test split, we will use **5-Fold Cross-Validation**.\n",
    "* This means we will train and test the models 5 times on different parts of the data and average the results.\n",
    "\n",
    "**The Three Contenders (Experiments):**\n",
    "1.  **Experiment A (BERT):**\n",
    "    * **Input:** 768 BERT embeddings (`bert_embeddings.csv`).\n",
    "    * **Hypothesis:** The most complex model, should understand context best.\n",
    "2.  **Experiment B (Word2Vec):**\n",
    "    * **Input:** 100 Word2Vec vectors (`word2vec_embeddings.csv`).\n",
    "    * **Hypothesis:** Faster and lighter, but might miss complex sentence structures.\n",
    "3.  **Experiment C (Baseline):**\n",
    "    * **Input:** Old VADER scores + TF-IDF (`nlp_master_features.csv`).\n",
    "    * **Hypothesis:** Our reference point. Can Deep Learning beat these simple statistics?\n",
    "\n",
    "**Method:**\n",
    "We will use a standard **Neural Network (MLP Classifier)** for all three experiments to keep the comparison fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Setup Configurations\n",
    "# We will use a standard Neural Network structure for all experiments to be fair.\n",
    "# Hidden Layers: (64, 32) -> A standard architecture for this data size.\n",
    "clf = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
    "\n",
    "# Cross Validation Setup: 5 splits, shuffled\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# File Paths (Assuming all files are in DATA_DIR)\n",
    "target_path = os.path.join(DATA_DIR, \"target_labels.csv\")\n",
    "\n",
    "# Define Experiments\n",
    "experiments = {\n",
    "    \"Baseline (TF-IDF + VADER)\": \"nlp_master_features.csv\",\n",
    "    \"Word2Vec (100 Dim)\": \"word2vec_embeddings.csv\",\n",
    "    \"BERT (768 Dim)\": \"bert_embeddings.csv\"\n",
    "}\n",
    "\n",
    "# 2. Load Target Data\n",
    "if os.path.exists(target_path):\n",
    "    df_target = pd.read_csv(target_path)\n",
    "    print(f\"Targets loaded. Shape: {df_target.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Target labels file not found!\")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting 5-Fold Cross-Validation Analysis...\\n\" + \"-\"*50)\n",
    "\n",
    "# 3. Execution Loop\n",
    "for exp_name, file_name in experiments.items():\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Running Experiment: {exp_name}...\")\n",
    "        \n",
    "        # Load Features\n",
    "        df_features = pd.read_csv(file_path)\n",
    "        \n",
    "        # Merge with Targets\n",
    "        # Use 'inner' to ensure we only have rows that exist in both files\n",
    "        df_merged = pd.merge(df_target, df_features, on='id', how='inner')\n",
    "        \n",
    "        # Define X (Features) and y (Target)\n",
    "        # Drop ID and target columns from X\n",
    "        X = df_merged.drop(columns=['id', 'value_category', 'target_encoded'])\n",
    "        # If 'bert' in name, drop non-numeric columns if any exist (safety check)\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        \n",
    "        y = df_merged['target_encoded']\n",
    "        \n",
    "        # Scale Data\n",
    "        # Neural Networks perform much better when data is scaled (0-1 range approx)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Run Cross-Validation\n",
    "        # scoring: 'accuracy' and 'f1_weighted' (good for imbalanced classes)\n",
    "        cv_results = cross_validate(clf, X_scaled, y, cv=cv, scoring=['accuracy', 'f1_weighted'])\n",
    "        \n",
    "        # Store Results\n",
    "        avg_acc = cv_results['test_accuracy'].mean()\n",
    "        avg_f1 = cv_results['test_f1_weighted'].mean()\n",
    "        \n",
    "        print(f\"   -> Accuracy: {avg_acc:.4f}\")\n",
    "        print(f\"   -> F1 Score: {avg_f1:.4f}\\n\")\n",
    "        \n",
    "        results.append({\n",
    "            \"Experiment\": exp_name,\n",
    "            \"Accuracy\": avg_acc,\n",
    "            \"F1 Score\": avg_f1\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping {exp_name}: File {file_name} not found.\")\n",
    "\n",
    "# 4. Final Comparison Table\n",
    "print(\"-\" * 50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Analysis of Step 8 Results\n",
    "\n",
    "We tested three models. Here is a fair comparison based on Accuracy and Computational Cost (Workload).\n",
    "\n",
    "**1. The Difference Between Accuracy and F1 Score**\n",
    "\n",
    "* **Accuracy (The Trap):** Accuracy can sometimes trick us.\n",
    "    * *Imagine this:* There is a class with 100 students. 95 are healthy, and 5 are sick.\n",
    "    * If the model says **\"Everyone is healthy\"**, the Accuracy is **95%**. It looks amazing on paper.\n",
    "    * *But:* It completely missed the 5 sick students. This is a useless model.\n",
    "* **F1 Score (The Balance Check):**\n",
    "    * F1 Score protects us from this trick. It checks if the model found the small groups (the 5 sick students) too.\n",
    "    * **Our Result (~0.52):** This is an **average score**. It means our models are learning, but they are not perfect yet.\n",
    "\n",
    "**2. Comparative Table: Performance vs. Workload**\n",
    "\n",
    "We scored the \"Workload\" (CPU/RAM usage) on a scale of 1 to 5 (1=Very Light, 5=Very Heavy).\n",
    "\n",
    "| Model | Accuracy | F1 Score | Workload (1-5) | Comment |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Baseline** (TF-IDF) | 49.68% | 0.49 | **1 / 5** | Instant results. The starting point. |\n",
    "| **Word2Vec** | 51.70% | 0.51 | **2 / 5** | **+2% better** than Baseline with low cost. Efficient. |\n",
    "| **BERT** | 52.44% | 0.52 | **5 / 5** | **+0.7% better** than Word2Vec but requires **max power**. |\n",
    "\n",
    "**3. Interpretation of Trade-offs**\n",
    "\n",
    "* **Word2Vec vs. Baseline:** By slightly increasing the workload (1 → 2), we gain a solid **2% increase** in accuracy. This is a \"profitable\" trade.\n",
    "* **BERT vs. Word2Vec:** To gain an extra **0.7% accuracy**, we must increase the workload significantly (2 → 5).\n",
    "\n",
    "**Current Status:**\n",
    "We have successfully extracted features using all three methods. We will keep these results in mind as we move forward to the next stages of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 9: Generating Continuous NLP Scores (Meta-Features)\n",
    "\n",
    "**The Goal:**\n",
    "We want to convert our complex NLP models into a simple, powerful \"Score\" that represents the value of a house based *only* on its text.\n",
    "\n",
    "**The Strategy: From Classes to Continuum**\n",
    "Instead of forcing the model to make a hard choice (\"Is it Fair or Excellent?\"), we will ask for its **confidence**.\n",
    "* A rigid model says: \"This is Fair.\"\n",
    "* Our approach asks: \"How close is it to Excellent? How close is it to Poor?\"\n",
    "\n",
    "**The Methodology:**\n",
    "1.  **Input:** We will take the three models we tested:\n",
    "    * **Baseline** (TF-IDF)\n",
    "    * **Word2Vec** (Embeddings)\n",
    "    * **BERT** (Deep Learning)<br><br>\n",
    "2.  **Probability Calculation:** For every house, the models will calculate the probability of each category (Poor, Fair, Excellent).<br><br>\n",
    "3.  **The Magic Formula:** We will convert these probabilities into a single number between **-1 and +1**.\n",
    "    * $$Score = (1 \\times P_{Excellent}) + (-1 \\times P_{Poor}) + (0 \\times P_{Fair})$$\n",
    "    * **Result +1.0:** Perfectly Excellent.\n",
    "    * **Result -1.0:** Perfectly Poor.\n",
    "    * **Result 0.0:** Perfectly Fair.\n",
    "    * **Result +0.4:** Fair, but leaning towards Excellent.<br><br>\n",
    "4.  **Cross-Validation:** We will generate these scores using \"Cross-Validation\" to ensure the model doesn't cheat by memorizing the answers.\n",
    "\n",
    "**Outcome:**\n",
    "We will save a new file `nlp_scores.csv` containing just 4 columns: `id`, `baseline_score`, `w2v_score`, `bert_score`. This file will be the \"Gold Standard\" input for our final project phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Setup Configurations\n",
    "# We will use the same Neural Network structure to be consistent.\n",
    "clf = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# File Paths\n",
    "target_path = os.path.join(DATA_DIR, \"target_labels.csv\")\n",
    "output_path = os.path.join(DATA_DIR, \"nlp_scores.csv\")\n",
    "\n",
    "# Input Files (The 3 experiments)\n",
    "experiments = {\n",
    "    \"baseline\": \"nlp_master_features.csv\",\n",
    "    \"w2v\": \"word2vec_embeddings.csv\",\n",
    "    \"bert\": \"bert_embeddings.csv\"\n",
    "}\n",
    "\n",
    "# 2. Load Target Data\n",
    "if os.path.exists(target_path):\n",
    "    df_target = pd.read_csv(target_path)\n",
    "    print(f\"Targets loaded. Shape: {df_target.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"target_labels.csv not found! Please check Step 7.\")\n",
    "\n",
    "# Initialize final dataframe with IDs\n",
    "# We will add columns to this dataframe\n",
    "df_scores = df_target[['id']].copy()\n",
    "\n",
    "print(\"\\nStarting Meta-Feature Generation (Stacking)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Execution Loop\n",
    "for name, file_name in experiments.items():\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Processing Model: {name}...\")\n",
    "        \n",
    "        # Load Features\n",
    "        df_features = pd.read_csv(file_path)\n",
    "        \n",
    "        # Merge with Targets (Inner Join)\n",
    "        df_merged = pd.merge(df_target, df_features, on='id', how='inner')\n",
    "        \n",
    "        # Prepare X (Features) and y (Target)\n",
    "        X = df_merged.drop(columns=['id', 'value_category', 'target_encoded'])\n",
    "        X = X.select_dtypes(include=[np.number]) # Use only numbers\n",
    "        y = df_merged['target_encoded']\n",
    "        \n",
    "        # Scale Data (Important for Neural Networks)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Generate Probabilities using Cross-Validation\n",
    "        # This ensures we get predictions for EVERY house as if it was in the test set.\n",
    "        # method='predict_proba' returns 3 columns: [Prob_Poor, Prob_Fair, Prob_Excellent]\n",
    "        print(f\"   -> Calculating probabilities...\")\n",
    "        probs = cross_val_predict(clf, X_scaled, y, cv=cv, method='predict_proba')\n",
    "        \n",
    "        # Apply the Magic Formula: Score = P(Excellent) - P(Poor)\n",
    "        # Column 0 = Poor, Column 1 = Fair, Column 2 = Excellent\n",
    "        # Fair (0) is ignored in the calculation as we discussed.\n",
    "        continuous_scores = probs[:, 2] - probs[:, 0]\n",
    "        \n",
    "        # Create a temporary dataframe to merge back safely\n",
    "        temp_df = pd.DataFrame({\n",
    "            'id': df_merged['id'],\n",
    "            f'{name}_score': continuous_scores\n",
    "        })\n",
    "        \n",
    "        # Merge into our main scorecard\n",
    "        df_scores = pd.merge(df_scores, temp_df, on='id', how='left')\n",
    "        \n",
    "        print(f\"   -> {name}_score generated. (Mean: {continuous_scores.mean():.3f})\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Warning: File {file_name} not found. Skipping.\")\n",
    "\n",
    "# 4. Save Final Scores\n",
    "# Drop any rows that might have missing values (safety check)\n",
    "df_scores.dropna(inplace=True)\n",
    "\n",
    "df_scores.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Meta-Feature Generation Complete!\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"Final Shape: {df_scores.shape}\")\n",
    "print(df_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Conclusion of NLP Feature Engineering\n",
    "\n",
    "**What did we do in Step 9?**\n",
    "We converted our three complex models (Baseline, Word2Vec, BERT) into simple \"Score\" values.\n",
    "Instead of dealing with raw text or huge vectors, we calculated a specific mathematical score for each house.\n",
    "* **-1.0:** Represents \"Poor Value\" confidence.\n",
    "* **+1.0:** Represents \"Excellent Value\" confidence.\n",
    "\n",
    "**Final Output:**\n",
    "We saved these scores to `nlp_scores.csv`.\n",
    "* This file contains the distilled knowledge of all our NLP experiments.\n",
    "* It is clean, simple, and ready to be used as a high-quality input for any future analysis.\n",
    "\n",
    "**Achievement:**\n",
    "We successfully transformed unstructured text descriptions into structured, powerful numerical features. The NLP feature engineering part of the project is now complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Step 10: Final Data Integration (Word2Vec Selection)\n",
    "\n",
    "**Objective:**\n",
    "We are now ready to create the final dataset for our Deep Learning Model.\n",
    "Based on our Cost-Performance analysis, we have decided to use the **Word2Vec Score**.\n",
    "* It offers the best balance: High Accuracy (~51.7%) with Low Computational Cost.\n",
    "\n",
    "**What will we do?**\n",
    "1.  **Load Data:**\n",
    "    * Load the processed numerical data (`numeric_final_data.csv` from `data/finalized`).\n",
    "    * Load our NLP scores (`nlp_scores.csv`).<br><br>\n",
    "2.  **Merge and Select:**\n",
    "    * Merge the two datasets using `id`.\n",
    "    * Select only the **`w2v_score`** (ignoring Baseline and BERT).<br><br>\n",
    "3.  **Cleanup:**\n",
    "    * Remove the `id` column (it is no longer needed for training).<br><br>\n",
    "4.  **Save:**\n",
    "    * Save the final, ready-to-train file as `final_data_with_nlp_score.csv` in the `data/finalized` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Define File Paths (Corrected)\n",
    "# NLP scores are in 'processed', Numeric data is in 'finalized'\n",
    "NLP_SCORES_PATH = \"../../data/processed/nlp_scores.csv\"\n",
    "NUMERIC_DATA_PATH = \"../../data/finalized/numeric_final_data.csv\"\n",
    "OUTPUT_PATH = \"../../data/finalized/final_data_with_nlp_score.csv\"\n",
    "\n",
    "# 2. Check and Load Data\n",
    "if os.path.exists(NUMERIC_DATA_PATH) and os.path.exists(NLP_SCORES_PATH):\n",
    "    print(\"Loading datasets...\")\n",
    "    df_numeric = pd.read_csv(NUMERIC_DATA_PATH)\n",
    "    df_nlp = pd.read_csv(NLP_SCORES_PATH)\n",
    "    \n",
    "    print(f\"Numeric Data Shape: {df_numeric.shape}\")\n",
    "    print(f\"NLP Scores Shape: {df_nlp.shape}\")\n",
    "\n",
    "    # 3. Merge Data (Numeric + Word2Vec Score)\n",
    "    # We explicitly select only 'id' and 'w2v_score' from the NLP dataframe\n",
    "    print(\"Merging Word2Vec scores...\")\n",
    "    df_final = pd.merge(df_numeric, df_nlp[['id', 'w2v_score']], on='id', how='inner')\n",
    "    \n",
    "    # 4. Reorder Columns\n",
    "    # We want 'w2v_score' to be the first feature (since we will drop ID)\n",
    "    cols = list(df_final.columns)\n",
    "    \n",
    "    # Move 'w2v_score' to the front (index 0)\n",
    "    if 'w2v_score' in cols:\n",
    "        cols.remove('w2v_score')\n",
    "        # Insert at position 1 (assuming ID is at 0) to verify, then drop ID.\n",
    "        # Or simpler: just put it at 0 after dropping ID. \n",
    "        # Let's insert it at index 1 first to be safe with the 'id' logic.\n",
    "        cols.insert(1, 'w2v_score')\n",
    "        df_final = df_final[cols]\n",
    "\n",
    "    # 5. Drop 'id'\n",
    "    df_final.drop(columns=['id'], inplace=True)\n",
    "    \n",
    "    # 6. Save Final File\n",
    "    df_final.to_csv(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Integration Complete!\")\n",
    "    print(f\"Selected NLP Feature: Word2Vec Score\")\n",
    "    print(f\"Dropped Column: id\")\n",
    "    print(f\"Final Dataset Saved to: {OUTPUT_PATH}\")\n",
    "    print(f\"Final Shape: {df_final.shape}\")\n",
    "    print(df_final.head())\n",
    "\n",
    "else:\n",
    "    print(\"Error: Input files not found.\")\n",
    "    print(f\"Looking for: {NUMERIC_DATA_PATH}\")\n",
    "    print(f\"Looking for: {NLP_SCORES_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
