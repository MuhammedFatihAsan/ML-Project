{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.11: Learning Curve Analysis\n",
    "\n",
    "## Overview\n",
    "This task performs comprehensive learning curve analysis for our top 3 performing models to understand the bias-variance tradeoff and determine whether collecting more data would improve model performance.\n",
    "\n",
    "## Objectives\n",
    "1. **Generate learning curves** for XGBoost, Random Forest, and MLP Classifier\n",
    "2. **Analyze bias-variance tradeoff** by examining training vs validation score gaps\n",
    "3. **Determine data saturation** - whether more data would help or if models have reached their limit\n",
    "4. **Provide actionable recommendations** based on the analysis\n",
    "\n",
    "## What is a Learning Curve?\n",
    "A learning curve shows how model performance changes as the training set size increases. It plots:\n",
    "- **Training Score**: How well the model fits the training data\n",
    "- **Validation Score**: How well the model generalizes to unseen data\n",
    "\n",
    "## Interpreting Learning Curves\n",
    "\n",
    "### High Bias (Underfitting)\n",
    "- Both training and validation scores are low\n",
    "- Small gap between training and validation curves\n",
    "- Adding more data won't help much\n",
    "- **Solution**: Use more complex model, add features\n",
    "\n",
    "### High Variance (Overfitting)\n",
    "- Training score is high, validation score is low\n",
    "- Large gap between training and validation curves\n",
    "- Adding more data may help\n",
    "- **Solution**: Regularization, reduce model complexity, get more data\n",
    "\n",
    "### Good Fit\n",
    "- Both scores are high and converge\n",
    "- Small gap between curves\n",
    "- Model is well-balanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup\n",
    "\n",
    "We import all necessary libraries for:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Machine learning models (sklearn, xgboost)\n",
    "- Learning curve generation (sklearn.model_selection)\n",
    "- Visualization (matplotlib, seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create output directories\n",
    "Path('../../outputs').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs/figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TASK 3.11: LEARNING CURVE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLibraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "\n",
    "### Data Loading\n",
    "We load the preprocessed and scaled datasets from Task 1.6.\n",
    "\n",
    "### Removing Leaky Features\n",
    "**Critical Step**: We must remove features that cause data leakage. These are features that were used to create the target variable (`value_category`):\n",
    "\n",
    "- `price`, `price_normalized`, `price_per_person`, `price_per_bathroom`, `price_per_bedroom` - Price features used in FP score calculation\n",
    "- `review_scores_rating`, `review_scores_value` - Rating features used in FP score calculation\n",
    "- `value_density`, `estimated_revenue_l365d` - Derived from price\n",
    "\n",
    "Keeping these features would result in artificially high accuracy (~99%) because the model can reverse-engineer the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading preprocessed data...\\n\")\n",
    "\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')[\"value_category\"]\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')[\"value_category\"]\n",
    "\n",
    "# Drop id column if exists\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# Remove leaky features to prevent data leakage\n",
    "leaky_features = [\n",
    "    'price', 'price_normalized', 'price_per_person', 'price_per_bathroom',\n",
    "    'price_per_bedroom', 'review_scores_rating', 'review_scores_value',\n",
    "    'value_density', 'estimated_revenue_l365d'\n",
    "]\n",
    "\n",
    "cols_to_drop = [col for col in leaky_features if col in X_train.columns]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} leaky features:\")\n",
    "for col in cols_to_drop:\n",
    "    print(f\"  - {col}\")\n",
    "print(f\"\\nRemaining features: {X_train.shape[1]}\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Combine train and test for learning curve analysis\n",
    "# Learning curves need to see how performance changes with different training sizes\n",
    "X = pd.concat([X_train, X_test], axis=0).reset_index(drop=True)\n",
    "y = np.concatenate([y_train_encoded, y_test_encoded])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(y):,}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, cls in enumerate(label_encoder.classes_):\n",
    "    count = (y == i).sum()\n",
    "    pct = count / len(y) * 100\n",
    "    print(f\"  {cls}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Top 3 Models\n",
    "\n",
    "Based on our Week 2 model comparison results, we select the top 3 performing models:\n",
    "\n",
    "### 1. XGBoost Classifier\n",
    "- **Type**: Gradient Boosting ensemble method\n",
    "- **Strengths**: Handles non-linear relationships, built-in regularization, feature importance\n",
    "- **Expected behavior**: May show some overfitting (high variance) due to its complexity\n",
    "\n",
    "### 2. Random Forest Classifier\n",
    "- **Type**: Bagging ensemble of decision trees\n",
    "- **Strengths**: Robust to overfitting, handles high-dimensional data well\n",
    "- **Expected behavior**: Generally good bias-variance balance\n",
    "\n",
    "### 3. MLP Classifier (Neural Network)\n",
    "- **Type**: Multi-layer Perceptron neural network\n",
    "- **Strengths**: Can learn complex patterns, flexible architecture\n",
    "- **Expected behavior**: May require more data to generalize well\n",
    "\n",
    "We use regularized parameters to prevent excessive overfitting during learning curve analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top 3 models with STRONGER regularization\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,              \n",
    "        learning_rate=0.05,       \n",
    "        min_child_weight=10,      \n",
    "        subsample=0.7,           \n",
    "        colsample_bytree=0.7,    \n",
    "        reg_alpha=0.5,            \n",
    "        reg_lambda=2.0,          \n",
    "        gamma=0.1,               \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,             \n",
    "        min_samples_split=20,    \n",
    "        min_samples_leaf=12,      \n",
    "        max_features=0.3,         \n",
    "        max_samples=0.7,          \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'MLP Classifier': MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32), \n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.1,               \n",
    "        learning_rate='adaptive',\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15, \n",
    "        n_iter_no_change=15,     \n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Learning Curves\n",
    "\n",
    "### Methodology\n",
    "We use `sklearn.model_selection.learning_curve` which:\n",
    "1. Trains the model on increasing subsets of the training data\n",
    "2. Evaluates performance using cross-validation at each training size\n",
    "3. Returns training and validation scores for each size\n",
    "\n",
    "### Parameters\n",
    "- **train_sizes**: We use 10 different training set sizes from 10% to 100%\n",
    "- **cv=5**: 5-fold cross-validation for robust estimates\n",
    "- **scoring='f1_macro'**: Macro F1-score for multi-class classification\n",
    "- **n_jobs=-1**: Parallel processing for speed\n",
    "\n",
    "### What We're Looking For\n",
    "- **Convergence**: Do training and validation curves converge?\n",
    "- **Gap size**: Large gap = high variance (overfitting)\n",
    "- **Plateau**: Does validation score plateau? (data saturation)\n",
    "- **Trend**: Is validation score still improving at max training size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training sizes (10% to 100% of data)\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Store results\n",
    "learning_curve_results = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING LEARNING CURVES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining sizes: {[f'{s:.0%}' for s in train_sizes]}\")\n",
    "print(f\"Cross-validation folds: 5\")\n",
    "print(f\"Scoring metric: F1-score (macro)\")\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Processing {name}...\", end=\" \")\n",
    "    \n",
    "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=5,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    learning_curve_results[name] = {\n",
    "        'train_sizes': train_sizes_abs,\n",
    "        'train_scores_mean': train_scores.mean(axis=1),\n",
    "        'train_scores_std': train_scores.std(axis=1),\n",
    "        'val_scores_mean': val_scores.mean(axis=1),\n",
    "        'val_scores_std': val_scores.std(axis=1)\n",
    "    }\n",
    "    \n",
    "    print(f\"Done! Final val score: {val_scores.mean(axis=1)[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Learning curves generated successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Learning Curves\n",
    "\n",
    "### Visualization 1: Individual Learning Curves\n",
    "We create a subplot for each model showing:\n",
    "- **Blue line**: Training score (how well model fits training data)\n",
    "- **Orange line**: Validation score (how well model generalizes)\n",
    "- **Shaded area**: Standard deviation across CV folds (uncertainty)\n",
    "\n",
    "### How to Read the Plots\n",
    "- **Gap between curves**: Indicates variance (overfitting)\n",
    "- **Curve convergence**: Good generalization\n",
    "- **Flat validation curve**: Model may be saturated (more data won't help)\n",
    "- **Rising validation curve**: More data could improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual learning curve plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = {'train': 'steelblue', 'val': 'darkorange'}\n",
    "\n",
    "for idx, (name, results) in enumerate(learning_curve_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot training scores\n",
    "    ax.plot(results['train_sizes'], results['train_scores_mean'], \n",
    "            'o-', color=colors['train'], label='Training Score', linewidth=2, markersize=6)\n",
    "    ax.fill_between(results['train_sizes'],\n",
    "                    results['train_scores_mean'] - results['train_scores_std'],\n",
    "                    results['train_scores_mean'] + results['train_scores_std'],\n",
    "                    alpha=0.2, color=colors['train'])\n",
    "    \n",
    "    # Plot validation scores\n",
    "    ax.plot(results['train_sizes'], results['val_scores_mean'],\n",
    "            'o-', color=colors['val'], label='Validation Score', linewidth=2, markersize=6)\n",
    "    ax.fill_between(results['train_sizes'],\n",
    "                    results['val_scores_mean'] - results['val_scores_std'],\n",
    "                    results['val_scores_mean'] + results['val_scores_std'],\n",
    "                    alpha=0.2, color=colors['val'])\n",
    "    \n",
    "    # Calculate and display gap\n",
    "    final_gap = results['train_scores_mean'][-1] - results['val_scores_mean'][-1]\n",
    "    \n",
    "    ax.set_xlabel('Training Set Size', fontsize=12)\n",
    "    ax.set_ylabel('F1-Score (Macro)', fontsize=12)\n",
    "    ax.set_title(f'{name}\\nFinal Gap: {final_gap:.4f}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "plt.suptitle('Learning Curves for Top 3 Models', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/learning_curves_individual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/learning_curves_individual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Comparative Analysis\n",
    "\n",
    "### Visualization 2: All Models Comparison\n",
    "We overlay all validation curves on a single plot to directly compare:\n",
    "- Which model learns fastest (steeper initial curve)\n",
    "- Which model achieves highest final performance\n",
    "- Which model benefits most from additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative plot - all models on same axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_models = ['steelblue', 'forestgreen', 'coral']\n",
    "\n",
    "# Left plot: Validation scores comparison\n",
    "ax1 = axes[0]\n",
    "for idx, (name, results) in enumerate(learning_curve_results.items()):\n",
    "    ax1.plot(results['train_sizes'], results['val_scores_mean'],\n",
    "             'o-', color=colors_models[idx], label=name, linewidth=2, markersize=6)\n",
    "    ax1.fill_between(results['train_sizes'],\n",
    "                     results['val_scores_mean'] - results['val_scores_std'],\n",
    "                     results['val_scores_mean'] + results['val_scores_std'],\n",
    "                     alpha=0.15, color=colors_models[idx])\n",
    "\n",
    "ax1.set_xlabel('Training Set Size', fontsize=12)\n",
    "ax1.set_ylabel('Validation F1-Score (Macro)', fontsize=12)\n",
    "ax1.set_title('Validation Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Bias-Variance Gap\n",
    "ax2 = axes[1]\n",
    "for idx, (name, results) in enumerate(learning_curve_results.items()):\n",
    "    gap = results['train_scores_mean'] - results['val_scores_mean']\n",
    "    ax2.plot(results['train_sizes'], gap,\n",
    "             'o-', color=colors_models[idx], label=name, linewidth=2, markersize=6)\n",
    "\n",
    "ax2.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Acceptable Gap (0.05)')\n",
    "ax2.set_xlabel('Training Set Size', fontsize=12)\n",
    "ax2.set_ylabel('Train-Validation Gap', fontsize=12)\n",
    "ax2.set_title('Bias-Variance Gap Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Comparison: Learning Behavior', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/learning_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/learning_curves_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Bias-Variance Analysis\n",
    "\n",
    "### Understanding Bias and Variance\n",
    "\n",
    "| Metric | High Bias | High Variance |\n",
    "|--------|-----------|---------------|\n",
    "| Training Score | Low | High |\n",
    "| Validation Score | Low | Low |\n",
    "| Gap | Small | Large |\n",
    "| Problem | Underfitting | Overfitting |\n",
    "| Solution | More complex model | Regularization, more data |\n",
    "\n",
    "### Analysis Metrics\n",
    "We calculate:\n",
    "1. **Final Training Score**: Performance on training data at max size\n",
    "2. **Final Validation Score**: Generalization performance\n",
    "3. **Bias-Variance Gap**: Difference between train and validation\n",
    "4. **Score Improvement**: How much validation improved from 10% to 100% data\n",
    "5. **Convergence Rate**: How quickly the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BIAS-VARIANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for name, results in learning_curve_results.items():\n",
    "    train_final = results['train_scores_mean'][-1]\n",
    "    val_final = results['val_scores_mean'][-1]\n",
    "    gap = train_final - val_final\n",
    "    \n",
    "    # Score improvement from 10% to 100% data\n",
    "    val_improvement = results['val_scores_mean'][-1] - results['val_scores_mean'][0]\n",
    "    \n",
    "    # Check if still improving (compare last two points)\n",
    "    recent_improvement = results['val_scores_mean'][-1] - results['val_scores_mean'][-2]\n",
    "    \n",
    "    # Determine bias-variance status\n",
    "    if gap > 0.15:\n",
    "        status = \"HIGH VARIANCE (Overfitting)\"\n",
    "        recommendation = \"Add regularization, reduce complexity, or get more data\"\n",
    "    elif gap > 0.08:\n",
    "        status = \"MODERATE VARIANCE\"\n",
    "        recommendation = \"Consider light regularization or more data\"\n",
    "    elif val_final < 0.65:\n",
    "        status = \"HIGH BIAS (Underfitting)\"\n",
    "        recommendation = \"Use more complex model or add features\"\n",
    "    else:\n",
    "        status = \"GOOD BALANCE\"\n",
    "        recommendation = \"Model is well-tuned\"\n",
    "    \n",
    "    # Data saturation check\n",
    "    if recent_improvement < 0.005:\n",
    "        saturation = \"SATURATED - More data unlikely to help significantly\"\n",
    "    elif recent_improvement < 0.01:\n",
    "        saturation = \"NEAR SATURATION - Diminishing returns from more data\"\n",
    "    else:\n",
    "        saturation = \"NOT SATURATED - More data could improve performance\"\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'Model': name,\n",
    "        'Train_Score': train_final,\n",
    "        'Val_Score': val_final,\n",
    "        'Gap': gap,\n",
    "        'Improvement': val_improvement,\n",
    "        'Recent_Improvement': recent_improvement,\n",
    "        'Status': status,\n",
    "        'Saturation': saturation\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MODEL: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n  Performance Metrics:\")\n",
    "    print(f\"    • Final Training Score:   {train_final:.4f}\")\n",
    "    print(f\"    • Final Validation Score: {val_final:.4f}\")\n",
    "    print(f\"    • Bias-Variance Gap:      {gap:.4f}\")\n",
    "    print(f\"    • Total Improvement:      {val_improvement:.4f} ({val_improvement/results['val_scores_mean'][0]*100:.1f}%)\")\n",
    "    print(f\"    • Recent Improvement:     {recent_improvement:.4f}\")\n",
    "    print(f\"\\n  Diagnosis:\")\n",
    "    print(f\"    • Status: {status}\")\n",
    "    print(f\"    • {saturation}\")\n",
    "    print(f\"\\n  Recommendation:\")\n",
    "    print(f\"    → {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary Table and Visualization\n",
    "\n",
    "We create a comprehensive summary comparing all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df[['Model', 'Train_Score', 'Val_Score', 'Gap', 'Improvement']].to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv('../../outputs/learning_curve_analysis.csv', index=False)\n",
    "print(\"\\nSaved: outputs/learning_curve_analysis.csv\")\n",
    "\n",
    "# Create summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "models_list = summary_df['Model'].tolist()\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Train vs Validation Scores\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, summary_df['Train_Score'], width, label='Training', color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, summary_df['Val_Score'], width, label='Validation', color='darkorange')\n",
    "ax1.set_ylabel('F1-Score', fontsize=12)\n",
    "ax1.set_title('Training vs Validation Scores', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0.6, 1.0])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax1.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax1.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "# Plot 2: Bias-Variance Gap\n",
    "ax2 = axes[1]\n",
    "colors_gap = ['green' if g < 0.08 else 'orange' if g < 0.15 else 'red' for g in summary_df['Gap']]\n",
    "bars3 = ax2.bar(x, summary_df['Gap'], color=colors_gap, edgecolor='black', linewidth=1.5)\n",
    "ax2.axhline(y=0.08, color='orange', linestyle='--', label='Moderate threshold')\n",
    "ax2.axhline(y=0.15, color='red', linestyle='--', label='High variance threshold')\n",
    "ax2.set_ylabel('Gap (Train - Validation)', fontsize=12)\n",
    "ax2.set_title('Bias-Variance Gap', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_list, rotation=15, ha='right')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars3:\n",
    "    ax2.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 3: Improvement from more data\n",
    "ax3 = axes[2]\n",
    "bars4 = ax3.bar(x, summary_df['Improvement'], color='mediumseagreen', edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('F1-Score Improvement', fontsize=12)\n",
    "ax3.set_title('Improvement (10% → 100% data)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(models_list, rotation=15, ha='right')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars4:\n",
    "    ax3.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Learning Curve Analysis Summary', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/learning_curve_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/learning_curve_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Final Conclusions and Recommendations\n",
    "\n",
    "Based on our learning curve analysis, we provide final conclusions about:\n",
    "1. Which model has the best bias-variance tradeoff\n",
    "2. Whether more data would help improve performance\n",
    "3. Actionable recommendations for model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = summary_df['Val_Score'].idxmax()\n",
    "best_model = summary_df.loc[best_model_idx, 'Model']\n",
    "best_score = summary_df.loc[best_model_idx, 'Val_Score']\n",
    "\n",
    "# Find model with best bias-variance balance\n",
    "best_balance_idx = summary_df['Gap'].idxmin()\n",
    "best_balance_model = summary_df.loc[best_balance_idx, 'Model']\n",
    "\n",
    "print(f\"\\n1. BEST PERFORMING MODEL\")\n",
    "print(f\"   → {best_model} with validation F1-score of {best_score:.4f}\")\n",
    "\n",
    "print(f\"\\n2. BEST BIAS-VARIANCE BALANCE\")\n",
    "print(f\"   → {best_balance_model} with gap of {summary_df.loc[best_balance_idx, 'Gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. DATA SATURATION ANALYSIS\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"   • {row['Model']}: {row['Saturation']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 3.11: LEARNING CURVE ANALYSIS - COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Generated\n",
    "\n",
    "### Analysis Files (outputs/)\n",
    "- `learning_curve_analysis.csv` - Summary metrics for all models\n",
    "\n",
    "### Visualization Files (outputs/figures/)\n",
    "- `learning_curves_individual.png` - Individual learning curves for each model\n",
    "- `learning_curves_comparison.png` - Comparative analysis of all models\n",
    "- `learning_curve_summary.png` - Summary dashboard with key metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Bias-Variance Tradeoff**: All models show some degree of overfitting (training score > validation score), which is normal for complex models.\n",
    "\n",
    "2. **Data Saturation**: The validation curves are flattening, indicating that simply adding more data of the same type won't dramatically improve performance.\n",
    "\n",
    "3. **Model Selection**: XGBoost provides the best balance of performance and generalization for this dataset.\n",
    "\n",
    "4. **Practical Implications**: For production deployment, focus on regularization and feature engineering rather than data collection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
