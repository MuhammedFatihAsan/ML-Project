{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 3.7: Bayesian Optimization for XGBoost\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to use **Bayesian Optimization** (via Optuna) to find optimal XGBoost hyperparameters. We will compare the results with the GridSearchCV approach used in Week 2 to demonstrate the advantages of Bayesian optimization.\n",
    "\n",
    "## Why Bayesian Optimization?\n",
    "\n",
    "In Week 2, we used **GridSearchCV** which exhaustively searches through a predefined grid of hyperparameters. While effective, this approach has limitations:\n",
    "\n",
    "| Aspect | GridSearchCV | Bayesian Optimization |\n",
    "|--------|--------------|----------------------|\n",
    "| Search Strategy | Exhaustive (tries all combinations) | Intelligent (learns from previous trials) |\n",
    "| Efficiency | O(n^k) where k = number of parameters | Typically finds optimum in fewer trials |\n",
    "| Continuous Parameters | Must discretize | Handles continuous ranges naturally |\n",
    "| Adaptability | Fixed grid | Adapts search based on results |\n",
    "| Scalability | Poor for large search spaces | Excellent for large search spaces |\n",
    "\n",
    "## How Bayesian Optimization Works\n",
    "\n",
    "1. **Surrogate Model:** Builds a probabilistic model of the objective function (e.g., Gaussian Process or Tree-structured Parzen Estimator)\n",
    "2. **Acquisition Function:** Decides which hyperparameters to try next based on:\n",
    "   - **Exploitation:** Areas where the model predicts good performance\n",
    "   - **Exploration:** Areas with high uncertainty\n",
    "3. **Iterative Improvement:** Each trial updates the surrogate model, making future predictions more accurate\n",
    "\n",
    "## Optuna Framework\n",
    "\n",
    "We use **Optuna** because:\n",
    "- State-of-the-art Bayesian optimization library\n",
    "- Uses Tree-structured Parzen Estimator (TPE) by default\n",
    "- Supports pruning (early stopping of unpromising trials)\n",
    "- Easy integration with XGBoost\n",
    "- Excellent visualization tools\n",
    "\n",
    "## Week 2 XGBoost Results (Baseline)\n",
    "\n",
    "From Task 2.3, our XGBoost model achieved:\n",
    "- **Test Accuracy:** 99.40%\n",
    "- **F1-Score (Macro):** 99.40%\n",
    "\n",
    "Our goal is to see if Bayesian optimization can match or improve these results more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Loading\n",
    "\n",
    "### Libraries Used:\n",
    "- **optuna:** Bayesian optimization framework\n",
    "- **xgboost:** XGBoost classifier\n",
    "- **sklearn:** For metrics and cross-validation\n",
    "- **matplotlib:** For visualization\n",
    "\n",
    "### Installation Note:\n",
    "If Optuna is not installed, run: `pip install optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optuna if not available\n",
    "# !pip install optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "# Remove ID columns if present\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train['value_category'])\n",
    "y_test_encoded = label_encoder.transform(y_test['value_category'])\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget classes: {label_encoder.classes_}\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val} ({label_encoder.classes_[val]}): {count} ({count/len(y_train_encoded)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Load Week 2 XGBoost Results (Baseline)\n",
    "\n",
    "Before running Bayesian optimization, let's load our Week 2 results to establish a baseline for comparison.\n",
    "\n",
    "### Week 2 Hyperparameters (from Task 2.3):\n",
    "- learning_rate = 0.1\n",
    "- max_depth = 6\n",
    "- n_estimators = 200\n",
    "- subsample = 0.8\n",
    "- colsample_bytree = 0.8\n",
    "\n",
    "These were manually selected based on common best practices. Bayesian optimization will search a wider range to potentially find better values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Week 2 XGBoost results\n",
    "try:\n",
    "    week2_results = pd.read_csv('../../data/processed/xgboost_results.csv')\n",
    "    print(\"=\"*60)\n",
    "    print(\"WEEK 2 XGBOOST RESULTS (BASELINE)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTraining Accuracy:  {week2_results['train_accuracy'].values[0]:.4f}\")\n",
    "    print(f\"Testing Accuracy:   {week2_results['test_accuracy'].values[0]:.4f}\")\n",
    "    print(f\"Precision (Macro):  {week2_results['precision_macro'].values[0]:.4f}\")\n",
    "    print(f\"Recall (Macro):     {week2_results['recall_macro'].values[0]:.4f}\")\n",
    "    print(f\"F1-Score (Macro):   {week2_results['f1_macro'].values[0]:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    baseline_accuracy = week2_results['test_accuracy'].values[0]\n",
    "    baseline_f1 = week2_results['f1_macro'].values[0]\n",
    "except FileNotFoundError:\n",
    "    print(\"Week 2 results not found. Using default baseline.\")\n",
    "    baseline_accuracy = 0.994\n",
    "    baseline_f1 = 0.994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Define Optuna Objective Function\n",
    "\n",
    "The objective function is the heart of Bayesian optimization. It:\n",
    "1. Receives hyperparameters suggested by Optuna\n",
    "2. Trains an XGBoost model with those parameters\n",
    "3. Returns a score (we want to maximize accuracy)\n",
    "\n",
    "### Hyperparameter Search Space:\n",
    "\n",
    "| Parameter | Range | Type | Description |\n",
    "|-----------|-------|------|-------------|\n",
    "| n_estimators | 100-500 | int | Number of boosting rounds |\n",
    "| max_depth | 3-12 | int | Maximum tree depth |\n",
    "| learning_rate | 0.01-0.3 | float (log) | Step size shrinkage |\n",
    "| subsample | 0.5-1.0 | float | Row sampling ratio |\n",
    "| colsample_bytree | 0.5-1.0 | float | Column sampling ratio |\n",
    "| min_child_weight | 1-10 | int | Minimum sum of instance weight in child |\n",
    "| gamma | 0-5 | float | Minimum loss reduction for split |\n",
    "| reg_alpha | 1e-8 to 10 | float (log) | L1 regularization |\n",
    "| reg_lambda | 1e-8 to 10 | float (log) | L2 regularization |\n",
    "\n",
    "### Why These Ranges?\n",
    "\n",
    "- **n_estimators (100-500):** Week 2 used 200; we explore wider range\n",
    "- **max_depth (3-12):** Week 2 used 6; deeper trees may capture more patterns\n",
    "- **learning_rate (0.01-0.3):** Log scale because small changes matter more at low values\n",
    "- **subsample/colsample (0.5-1.0):** Standard range for regularization\n",
    "- **min_child_weight (1-10):** Controls overfitting; higher = more conservative\n",
    "- **gamma (0-5):** Pruning parameter; higher = more pruning\n",
    "- **reg_alpha/reg_lambda:** L1/L2 regularization; log scale for wide range\n",
    "\n",
    "### Cross-Validation Strategy:\n",
    "\n",
    "We use **5-fold Stratified Cross-Validation** to:\n",
    "- Get robust performance estimates\n",
    "- Maintain class distribution in each fold\n",
    "- Reduce variance in optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization.\n",
    "    \n",
    "    This function:\n",
    "    1. Suggests hyperparameters from defined ranges\n",
    "    2. Creates an XGBoost model with those parameters\n",
    "    3. Evaluates using 5-fold cross-validation\n",
    "    4. Returns mean accuracy (to be maximized)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10, log=True),\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    # 5-fold stratified cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train_encoded, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "print(\"Objective function defined successfully!\")\n",
    "print(\"\\nSearch space summary:\")\n",
    "print(\"  - n_estimators: [100, 500]\")\n",
    "print(\"  - max_depth: [3, 12]\")\n",
    "print(\"  - learning_rate: [0.01, 0.3] (log scale)\")\n",
    "print(\"  - subsample: [0.5, 1.0]\")\n",
    "print(\"  - colsample_bytree: [0.5, 1.0]\")\n",
    "print(\"  - min_child_weight: [1, 10]\")\n",
    "print(\"  - gamma: [0, 5]\")\n",
    "print(\"  - reg_alpha: [1e-8, 10] (log scale)\")\n",
    "print(\"  - reg_lambda: [1e-8, 10] (log scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Run Bayesian Optimization\n",
    "\n",
    "Now we run the optimization process. Optuna will:\n",
    "1. Start with random exploration\n",
    "2. Build a surrogate model of the objective function\n",
    "3. Use TPE (Tree-structured Parzen Estimator) to suggest promising hyperparameters\n",
    "4. Iteratively improve until we reach the trial limit\n",
    "\n",
    "### Configuration:\n",
    "- **n_trials = 50:** Number of different hyperparameter combinations to try\n",
    "- **sampler = TPESampler:** Tree-structured Parzen Estimator (default, most effective)\n",
    "- **direction = 'maximize':** We want to maximize accuracy\n",
    "\n",
    "### Why 50 Trials?\n",
    "\n",
    "- Enough to explore the search space effectively\n",
    "- Bayesian optimization typically converges within 30-50 trials\n",
    "- Balances computation time with optimization quality\n",
    "- Can be increased for potentially better results\n",
    "\n",
    "### Expected Runtime:\n",
    "Approximately 10-20 minutes depending on hardware (each trial involves 5-fold CV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNumber of trials: 50\")\n",
    "print(f\"Optimization metric: Accuracy (maximize)\")\n",
    "print(f\"Cross-validation: 5-fold Stratified\")\n",
    "print(f\"\\nThis may take 10-20 minutes...\\n\")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create and run study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    study_name='xgboost_bayesian_optimization'\n",
    ")\n",
    "\n",
    "# Run optimization with progress callback\n",
    "def callback(study, trial):\n",
    "    if (trial.number + 1) % 10 == 0:\n",
    "        print(f\"  Trial {trial.number + 1}/50 completed. Best so far: {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(objective, n_trials=50, callbacks=[callback], show_progress_bar=False)\n",
    "\n",
    "# Record end time\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal optimization time: {optimization_time/60:.2f} minutes\")\n",
    "print(f\"Best cross-validation accuracy: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Optimization Results\n",
    "\n",
    "Let's examine the best hyperparameters found by Bayesian optimization and understand why they might be optimal.\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Best Parameters:** Compare with Week 2 manual selection\n",
    "2. **Parameter Importance:** Which hyperparameters had the most impact?\n",
    "3. **Convergence:** Did the optimization converge to a stable solution?\n",
    "4. **Trial History:** How did performance improve over trials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "print(\"=\"*60)\n",
    "print(\"BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"\\nOptimal hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Week 2 vs Bayesian Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "week2_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 1\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Parameter':<20} {'Week 2':<15} {'Bayesian Opt':<15}\")\n",
    "print(\"-\"*50)\n",
    "for param in week2_params.keys():\n",
    "    w2_val = week2_params[param]\n",
    "    bo_val = best_params.get(param, 'N/A')\n",
    "    if isinstance(bo_val, float):\n",
    "        print(f\"{param:<20} {w2_val:<15} {bo_val:<15.4f}\")\n",
    "    else:\n",
    "        print(f\"{param:<20} {w2_val:<15} {bo_val:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Train Final Model with Best Parameters\n",
    "\n",
    "Now we train the final XGBoost model using the optimal hyperparameters found by Bayesian optimization.\n",
    "\n",
    "### Why Retrain?\n",
    "\n",
    "During optimization, we used cross-validation which trains on subsets of data. Now we:\n",
    "1. Train on the **full training set** for maximum learning\n",
    "2. Evaluate on the **held-out test set** for unbiased performance estimate\n",
    "3. Save the final model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best parameters\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING FINAL MODEL WITH OPTIMAL PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_params = {\n",
    "    **best_params,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Train model\n",
    "best_model = XGBClassifier(**final_params)\n",
    "best_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
    "test_acc = accuracy_score(y_test_encoded, y_test_pred)\n",
    "test_precision = precision_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test_encoded, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test_encoded, y_test_pred, average='macro')\n",
    "\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"  Training Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"  Testing Accuracy:   {test_acc:.4f}\")\n",
    "print(f\"  Precision (Macro):  {test_precision:.4f}\")\n",
    "print(f\"  Recall (Macro):     {test_recall:.4f}\")\n",
    "print(f\"  F1-Score (Macro):   {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_encoded, y_test_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Week 2 Results\n",
    "\n",
    "This is the key comparison: Did Bayesian optimization improve upon our Week 2 GridSearchCV results?\n",
    "\n",
    "### Comparison Metrics:\n",
    "\n",
    "1. **Performance:** Test accuracy and F1-score\n",
    "2. **Efficiency:** Number of trials/combinations evaluated\n",
    "3. **Search Space:** Range of parameters explored\n",
    "\n",
    "### Expected Outcomes:\n",
    "\n",
    "- **Similar or slightly better performance:** Week 2 already achieved 99.4% accuracy\n",
    "- **Much better efficiency:** Bayesian optimization explores intelligently vs exhaustively\n",
    "- **Wider search space:** Found parameters we might not have tried manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE COMPARISON: Week 2 vs Bayesian Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Metric':<25} {'Week 2 (GridSearch)':<20} {'Bayesian Opt':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Training Accuracy':<25} {baseline_accuracy:.4f}{'':<14} {train_acc:.4f}\")\n",
    "print(f\"{'Testing Accuracy':<25} {baseline_accuracy:.4f}{'':<14} {test_acc:.4f}\")\n",
    "print(f\"{'F1-Score (Macro)':<25} {baseline_f1:.4f}{'':<14} {test_f1:.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "acc_improvement = (test_acc - baseline_accuracy) * 100\n",
    "f1_improvement = (test_f1 - baseline_f1) * 100\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"IMPROVEMENT ANALYSIS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Accuracy change: {acc_improvement:+.2f}%\")\n",
    "print(f\"F1-Score change: {f1_improvement:+.2f}%\")\n",
    "\n",
    "if acc_improvement > 0:\n",
    "    print(\"\\n Bayesian Optimization IMPROVED performance!\")\n",
    "elif acc_improvement == 0:\n",
    "    print(\"\\n Bayesian Optimization achieved EQUAL performance.\")\n",
    "else:\n",
    "    print(\"\\n Bayesian Optimization did not improve performance.\")\n",
    "    print(\"   This can happen when the baseline is already near-optimal.\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EFFICIENCY COMPARISON\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Week 2 GridSearchCV: Exhaustive search (all combinations)\")\n",
    "print(f\"Bayesian Optimization: 50 intelligent trials\")\n",
    "print(f\"Optimization time: {optimization_time/60:.2f} minutes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 8: Visualization - Optimization History\n",
    "\n",
    "Visualizing the optimization process helps us understand:\n",
    "1. How quickly the algorithm found good solutions\n",
    "2. Whether it converged or was still improving\n",
    "3. The distribution of trial performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Optimization History\n",
    "ax1 = axes[0, 0]\n",
    "trials_df = study.trials_dataframe()\n",
    "ax1.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.5, label='Trial Accuracy')\n",
    "ax1.scatter(trials_df['number'], trials_df['value'], c='blue', alpha=0.6, s=30)\n",
    "\n",
    "# Best value line\n",
    "best_values = [max(trials_df['value'][:i+1]) for i in range(len(trials_df))]\n",
    "ax1.plot(trials_df['number'], best_values, 'r-', linewidth=2, label='Best So Far')\n",
    "ax1.axhline(y=baseline_accuracy, color='green', linestyle='--', label=f'Week 2 Baseline ({baseline_accuracy:.4f})')\n",
    "\n",
    "ax1.set_xlabel('Trial Number', fontsize=11)\n",
    "ax1.set_ylabel('Cross-Validation Accuracy', fontsize=11)\n",
    "ax1.set_title('Optimization History', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Parameter Importance\n",
    "ax2 = axes[0, 1]\n",
    "param_importance = optuna.importance.get_param_importances(study)\n",
    "params_sorted = sorted(param_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "param_names = [p[0] for p in params_sorted]\n",
    "param_values = [p[1] for p in params_sorted]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(param_names)))\n",
    "bars = ax2.barh(param_names, param_values, color=colors)\n",
    "ax2.set_xlabel('Importance', fontsize=11)\n",
    "ax2.set_title('Hyperparameter Importance', fontsize=12, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. Trial Value Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(trials_df['value'], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=study.best_value, color='red', linestyle='--', linewidth=2, label=f'Best: {study.best_value:.4f}')\n",
    "ax3.axvline(x=baseline_accuracy, color='green', linestyle='--', linewidth=2, label=f'Week 2: {baseline_accuracy:.4f}')\n",
    "ax3.set_xlabel('Cross-Validation Accuracy', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('Distribution of Trial Accuracies', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Comparison Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "methods = ['Week 2\\n(GridSearch)', 'Week 3\\n(Bayesian Opt)']\n",
    "accuracies = [baseline_accuracy, test_acc]\n",
    "f1_scores = [baseline_f1, test_f1]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, accuracies, width, label='Accuracy', color='steelblue')\n",
    "bars2 = ax4.bar(x + width/2, f1_scores, width, label='F1-Score', color='coral')\n",
    "\n",
    "ax4.set_ylabel('Score', fontsize=11)\n",
    "ax4.set_title('Performance Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(methods)\n",
    "ax4.legend()\n",
    "ax4.set_ylim(0.9, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax4.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax4.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/bayesian_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: ../../outputs/figures/bayesian_optimization_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 9: Save Results and Model\n",
    "\n",
    "We save all important outputs for documentation and future use:\n",
    "\n",
    "1. **Optimized model** - For deployment\n",
    "2. **Best parameters** - For reproducibility\n",
    "3. **Trial history** - For analysis\n",
    "4. **Comparison results** - For reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "with open('../../models/xgboost_bayesian_optimized.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save best parameters\n",
    "best_params_df = pd.DataFrame([best_params])\n",
    "best_params_df.to_csv('../../outputs/bayesian_best_params.csv', index=False)\n",
    "\n",
    "# Save trial history\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df.to_csv('../../outputs/bayesian_optimization_trials.csv', index=False)\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Week 2 (GridSearch)', 'Week 3 (Bayesian Optimization)'],\n",
    "    'Test_Accuracy': [baseline_accuracy, test_acc],\n",
    "    'F1_Score': [baseline_f1, test_f1],\n",
    "    'n_estimators': [200, best_params['n_estimators']],\n",
    "    'max_depth': [6, best_params['max_depth']],\n",
    "    'learning_rate': [0.1, best_params['learning_rate']],\n",
    "    'subsample': [0.8, best_params['subsample']],\n",
    "    'colsample_bytree': [0.8, best_params['colsample_bytree']]\n",
    "})\n",
    "comparison_df.to_csv('../../outputs/bayesian_vs_gridsearch_comparison.csv', index=False)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred\n",
    "})\n",
    "predictions_df.to_csv('../../data/processed/xgboost_bayesian_predictions.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModel:\")\n",
    "print(\"  - ../../models/xgboost_bayesian_optimized.pkl\")\n",
    "print(\"\\nResults:\")\n",
    "print(\"  - ../../outputs/bayesian_best_params.csv\")\n",
    "print(\"  - ../../outputs/bayesian_optimization_trials.csv\")\n",
    "print(\"  - ../../outputs/bayesian_vs_gridsearch_comparison.csv\")\n",
    "print(\"  - ../../data/processed/xgboost_bayesian_predictions.csv\")\n",
    "print(\"\\nVisualization:\")\n",
    "print(\"  - ../../outputs/figures/bayesian_optimization_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this task, we implemented **Bayesian Optimization using Optuna** to find optimal XGBoost hyperparameters and compared the results with Week 2's GridSearchCV approach.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Performance:** Bayesian optimization achieved comparable or improved results compared to Week 2\n",
    "2. **Efficiency:** Explored a much larger search space with only 50 intelligent trials\n",
    "3. **Parameter Insights:** Identified which hyperparameters have the most impact on model performance\n",
    "\n",
    "### Advantages of Bayesian Optimization\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Intelligent Search** | Learns from previous trials to focus on promising regions |\n",
    "| **Efficiency** | Finds good solutions with fewer evaluations |\n",
    "| **Continuous Parameters** | Handles continuous ranges naturally (no discretization needed) |\n",
    "| **Scalability** | Works well even with many hyperparameters |\n",
    "| **Uncertainty Quantification** | Balances exploration and exploitation |\n",
    "\n",
    "### When to Use Bayesian Optimization\n",
    "\n",
    "- Large hyperparameter search spaces\n",
    "- Expensive model training (each evaluation is costly)\n",
    "- Continuous hyperparameters\n",
    "- When GridSearchCV is too slow\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- May not significantly improve already near-optimal models (like our 99.4% baseline)\n",
    "- Requires more setup than simple GridSearchCV\n",
    "- Results can vary between runs (though we use seeds for reproducibility)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply similar optimization to Random Forest (Task 3.8)\n",
    "- Use the optimized model for SHAP analysis (Task 3.13)\n",
    "- Include these results in the final report (Task 3.18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
