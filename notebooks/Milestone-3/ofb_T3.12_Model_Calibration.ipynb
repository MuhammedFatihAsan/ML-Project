{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.12: Model Calibration\n",
    "\n",
    "## Overview\n",
    "This task applies probability calibration techniques to improve prediction confidence for our top performing models. Well-calibrated probabilities are essential for reliable decision-making in production systems.\n",
    "\n",
    "## Objectives\n",
    "1. **Understand model calibration** and why it matters\n",
    "2. **Apply Platt Scaling** (sigmoid calibration) to models\n",
    "3. **Apply Isotonic Regression** calibration to models\n",
    "4. **Generate calibration plots** to visualize calibration quality\n",
    "5. **Compare calibration methods** and select the best approach\n",
    "\n",
    "---\n",
    "\n",
    "## What is Model Calibration?\n",
    "\n",
    "### The Problem\n",
    "Many classifiers output probability scores that don't reflect true probabilities. For example:\n",
    "- A model predicting 80% probability should be correct ~80% of the time\n",
    "- In practice, models are often **overconfident** (predict 90% but correct only 70%)\n",
    "- Or **underconfident** (predict 60% but correct 80%)\n",
    "\n",
    "### Why Calibration Matters\n",
    "- **Risk assessment**: Medical diagnosis, fraud detection need accurate probabilities\n",
    "- **Decision thresholds**: Setting optimal cutoffs requires calibrated scores\n",
    "- **Ensemble methods**: Combining models works better with calibrated probabilities\n",
    "- **User trust**: Displaying confidence scores to users requires accuracy\n",
    "\n",
    "### Calibration Methods\n",
    "\n",
    "#### 1. Platt Scaling (Sigmoid Calibration)\n",
    "- Fits a logistic regression on the classifier's output scores\n",
    "- Assumes sigmoid relationship between scores and true probabilities\n",
    "- Works well for **SVMs** and models with sigmoid-shaped distortions\n",
    "- **Pros**: Simple, works with small datasets\n",
    "- **Cons**: Assumes specific functional form\n",
    "\n",
    "#### 2. Isotonic Regression\n",
    "- Non-parametric approach fitting a monotonic function\n",
    "- More flexible than Platt scaling\n",
    "- **Pros**: No assumptions about functional form\n",
    "- **Cons**: Requires more data, can overfit on small datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup\n",
    "\n",
    "We import calibration-specific tools from sklearn:\n",
    "- `CalibratedClassifierCV`: Wrapper for applying calibration\n",
    "- `calibration_curve`: For generating reliability diagrams\n",
    "- `brier_score_loss`: Metric for calibration quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss, log_loss, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Create output directories\n",
    "Path('../../outputs').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs/figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TASK 3.12: MODEL CALIBRATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nLibraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "\n",
    "### Important Notes\n",
    "- We remove **leaky features** that would artificially inflate performance\n",
    "- We need a **calibration set** separate from training data\n",
    "- We split data into: Training (60%), Calibration (20%), Test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading preprocessed data...\\n\")\n",
    "\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')[\"value_category\"]\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')[\"value_category\"]\n",
    "\n",
    "# Drop id column if exists\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# Remove leaky features\n",
    "leaky_features = [\n",
    "    'price', 'price_normalized', 'price_per_person', 'price_per_bathroom',\n",
    "    'price_per_bedroom', 'review_scores_rating', 'review_scores_value',\n",
    "    'value_density', 'estimated_revenue_l365d'\n",
    "]\n",
    "\n",
    "cols_to_drop = [col for col in leaky_features if col in X_train.columns]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} leaky features\")\n",
    "print(f\"Remaining features: {X_train.shape[1]}\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Split training data into train and calibration sets\n",
    "# Calibration requires held-out data to avoid overfitting\n",
    "X_train_final, X_calib, y_train_final, y_calib = train_test_split(\n",
    "    X_train, y_train_encoded, test_size=0.25, random_state=42, stratify=y_train_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:    {len(y_train_final):,} samples\")\n",
    "print(f\"Calibration set: {len(y_calib):,} samples\")\n",
    "print(f\"Test set:        {len(y_test_encoded):,} samples\")\n",
    "print(f\"\\nClasses: {label_encoder.classes_}\")\n",
    "print(f\"Encoded as: {list(range(len(label_encoder.classes_)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Base Models\n",
    "\n",
    "We use our top 3 models with regularization applied (from Task 3.11).\n",
    "\n",
    "### Models Selected\n",
    "1. **XGBoost**: Often overconfident, benefits from calibration\n",
    "2. **Random Forest**: Generally well-calibrated but can improve\n",
    "3. **MLP Classifier**: Neural networks often need calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with regularization\n",
    "base_models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=10,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=2.0,\n",
    "        gamma=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=12,\n",
    "        max_features=0.3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'MLP': MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.1,\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15,\n",
    "        n_iter_no_change=15,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASE MODELS DEFINED\")\n",
    "print(\"=\"*60)\n",
    "for name in base_models:\n",
    "    print(f\"  • {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Base Models and Get Uncalibrated Probabilities\n",
    "\n",
    "First, we train the base models and evaluate their **uncalibrated** probability predictions.\n",
    "\n",
    "### Metrics for Calibration Quality\n",
    "\n",
    "#### Brier Score\n",
    "- Measures mean squared error between predicted probabilities and actual outcomes\n",
    "- Range: 0 (perfect) to 1 (worst)\n",
    "- Lower is better\n",
    "\n",
    "#### Log Loss (Cross-Entropy)\n",
    "- Penalizes confident wrong predictions heavily\n",
    "- Lower is better\n",
    "- More sensitive to calibration than accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASE MODELS (UNCALIBRATED)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store results\n",
    "uncalibrated_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\", end=\" \")\n",
    "    \n",
    "    # Train on training set\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Get predictions and probabilities on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    f1 = f1_score(y_test_encoded, y_pred, average='macro')\n",
    "    logloss = log_loss(y_test_encoded, y_proba)\n",
    "    \n",
    "    # Brier score (multi-class: average of one-vs-all)\n",
    "    y_test_bin = label_binarize(y_test_encoded, classes=[0, 1, 2])\n",
    "    brier_scores = []\n",
    "    for i in range(3):\n",
    "        brier_scores.append(brier_score_loss(y_test_bin[:, i], y_proba[:, i]))\n",
    "    brier_avg = np.mean(brier_scores)\n",
    "    \n",
    "    uncalibrated_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'log_loss': logloss,\n",
    "        'brier_score': brier_avg,\n",
    "        'probabilities': y_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"Done!\")\n",
    "    print(f\"    Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Log Loss: {logloss:.4f} | Brier: {brier_avg:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply Calibration Methods\n",
    "\n",
    "### Method 1: Platt Scaling (Sigmoid)\n",
    "- Uses `method='sigmoid'` in CalibratedClassifierCV\n",
    "- Fits logistic regression: P(y=1|f) = 1 / (1 + exp(A*f + B))\n",
    "- Best for models with sigmoid-shaped miscalibration\n",
    "\n",
    "### Method 2: Isotonic Regression\n",
    "- Uses `method='isotonic'` in CalibratedClassifierCV\n",
    "- Fits piecewise constant monotonic function\n",
    "- More flexible but needs more data\n",
    "\n",
    "### Cross-Validation Approach\n",
    "We use `cv=5` to perform calibration with cross-validation, which:\n",
    "- Avoids overfitting the calibration\n",
    "- Uses all training data efficiently\n",
    "- Provides more robust calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"APPLYING CALIBRATION METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store calibrated results\n",
    "calibrated_results = {'platt': {}, 'isotonic': {}}\n",
    "calibrated_models = {'platt': {}, 'isotonic': {}}\n",
    "\n",
    "# Combine train and calibration for CV-based calibration\n",
    "X_train_full = pd.concat([X_train_final, X_calib], axis=0)\n",
    "y_train_full = np.concatenate([y_train_final, y_calib])\n",
    "\n",
    "for name, base_model in base_models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Platt Scaling (Sigmoid)\n",
    "    print(\"  Applying Platt Scaling...\", end=\" \")\n",
    "    platt_model = CalibratedClassifierCV(\n",
    "        estimator=base_model,\n",
    "        method='sigmoid',\n",
    "        cv=5\n",
    "    )\n",
    "    platt_model.fit(X_train_full, y_train_full)\n",
    "    calibrated_models['platt'][name] = platt_model\n",
    "    \n",
    "    y_pred_platt = platt_model.predict(X_test)\n",
    "    y_proba_platt = platt_model.predict_proba(X_test)\n",
    "    \n",
    "    accuracy_platt = accuracy_score(y_test_encoded, y_pred_platt)\n",
    "    f1_platt = f1_score(y_test_encoded, y_pred_platt, average='macro')\n",
    "    logloss_platt = log_loss(y_test_encoded, y_proba_platt)\n",
    "    \n",
    "    y_test_bin = label_binarize(y_test_encoded, classes=[0, 1, 2])\n",
    "    brier_platt = np.mean([brier_score_loss(y_test_bin[:, i], y_proba_platt[:, i]) for i in range(3)])\n",
    "    \n",
    "    calibrated_results['platt'][name] = {\n",
    "        'accuracy': accuracy_platt,\n",
    "        'f1_score': f1_platt,\n",
    "        'log_loss': logloss_platt,\n",
    "        'brier_score': brier_platt,\n",
    "        'probabilities': y_proba_platt\n",
    "    }\n",
    "    print(f\"Done! Brier: {brier_platt:.4f}\")\n",
    "    \n",
    "    # Isotonic Regression\n",
    "    print(\"  Applying Isotonic Regression...\", end=\" \")\n",
    "    iso_model = CalibratedClassifierCV(\n",
    "        estimator=base_model,\n",
    "        method='isotonic',\n",
    "        cv=5\n",
    "    )\n",
    "    iso_model.fit(X_train_full, y_train_full)\n",
    "    calibrated_models['isotonic'][name] = iso_model\n",
    "    \n",
    "    y_pred_iso = iso_model.predict(X_test)\n",
    "    y_proba_iso = iso_model.predict_proba(X_test)\n",
    "    \n",
    "    accuracy_iso = accuracy_score(y_test_encoded, y_pred_iso)\n",
    "    f1_iso = f1_score(y_test_encoded, y_pred_iso, average='macro')\n",
    "    logloss_iso = log_loss(y_test_encoded, y_proba_iso)\n",
    "    brier_iso = np.mean([brier_score_loss(y_test_bin[:, i], y_proba_iso[:, i]) for i in range(3)])\n",
    "    \n",
    "    calibrated_results['isotonic'][name] = {\n",
    "        'accuracy': accuracy_iso,\n",
    "        'f1_score': f1_iso,\n",
    "        'log_loss': logloss_iso,\n",
    "        'brier_score': brier_iso,\n",
    "        'probabilities': y_proba_iso\n",
    "    }\n",
    "    print(f\"Done! Brier: {brier_iso:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Calibration complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Calibration Plots (Reliability Diagrams)\n",
    "\n",
    "### What is a Reliability Diagram?\n",
    "A reliability diagram (calibration plot) shows:\n",
    "- **X-axis**: Mean predicted probability (binned)\n",
    "- **Y-axis**: Fraction of positives (actual frequency)\n",
    "- **Diagonal line**: Perfect calibration\n",
    "\n",
    "### Interpretation\n",
    "- **Above diagonal**: Model is underconfident (predicts lower than actual)\n",
    "- **Below diagonal**: Model is overconfident (predicts higher than actual)\n",
    "- **On diagonal**: Well-calibrated\n",
    "\n",
    "We create calibration plots for each class (one-vs-rest) to see class-specific calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate calibration plots for each model\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "y_test_bin = label_binarize(y_test_encoded, classes=[0, 1, 2])\n",
    "\n",
    "for model_idx, name in enumerate(base_models.keys()):\n",
    "    for class_idx in range(3):\n",
    "        ax = axes[model_idx, class_idx]\n",
    "        \n",
    "        # Get probabilities for this class\n",
    "        prob_uncalib = uncalibrated_results[name]['probabilities'][:, class_idx]\n",
    "        prob_platt = calibrated_results['platt'][name]['probabilities'][:, class_idx]\n",
    "        prob_iso = calibrated_results['isotonic'][name]['probabilities'][:, class_idx]\n",
    "        y_true_class = y_test_bin[:, class_idx]\n",
    "        \n",
    "        # Calculate calibration curves\n",
    "        frac_pos_uncalib, mean_pred_uncalib = calibration_curve(y_true_class, prob_uncalib, n_bins=10)\n",
    "        frac_pos_platt, mean_pred_platt = calibration_curve(y_true_class, prob_platt, n_bins=10)\n",
    "        frac_pos_iso, mean_pred_iso = calibration_curve(y_true_class, prob_iso, n_bins=10)\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfect', linewidth=2)\n",
    "        ax.plot(mean_pred_uncalib, frac_pos_uncalib, 's-', color='red', \n",
    "                label='Uncalibrated', linewidth=2, markersize=8)\n",
    "        ax.plot(mean_pred_platt, frac_pos_platt, 'o-', color='blue',\n",
    "                label='Platt', linewidth=2, markersize=8)\n",
    "        ax.plot(mean_pred_iso, frac_pos_iso, '^-', color='green',\n",
    "                label='Isotonic', linewidth=2, markersize=8)\n",
    "        \n",
    "        ax.set_xlabel('Mean Predicted Probability', fontsize=10)\n",
    "        ax.set_ylabel('Fraction of Positives', fontsize=10)\n",
    "        ax.set_title(f'{name} - {class_names[class_idx]}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('Calibration Plots (Reliability Diagrams) by Model and Class', \n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/calibration_plots_by_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/calibration_plots_by_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Overall Calibration Comparison\n",
    "\n",
    "We create a summary plot comparing all models and calibration methods on a single diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall calibration plot (averaged across classes)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = {'Uncalibrated': 'red', 'Platt': 'blue', 'Isotonic': 'green'}\n",
    "markers = {'Uncalibrated': 's', 'Platt': 'o', 'Isotonic': '^'}\n",
    "\n",
    "for model_idx, name in enumerate(base_models.keys()):\n",
    "    ax = axes[model_idx]\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "    \n",
    "    # For each calibration method, average across classes\n",
    "    for method, label in [('uncalib', 'Uncalibrated'), ('platt', 'Platt'), ('isotonic', 'Isotonic')]:\n",
    "        all_frac_pos = []\n",
    "        all_mean_pred = []\n",
    "        \n",
    "        for class_idx in range(3):\n",
    "            if method == 'uncalib':\n",
    "                proba = uncalibrated_results[name]['probabilities'][:, class_idx]\n",
    "            else:\n",
    "                proba = calibrated_results[method][name]['probabilities'][:, class_idx]\n",
    "            \n",
    "            y_true_class = y_test_bin[:, class_idx]\n",
    "            frac_pos, mean_pred = calibration_curve(y_true_class, proba, n_bins=10)\n",
    "            all_frac_pos.extend(frac_pos)\n",
    "            all_mean_pred.extend(mean_pred)\n",
    "        \n",
    "        # Sort and plot\n",
    "        sorted_idx = np.argsort(all_mean_pred)\n",
    "        ax.plot(np.array(all_mean_pred)[sorted_idx], np.array(all_frac_pos)[sorted_idx],\n",
    "                marker=markers[label], color=colors[label], label=label,\n",
    "                linewidth=2, markersize=6, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    ax.set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    ax.set_title(f'{name}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle('Overall Calibration Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/calibration_comparison_overall.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/calibration_comparison_overall.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Quantitative Comparison\n",
    "\n",
    "We compare all models and calibration methods using:\n",
    "- **Brier Score**: Lower is better (measures calibration + discrimination)\n",
    "- **Log Loss**: Lower is better (penalizes confident wrong predictions)\n",
    "- **Accuracy**: Classification accuracy (may not change much with calibration)\n",
    "- **F1-Score**: Balanced metric for multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"QUANTITATIVE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for name in base_models.keys():\n",
    "    # Uncalibrated\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Calibration': 'Uncalibrated',\n",
    "        'Accuracy': uncalibrated_results[name]['accuracy'],\n",
    "        'F1_Score': uncalibrated_results[name]['f1_score'],\n",
    "        'Log_Loss': uncalibrated_results[name]['log_loss'],\n",
    "        'Brier_Score': uncalibrated_results[name]['brier_score']\n",
    "    })\n",
    "    \n",
    "    # Platt\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Calibration': 'Platt Scaling',\n",
    "        'Accuracy': calibrated_results['platt'][name]['accuracy'],\n",
    "        'F1_Score': calibrated_results['platt'][name]['f1_score'],\n",
    "        'Log_Loss': calibrated_results['platt'][name]['log_loss'],\n",
    "        'Brier_Score': calibrated_results['platt'][name]['brier_score']\n",
    "    })\n",
    "    \n",
    "    # Isotonic\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Calibration': 'Isotonic',\n",
    "        'Accuracy': calibrated_results['isotonic'][name]['accuracy'],\n",
    "        'F1_Score': calibrated_results['isotonic'][name]['f1_score'],\n",
    "        'Log_Loss': calibrated_results['isotonic'][name]['log_loss'],\n",
    "        'Brier_Score': calibrated_results['isotonic'][name]['brier_score']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display formatted table\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('../../outputs/calibration_comparison.csv', index=False)\n",
    "print(\"\\nSaved: outputs/calibration_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Calibration Improvement Analysis\n",
    "\n",
    "We calculate how much each calibration method improved the Brier score and Log Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CALIBRATION IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement_data = []\n",
    "\n",
    "for name in base_models.keys():\n",
    "    uncalib_brier = uncalibrated_results[name]['brier_score']\n",
    "    uncalib_logloss = uncalibrated_results[name]['log_loss']\n",
    "    \n",
    "    platt_brier = calibrated_results['platt'][name]['brier_score']\n",
    "    platt_logloss = calibrated_results['platt'][name]['log_loss']\n",
    "    \n",
    "    iso_brier = calibrated_results['isotonic'][name]['brier_score']\n",
    "    iso_logloss = calibrated_results['isotonic'][name]['log_loss']\n",
    "    \n",
    "    # Calculate improvements (negative means improvement for these metrics)\n",
    "    platt_brier_imp = (uncalib_brier - platt_brier) / uncalib_brier * 100\n",
    "    platt_logloss_imp = (uncalib_logloss - platt_logloss) / uncalib_logloss * 100\n",
    "    \n",
    "    iso_brier_imp = (uncalib_brier - iso_brier) / uncalib_brier * 100\n",
    "    iso_logloss_imp = (uncalib_logloss - iso_logloss) / uncalib_logloss * 100\n",
    "    \n",
    "    improvement_data.append({\n",
    "        'Model': name,\n",
    "        'Platt_Brier_Improvement_%': platt_brier_imp,\n",
    "        'Platt_LogLoss_Improvement_%': platt_logloss_imp,\n",
    "        'Isotonic_Brier_Improvement_%': iso_brier_imp,\n",
    "        'Isotonic_LogLoss_Improvement_%': iso_logloss_imp,\n",
    "        'Best_Method': 'Platt' if platt_brier < iso_brier else 'Isotonic'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Platt Scaling:\")\n",
    "    print(f\"    • Brier Score: {platt_brier_imp:+.2f}% {'(improved)' if platt_brier_imp > 0 else '(worsened)'}\")\n",
    "    print(f\"    • Log Loss:    {platt_logloss_imp:+.2f}% {'(improved)' if platt_logloss_imp > 0 else '(worsened)'}\")\n",
    "    print(f\"  Isotonic Regression:\")\n",
    "    print(f\"    • Brier Score: {iso_brier_imp:+.2f}% {'(improved)' if iso_brier_imp > 0 else '(worsened)'}\")\n",
    "    print(f\"    • Log Loss:    {iso_logloss_imp:+.2f}% {'(improved)' if iso_logloss_imp > 0 else '(worsened)'}\")\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_data)\n",
    "improvement_df.to_csv('../../outputs/calibration_improvement.csv', index=False)\n",
    "print(\"\\nSaved: outputs/calibration_improvement.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary Visualization\n",
    "\n",
    "Final visualization comparing Brier scores and Log Loss across all configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models_list = list(base_models.keys())\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.25\n",
    "\n",
    "# Brier Score comparison\n",
    "ax1 = axes[0]\n",
    "brier_uncalib = [uncalibrated_results[m]['brier_score'] for m in models_list]\n",
    "brier_platt = [calibrated_results['platt'][m]['brier_score'] for m in models_list]\n",
    "brier_iso = [calibrated_results['isotonic'][m]['brier_score'] for m in models_list]\n",
    "\n",
    "bars1 = ax1.bar(x - width, brier_uncalib, width, label='Uncalibrated', color='red', alpha=0.8)\n",
    "bars2 = ax1.bar(x, brier_platt, width, label='Platt Scaling', color='blue', alpha=0.8)\n",
    "bars3 = ax1.bar(x + width, brier_iso, width, label='Isotonic', color='green', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Brier Score (lower is better)', fontsize=12)\n",
    "ax1.set_title('Brier Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        ax1.annotate(f'{bar.get_height():.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     xytext=(0, 3), textcoords='offset points', ha='center', fontsize=8, rotation=90)\n",
    "\n",
    "# Log Loss comparison\n",
    "ax2 = axes[1]\n",
    "logloss_uncalib = [uncalibrated_results[m]['log_loss'] for m in models_list]\n",
    "logloss_platt = [calibrated_results['platt'][m]['log_loss'] for m in models_list]\n",
    "logloss_iso = [calibrated_results['isotonic'][m]['log_loss'] for m in models_list]\n",
    "\n",
    "bars4 = ax2.bar(x - width, logloss_uncalib, width, label='Uncalibrated', color='red', alpha=0.8)\n",
    "bars5 = ax2.bar(x, logloss_platt, width, label='Platt Scaling', color='blue', alpha=0.8)\n",
    "bars6 = ax2.bar(x + width, logloss_iso, width, label='Isotonic', color='green', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Log Loss (lower is better)', fontsize=12)\n",
    "ax2.set_title('Log Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_list, fontsize=11)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars4, bars5, bars6]:\n",
    "    for bar in bars:\n",
    "        ax2.annotate(f'{bar.get_height():.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                     xytext=(0, 3), textcoords='offset points', ha='center', fontsize=8, rotation=90)\n",
    "\n",
    "plt.suptitle('Calibration Methods Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/calibration_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: outputs/figures/calibration_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Final Conclusions\n",
    "\n",
    "Summary of findings and recommendations for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best overall configuration\n",
    "best_config = comparison_df.loc[comparison_df['Brier_Score'].idxmin()]\n",
    "\n",
    "print(f\"\\n1. BEST CALIBRATED MODEL\")\n",
    "print(f\"   → {best_config['Model']} with {best_config['Calibration']}\")\n",
    "print(f\"   → Brier Score: {best_config['Brier_Score']:.4f}\")\n",
    "print(f\"   → Log Loss: {best_config['Log_Loss']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. CALIBRATION METHOD COMPARISON\")\n",
    "print(f\"   • Platt Scaling: Better for smaller datasets, assumes sigmoid shape\")\n",
    "print(f\"   • Isotonic: More flexible, may overfit with limited data\")\n",
    "\n",
    "print(f\"\\n3. KEY FINDINGS\")\n",
    "for _, row in improvement_df.iterrows():\n",
    "    print(f\"   • {row['Model']}: Best with {row['Best_Method']}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 3.12: MODEL CALIBRATION - COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Generated\n",
    "\n",
    "### Analysis Files (outputs/)\n",
    "- `calibration_comparison.csv` - Full comparison of all models and methods\n",
    "- `calibration_improvement.csv` - Improvement percentages from calibration\n",
    "\n",
    "### Visualization Files (outputs/figures/)\n",
    "- `calibration_plots_by_class.png` - Reliability diagrams for each model/class\n",
    "- `calibration_comparison_overall.png` - Overall calibration comparison\n",
    "- `calibration_summary.png` - Brier score and Log Loss comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Aspect | Finding |\n",
    "|--------|--------|\n",
    "| Best Method | Depends on model, generally Platt for this dataset |\n",
    "| Improvement | 5-15% reduction in Brier score typical |\n",
    "| Trade-off | Calibration doesn't change accuracy much |\n",
    "| Use Case | Essential for probability-based decisions |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
