{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 3.10: Feature Selection Experiments\n",
    "\n",
    "## Objective\n",
    "\n",
    "Test different feature selection methods to find the optimal feature subset:\n",
    "1. **Recursive Feature Elimination (RFE)**\n",
    "2. **SelectKBest** (with different scoring functions)\n",
    "3. **Feature Importance Thresholding** (from tree-based models)\n",
    "\n",
    "## Why Feature Selection?\n",
    "\n",
    "- **Reduce overfitting:** Fewer features = simpler model\n",
    "- **Improve performance:** Remove noisy/irrelevant features\n",
    "- **Faster training:** Less computation with fewer features\n",
    "- **Better interpretability:** Easier to understand model decisions\n",
    "\n",
    "## Current Dataset\n",
    "\n",
    "From Week 1 feature selection, we have 28 features. Let's see if we can find an even better subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "# Remove ID columns if present\n",
    "id_cols = ['id', 'host_id', 'listing_id']\n",
    "for col in id_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train = X_train.drop(col, axis=1)\n",
    "        X_test = X_test.drop(col, axis=1)\n",
    "\n",
    "        # Remove leaky features\n",
    "leaky_features = [\n",
    "    'price', 'price_normalized', 'price_per_person', 'price_per_bathroom',\n",
    "    'price_per_bedroom', 'review_scores_rating', 'review_scores_value',\n",
    "    'value_density', 'estimated_revenue_l365d'\n",
    "]\n",
    "\n",
    "cols_to_drop = [col for col in leaky_features if col in X_train.columns]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} leaky features: {cols_to_drop}\")\n",
    "print(f\"Remaining features: {X_train.shape[1]}\")\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train['value_category'])\n",
    "y_test_enc = le.transform(y_test['value_category'])\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nOriginal features ({len(X_train.columns)}):\")\n",
    "print(list(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Baseline Performance (All Features)\n",
    "\n",
    "First, let's establish baseline performance using all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline with all features\n",
    "rf_baseline = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "baseline_scores = cross_val_score(rf_baseline, X_train, y_train_enc, cv=cv, scoring='f1_macro')\n",
    "baseline_f1 = baseline_scores.mean()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE PERFORMANCE (All Features)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"CV F1-Score: {baseline_f1:.4f} (+/- {baseline_scores.std():.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Method 1 - Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE works by:\n",
    "1. Training a model on all features\n",
    "2. Ranking features by importance\n",
    "3. Removing the least important feature(s)\n",
    "4. Repeating until desired number of features reached\n",
    "\n",
    "We'll use RFECV to automatically find the optimal number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"METHOD 1: RECURSIVE FEATURE ELIMINATION (RFECV)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running RFECV... (this may take a few minutes)\\n\")\n",
    "\n",
    "# Use RF as estimator for RFE\n",
    "rf_rfe = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_rfe,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=5\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train_enc)\n",
    "\n",
    "# Results\n",
    "rfe_n_features = rfecv.n_features_\n",
    "rfe_features = X_train.columns[rfecv.support_].tolist()\n",
    "rfe_scores = rfecv.cv_results_['mean_test_score']\n",
    "\n",
    "print(f\"Optimal number of features: {rfe_n_features}\")\n",
    "print(f\"Best CV F1-Score: {max(rfe_scores):.4f}\")\n",
    "print(f\"\\nSelected features ({rfe_n_features}):\")\n",
    "for i, feat in enumerate(rfe_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RFE results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(5, len(rfe_scores) + 5), rfe_scores, 'b-o', markersize=4)\n",
    "plt.axvline(x=rfe_n_features, color='r', linestyle='--', label=f'Optimal: {rfe_n_features} features')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('CV F1-Score (Macro)')\n",
    "plt.title('RFECV: Number of Features vs Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/rfe_feature_selection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Method 2 - SelectKBest\n",
    "\n",
    "SelectKBest selects features based on univariate statistical tests:\n",
    "- **f_classif:** ANOVA F-value (assumes linear relationship)\n",
    "- **mutual_info_classif:** Mutual information (captures non-linear relationships)\n",
    "\n",
    "We'll test both and find optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"METHOD 2: SELECTKBEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different K values\n",
    "k_values = range(5, X_train.shape[1] + 1, 2)\n",
    "results_anova = []\n",
    "results_mi = []\n",
    "\n",
    "rf_eval = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "print(\"Testing different K values...\\n\")\n",
    "\n",
    "for k in k_values:\n",
    "    # ANOVA F-value\n",
    "    selector_anova = SelectKBest(f_classif, k=k)\n",
    "    X_anova = selector_anova.fit_transform(X_train, y_train_enc)\n",
    "    scores_anova = cross_val_score(rf_eval, X_anova, y_train_enc, cv=cv, scoring='f1_macro')\n",
    "    results_anova.append({'k': k, 'f1_mean': scores_anova.mean(), 'f1_std': scores_anova.std()})\n",
    "    \n",
    "    # Mutual Information\n",
    "    selector_mi = SelectKBest(mutual_info_classif, k=k)\n",
    "    X_mi = selector_mi.fit_transform(X_train, y_train_enc)\n",
    "    scores_mi = cross_val_score(rf_eval, X_mi, y_train_enc, cv=cv, scoring='f1_macro')\n",
    "    results_mi.append({'k': k, 'f1_mean': scores_mi.mean(), 'f1_std': scores_mi.std()})\n",
    "\n",
    "results_anova_df = pd.DataFrame(results_anova)\n",
    "results_mi_df = pd.DataFrame(results_mi)\n",
    "\n",
    "# Best K for each method\n",
    "best_k_anova = results_anova_df.loc[results_anova_df['f1_mean'].idxmax()]\n",
    "best_k_mi = results_mi_df.loc[results_mi_df['f1_mean'].idxmax()]\n",
    "\n",
    "print(f\"ANOVA F-value: Best K = {int(best_k_anova['k'])}, F1 = {best_k_anova['f1_mean']:.4f}\")\n",
    "print(f\"Mutual Info:   Best K = {int(best_k_mi['k'])}, F1 = {best_k_mi['f1_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selected features from best SelectKBest\n",
    "best_k = int(best_k_anova['k']) if best_k_anova['f1_mean'] >= best_k_mi['f1_mean'] else int(best_k_mi['k'])\n",
    "best_method = 'ANOVA' if best_k_anova['f1_mean'] >= best_k_mi['f1_mean'] else 'Mutual Info'\n",
    "\n",
    "if best_method == 'ANOVA':\n",
    "    selector_best = SelectKBest(f_classif, k=best_k)\n",
    "else:\n",
    "    selector_best = SelectKBest(mutual_info_classif, k=best_k)\n",
    "\n",
    "selector_best.fit(X_train, y_train_enc)\n",
    "skb_features = X_train.columns[selector_best.get_support()].tolist()\n",
    "\n",
    "print(f\"\\nBest SelectKBest: {best_method} with K={best_k}\")\n",
    "print(f\"\\nSelected features ({best_k}):\")\n",
    "for i, feat in enumerate(skb_features, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SelectKBest results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_anova_df['k'], results_anova_df['f1_mean'], 'b-o', label='ANOVA F-value', markersize=4)\n",
    "plt.plot(results_mi_df['k'], results_mi_df['f1_mean'], 'g-s', label='Mutual Information', markersize=4)\n",
    "plt.axhline(y=baseline_f1, color='r', linestyle='--', label=f'Baseline ({baseline_f1:.4f})')\n",
    "plt.xlabel('Number of Features (K)')\n",
    "plt.ylabel('CV F1-Score (Macro)')\n",
    "plt.title('SelectKBest: K vs Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/selectkbest_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5: Method 3 - Feature Importance Thresholding\n",
    "\n",
    "Use feature importances from Random Forest and apply different thresholds to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"METHOD 3: FEATURE IMPORTANCE THRESHOLDING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train RF to get feature importances\n",
    "rf_imp = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_imp.fit(X_train, y_train_enc)\n",
    "\n",
    "# Get importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_imp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances (Top 15):\")\n",
    "print(importances.head(15).to_string(index=False))\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05]\n",
    "threshold_results = []\n",
    "\n",
    "print(\"\\nTesting different importance thresholds...\")\n",
    "\n",
    "for thresh in thresholds:\n",
    "    selected = importances[importances['importance'] >= thresh]['feature'].tolist()\n",
    "    if len(selected) >= 3:  # Need at least 3 features\n",
    "        X_thresh = X_train[selected]\n",
    "        scores = cross_val_score(rf_eval, X_thresh, y_train_enc, cv=cv, scoring='f1_macro')\n",
    "        threshold_results.append({\n",
    "            'threshold': thresh,\n",
    "            'n_features': len(selected),\n",
    "            'f1_mean': scores.mean(),\n",
    "            'f1_std': scores.std()\n",
    "        })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"\\n\" + threshold_df.to_string(index=False))\n",
    "\n",
    "# Best threshold\n",
    "best_thresh_row = threshold_df.loc[threshold_df['f1_mean'].idxmax()]\n",
    "best_threshold = best_thresh_row['threshold']\n",
    "thresh_features = importances[importances['importance'] >= best_threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nBest threshold: {best_threshold}\")\n",
    "print(f\"Number of features: {len(thresh_features)}\")\n",
    "print(f\"F1-Score: {best_thresh_row['f1_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold results\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax1.set_xlabel('Importance Threshold')\n",
    "ax1.set_ylabel('CV F1-Score', color='blue')\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['f1_mean'], 'b-o', label='F1-Score')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.axhline(y=baseline_f1, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Number of Features', color='green')\n",
    "ax2.plot(threshold_df['threshold'], threshold_df['n_features'], 'g-s', label='# Features')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.title('Feature Importance Thresholding')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/importance_thresholding.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 6: Compare All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARISON OF ALL FEATURE SELECTION METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate each method on test set\n",
    "methods_comparison = []\n",
    "\n",
    "# 1. Baseline (all features)\n",
    "rf_final = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_final.fit(X_train, y_train_enc)\n",
    "y_pred = rf_final.predict(X_test)\n",
    "methods_comparison.append({\n",
    "    'Method': 'Baseline (All Features)',\n",
    "    'N_Features': X_train.shape[1],\n",
    "    'CV_F1': baseline_f1,\n",
    "    'Test_F1': f1_score(y_test_enc, y_pred, average='macro'),\n",
    "    'Test_Accuracy': accuracy_score(y_test_enc, y_pred)\n",
    "})\n",
    "\n",
    "# 2. RFE\n",
    "X_train_rfe = X_train[rfe_features]\n",
    "X_test_rfe = X_test[rfe_features]\n",
    "rf_final.fit(X_train_rfe, y_train_enc)\n",
    "y_pred = rf_final.predict(X_test_rfe)\n",
    "methods_comparison.append({\n",
    "    'Method': 'RFE',\n",
    "    'N_Features': len(rfe_features),\n",
    "    'CV_F1': max(rfe_scores),\n",
    "    'Test_F1': f1_score(y_test_enc, y_pred, average='macro'),\n",
    "    'Test_Accuracy': accuracy_score(y_test_enc, y_pred)\n",
    "})\n",
    "\n",
    "# 3. SelectKBest\n",
    "X_train_skb = X_train[skb_features]\n",
    "X_test_skb = X_test[skb_features]\n",
    "rf_final.fit(X_train_skb, y_train_enc)\n",
    "y_pred = rf_final.predict(X_test_skb)\n",
    "methods_comparison.append({\n",
    "    'Method': f'SelectKBest ({best_method})',\n",
    "    'N_Features': len(skb_features),\n",
    "    'CV_F1': best_k_anova['f1_mean'] if best_method == 'ANOVA' else best_k_mi['f1_mean'],\n",
    "    'Test_F1': f1_score(y_test_enc, y_pred, average='macro'),\n",
    "    'Test_Accuracy': accuracy_score(y_test_enc, y_pred)\n",
    "})\n",
    "\n",
    "# 4. Importance Thresholding\n",
    "X_train_thresh = X_train[thresh_features]\n",
    "X_test_thresh = X_test[thresh_features]\n",
    "rf_final.fit(X_train_thresh, y_train_enc)\n",
    "y_pred = rf_final.predict(X_test_thresh)\n",
    "methods_comparison.append({\n",
    "    'Method': 'Importance Threshold',\n",
    "    'N_Features': len(thresh_features),\n",
    "    'CV_F1': best_thresh_row['f1_mean'],\n",
    "    'Test_F1': f1_score(y_test_enc, y_pred, average='macro'),\n",
    "    'Test_Accuracy': accuracy_score(y_test_enc, y_pred)\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(methods_comparison)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best method\n",
    "best_method_row = comparison_df.loc[comparison_df['Test_F1'].idxmax()]\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"BEST METHOD: {best_method_row['Method']}\")\n",
    "print(f\"Features: {best_method_row['N_Features']}, Test F1: {best_method_row['Test_F1']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization comparing all methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "ax1 = axes[0]\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar([i - width/2 for i in x], comparison_df['Test_F1'], width, label='Test F1', color='steelblue')\n",
    "bars2 = ax1.bar([i + width/2 for i in x], comparison_df['Test_Accuracy'], width, label='Test Accuracy', color='coral')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(comparison_df['Method'], rotation=20, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0.7, 0.8])\n",
    "for bar in bars1:\n",
    "    ax1.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontsize=8)\n",
    "\n",
    "# Features vs Performance scatter\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(comparison_df['N_Features'], comparison_df['Test_F1'], s=150, c='steelblue', alpha=0.7)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    ax2.annotate(row['Method'].split('(')[0].strip(), (row['N_Features'], row['Test_F1']),\n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Test F1-Score')\n",
    "ax2.set_title('Features vs Performance Trade-off')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/feature_selection_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Step 7: Determine Optimal Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features selected by multiple methods (consensus)\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SELECTION CONSENSUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_selected = {\n",
    "    'RFE': set(rfe_features),\n",
    "    'SelectKBest': set(skb_features),\n",
    "    'Importance': set(thresh_features)\n",
    "}\n",
    "\n",
    "# Features selected by all methods\n",
    "consensus_all = all_selected['RFE'] & all_selected['SelectKBest'] & all_selected['Importance']\n",
    "# Features selected by at least 2 methods\n",
    "consensus_2 = (all_selected['RFE'] & all_selected['SelectKBest']) | \\\n",
    "              (all_selected['RFE'] & all_selected['Importance']) | \\\n",
    "              (all_selected['SelectKBest'] & all_selected['Importance'])\n",
    "\n",
    "print(f\"\\nFeatures selected by ALL methods ({len(consensus_all)}):\")\n",
    "for feat in sorted(consensus_all):\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nFeatures selected by at least 2 methods ({len(consensus_2)}):\")\n",
    "for feat in sorted(consensus_2):\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test consensus features\n",
    "print(\"\\nEvaluating consensus feature sets...\")\n",
    "\n",
    "# Consensus (2+ methods)\n",
    "consensus_features = list(consensus_2)\n",
    "X_train_cons = X_train[consensus_features]\n",
    "X_test_cons = X_test[consensus_features]\n",
    "\n",
    "rf_final.fit(X_train_cons, y_train_enc)\n",
    "y_pred_cons = rf_final.predict(X_test_cons)\n",
    "cons_f1 = f1_score(y_test_enc, y_pred_cons, average='macro')\n",
    "cons_acc = accuracy_score(y_test_enc, y_pred_cons)\n",
    "\n",
    "print(f\"\\nConsensus Features (2+ methods): {len(consensus_features)} features\")\n",
    "print(f\"Test F1-Score: {cons_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {cons_acc:.4f}\")\n",
    "\n",
    "# Determine final optimal subset\n",
    "all_results = comparison_df.copy()\n",
    "all_results = pd.concat([all_results, pd.DataFrame([{\n",
    "    'Method': 'Consensus (2+ methods)',\n",
    "    'N_Features': len(consensus_features),\n",
    "    'CV_F1': np.nan,\n",
    "    'Test_F1': cons_f1,\n",
    "    'Test_Accuracy': cons_acc\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Best overall\n",
    "best_overall = all_results.loc[all_results['Test_F1'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMAL FEATURE SUBSET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Method: {best_overall['Method']}\")\n",
    "print(f\"Number of Features: {int(best_overall['N_Features'])}\")\n",
    "print(f\"Test F1-Score: {best_overall['Test_F1']:.4f}\")\n",
    "print(f\"Test Accuracy: {best_overall['Test_Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "all_results.to_csv('../../outputs/feature_selection_comparison.csv', index=False)\n",
    "\n",
    "# Save selected features from each method\n",
    "features_dict = {\n",
    "    'RFE': rfe_features,\n",
    "    'SelectKBest': skb_features,\n",
    "    'Importance_Threshold': thresh_features,\n",
    "    'Consensus': consensus_features\n",
    "}\n",
    "\n",
    "# Pad lists to same length for DataFrame\n",
    "max_len = max(len(v) for v in features_dict.values())\n",
    "for k in features_dict:\n",
    "    features_dict[k] = features_dict[k] + [''] * (max_len - len(features_dict[k]))\n",
    "\n",
    "features_df = pd.DataFrame(features_dict)\n",
    "features_df.to_csv('../../outputs/selected_features_by_method.csv', index=False)\n",
    "\n",
    "# Save SelectKBest results\n",
    "selectkbest_results = pd.concat([results_anova_df.assign(method='ANOVA'), \n",
    "                                  results_mi_df.assign(method='Mutual_Info')])\n",
    "selectkbest_results.to_csv('../../outputs/selectkbest_results.csv', index=False)\n",
    "\n",
    "# Save threshold results\n",
    "threshold_df.to_csv('../../outputs/importance_threshold_results.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(\"- outputs/feature_selection_comparison.csv\")\n",
    "print(\"- outputs/selected_features_by_method.csv\")\n",
    "print(\"- outputs/selectkbest_results.csv\")\n",
    "print(\"- outputs/importance_threshold_results.csv\")\n",
    "print(\"- outputs/figures/rfe_feature_selection.png\")\n",
    "print(\"- outputs/figures/selectkbest_comparison.png\")\n",
    "print(\"- outputs/figures/importance_thresholding.png\")\n",
    "print(\"- outputs/figures/feature_selection_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "We tested three feature selection methods:\n",
    "\n",
    "1. **RFE (Recursive Feature Elimination):** Iteratively removes least important features\n",
    "2. **SelectKBest:** Selects features based on statistical tests (ANOVA, Mutual Info)\n",
    "3. **Feature Importance Thresholding:** Uses RF importance scores with threshold\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- All methods achieved similar performance to baseline\n",
    "- Feature reduction possible without significant performance loss\n",
    "- Consensus features (selected by 2+ methods) provide robust subset\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "Use the method that provides best trade-off between:\n",
    "- Model performance (F1-score)\n",
    "- Model simplicity (fewer features)\n",
    "- Interpretability requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
