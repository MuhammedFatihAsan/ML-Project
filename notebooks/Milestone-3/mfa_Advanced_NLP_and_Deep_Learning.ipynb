{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Milestone 3: Advanced NLP and Deep Learning Models\n",
    "\n",
    "**Objective**\n",
    "In this notebook, we will improve our text analysis using Deep Learning.\n",
    "1. **BERT Analysis:** Use pre-trained transformers to understand listing descriptions.\n",
    "2. **Word Embeddings:** Create Word2Vec vectors to capture semantic meaning.\n",
    "3. **Deep Learning Model:** Train a Neural Network to predict values.\n",
    "\n",
    "**Final Goal: Integration Strategy (The \"Meta-Feature\")**\n",
    "After building these complex models, we will not just stop at comparisons.\n",
    "We will **condense** all our Deep Learning insights into **1 or 2 simple numeric features** (like a \"BERT_Score\").\n",
    "We will save these new features so they can be easily used by other models (like XGBoost or Random Forest) to improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "**What will we do?**\n",
    "In this step, we will prepare our environment and load the data.\n",
    "\n",
    "1.  **Import Libraries:** We will import Pandas, NumPy, and the Transformers library for BERT.\n",
    "2.  **Load Text Data (`listings_text_cleaned.csv`):**\n",
    "    * We created this file in milestone 1.\n",
    "    * It contains **two versions** of the text for different future tasks:\n",
    "        * **Raw Text:** The original text with punctuation. We need this for **BERT** to understand the context.\n",
    "        * **Clean Text:** The processed text without stopwords. We used this for TF-IDF before.\n",
    "3.  **Load Baseline Features (`nlp_master_features.csv`):**\n",
    "    * This file contains our old VADER sentiment scores.\n",
    "    * We will keep these scores to compare them with our new Deep Learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# We ignore warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import Transformers (for BERT)\n",
    "try:\n",
    "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "    print(\"Transformers library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Transformers library is not found. Please install it.\")\n",
    "\n",
    "# 2. Define File Paths\n",
    "# We assume the data is in the processed folder\n",
    "DATA_DIR = \"../../data/processed/\"\n",
    "TEXT_FILE = os.path.join(DATA_DIR, \"listings_text_cleaned.csv\")\n",
    "FEATURES_FILE = os.path.join(DATA_DIR, \"nlp_master_features.csv\")\n",
    "\n",
    "# 3. Load Data\n",
    "if os.path.exists(TEXT_FILE) and os.path.exists(FEATURES_FILE):\n",
    "    # Load text data (contains 'description' and 'description_clean')\n",
    "    df_text = pd.read_csv(TEXT_FILE)\n",
    "    \n",
    "    # Load old features (contains VADER scores)\n",
    "    df_features = pd.read_csv(FEATURES_FILE)\n",
    "    \n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Text Data Shape: {df_text.shape}\")\n",
    "    print(f\"Features Data Shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Please check your paths.\")\n",
    "\n",
    "# 4. Prepare Baseline Features\n",
    "# Strategy: We keep VADER scores and Counts. We DROP old TF-IDF columns.\n",
    "# We will use these to compare with our new Deep Learning model later.\n",
    "keep_columns = [\n",
    "    'id', \n",
    "    'description_sentiment', \n",
    "    'host_about_sentiment', \n",
    "    'desc_word_count', \n",
    "    'desc_length', \n",
    "    'name_length'\n",
    "]\n",
    "\n",
    "# Create the baseline dataframe\n",
    "if 'df_features' in locals():\n",
    "    df_baseline = df_features[keep_columns].copy()\n",
    "    print(\"\\nBaseline Features Selected (VADER + Structure):\")\n",
    "    print(df_baseline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Interpretation of Step 1\n",
    "\n",
    "We successfully loaded the data and created three dataframes:\n",
    "\n",
    "1.  **df_text**:\n",
    "    * **Source:** `listings_text_cleaned.csv`\n",
    "    * **Content:** Contains the raw `description` text.\n",
    "    * **Why?** We will feed this raw text into the BERT model to understand the context.<br><br>\n",
    "\n",
    "2.  **df_features**:\n",
    "    * **Source:** `nlp_master_features.csv`\n",
    "    * **Content:** Contains all 107 NLP features from Milestone 1 (including VADER scores and TF-IDF).\n",
    "    * **Why?** We loaded this to extract the specific columns we need.<br><br>\n",
    "\n",
    "3.  **df_baseline**:\n",
    "    * **Source:** Selected columns from `df_features`.\n",
    "    * **Content:** Contains only `id`, VADER sentiment scores, and word counts.\n",
    "    * **Why?** These are our \"Baseline\" features. Later, we will compare these old scores with the new BERT scores to see if Deep Learning is better.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Initialize BERT Tokenizer and Model\n",
    "\n",
    "**What will we do?**\n",
    "Computers cannot read words. They only understand numbers.\n",
    "1.  **Tokenizer:** We will load a tool that converts our text into numbers (tokens).\n",
    "2.  **Model:** We will download the **DistilBERT** model.\n",
    "    * **Why DistilBERT?** It is a smaller, faster, and lighter version of BERT. It gives 95% of the performance but runs 60% faster.\n",
    "    * **Pre-trained:** The model already \"knows\" English because it was trained on Wikipedia and Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize Tokenizer\n",
    "# We use 'distilbert-base-uncased'.\n",
    "# 'Uncased' means it treats \"Hello\" and \"hello\" as the same word.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 2. Initialize Model\n",
    "# This loads the pre-trained neural network weights.\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "print(\"DistilBERT Tokenizer and Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
