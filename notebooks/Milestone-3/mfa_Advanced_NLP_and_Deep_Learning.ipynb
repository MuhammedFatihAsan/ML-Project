{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Milestone 3: Advanced NLP and Deep Learning Models\n",
    "\n",
    "**Objective**\n",
    "In this notebook, we will improve our text analysis using Deep Learning.\n",
    "1. **BERT Analysis:** Use pre-trained transformers to understand listing descriptions.\n",
    "2. **Word Embeddings:** Create Word2Vec vectors to capture semantic meaning.\n",
    "3. **Deep Learning Model:** Train a Neural Network to predict values.\n",
    "\n",
    "**Final Goal: Integration Strategy (The \"Meta-Feature\")**\n",
    "After building these complex models, we will not just stop at comparisons.\n",
    "We will **condense** all our Deep Learning insights into **1 or 2 simple numeric features** (like a \"BERT_Score\").\n",
    "We will save these new features so they can be easily used by other models (like XGBoost or Random Forest) to improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "**What will we do?**\n",
    "In this step, we will prepare our environment and load the data.\n",
    "\n",
    "1.  **Import Libraries:** We will import Pandas, NumPy, and the Transformers library for BERT.\n",
    "2.  **Load Text Data (`listings_text_cleaned.csv`):**\n",
    "    * We created this file in milestone 1.\n",
    "    * It contains **two versions** of the text for different future tasks:\n",
    "        * **Raw Text:** The original text with punctuation. We need this for **BERT** to understand the context.\n",
    "        * **Clean Text:** The processed text without stopwords. We used this for TF-IDF before.\n",
    "3.  **Load Baseline Features (`nlp_master_features.csv`):**\n",
    "    * This file contains our old VADER sentiment scores.\n",
    "    * We will keep these scores to compare them with our new Deep Learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# We ignore warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import Transformers (for BERT)\n",
    "try:\n",
    "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "    print(\"Transformers library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Transformers library is not found. Please install it.\")\n",
    "\n",
    "# 2. Define File Paths\n",
    "# We assume the data is in the processed folder\n",
    "DATA_DIR = \"../../data/processed/\"\n",
    "TEXT_FILE = os.path.join(DATA_DIR, \"listings_text_cleaned.csv\")\n",
    "FEATURES_FILE = os.path.join(DATA_DIR, \"nlp_master_features.csv\")\n",
    "\n",
    "# 3. Load Data\n",
    "if os.path.exists(TEXT_FILE) and os.path.exists(FEATURES_FILE):\n",
    "    # Load text data (contains 'description' and 'description_clean')\n",
    "    df_text = pd.read_csv(TEXT_FILE)\n",
    "    \n",
    "    # Load old features (contains VADER scores)\n",
    "    df_features = pd.read_csv(FEATURES_FILE)\n",
    "    \n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Text Data Shape: {df_text.shape}\")\n",
    "    print(f\"Features Data Shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Please check your paths.\")\n",
    "\n",
    "# 4. Prepare Baseline Features\n",
    "# Strategy: We keep VADER scores and Counts. We DROP old TF-IDF columns.\n",
    "# We will use these to compare with our new Deep Learning model later.\n",
    "keep_columns = [\n",
    "    'id', \n",
    "    'description_sentiment', \n",
    "    'host_about_sentiment', \n",
    "    'desc_word_count', \n",
    "    'desc_length', \n",
    "    'name_length'\n",
    "]\n",
    "\n",
    "# Create the baseline dataframe\n",
    "if 'df_features' in locals():\n",
    "    df_baseline = df_features[keep_columns].copy()\n",
    "    print(\"\\nBaseline Features Selected (VADER + Structure):\")\n",
    "    print(df_baseline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Interpretation of Step 1\n",
    "\n",
    "We successfully loaded the data and created three dataframes:\n",
    "\n",
    "1.  **df_text**:\n",
    "    * **Source:** `listings_text_cleaned.csv`\n",
    "    * **Content:** Contains the raw `description` text.\n",
    "    * **Why?** We will feed this raw text into the BERT model to understand the context.<br><br>\n",
    "\n",
    "2.  **df_features**:\n",
    "    * **Source:** `nlp_master_features.csv`\n",
    "    * **Content:** Contains all 107 NLP features from Milestone 1 (including VADER scores and TF-IDF).\n",
    "    * **Why?** We loaded this to extract the specific columns we need.<br><br>\n",
    "\n",
    "3.  **df_baseline**:\n",
    "    * **Source:** Selected columns from `df_features`.\n",
    "    * **Content:** Contains only `id`, VADER sentiment scores, and word counts.\n",
    "    * **Why?** These are our \"Baseline\" features. Later, we will compare these old scores with the new BERT scores to see if Deep Learning is better.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Initialize BERT Tokenizer and Model\n",
    "\n",
    "**What will we do?**\n",
    "Computers cannot read words. They only understand numbers.\n",
    "1.  **Tokenizer:** We will load a tool that converts our text into numbers (tokens).\n",
    "2.  **Model:** We will download the **DistilBERT** model.\n",
    "    * **Why DistilBERT?** It is a smaller, faster, and lighter version of BERT. It gives 95% of the performance but runs 60% faster.\n",
    "    * **Pre-trained:** The model already \"knows\" English because it was trained on Wikipedia and Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import PyTorch and Transformers\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Check device (Use GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Tokenizer\n",
    "# We use 'distilbert-base-uncased' (Standard English model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 3. Initialize Model (PyTorch Version)\n",
    "# We switched to PyTorch (DistilBertModel) to avoid Keras errors.\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move the model to the active device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "print(\"DistilBERT Model (PyTorch) loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Interpretation of Step 2\n",
    "\n",
    "We successfully loaded the model.\n",
    "\n",
    "**Model Status:** The DistilBERT model is ready to process our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Generating BERT Embeddings\n",
    "\n",
    "**What will we do?**\n",
    "We will now convert the listing descriptions into numbers (vectors).\n",
    "\n",
    "**How will we do it?**\n",
    "1.  **Batch Processing:** We cannot process all 20,000 listings at once. It would crash the computer's memory (RAM).\n",
    "2.  **Loop:** We will take small groups (e.g., 32 listings at a time).\n",
    "3.  **Tokenize & Encode:**\n",
    "    * First, we convert words to tokens.\n",
    "    * Then, we feed them into the DistilBERT model.\n",
    "    * The model gives us a **vector of size 768** for each listing. This vector represents the \"meaning\" of the description.\n",
    "\n",
    "**Note:** This process might take some time (10-20 minutes on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data\n",
    "# We fill empty descriptions with \" \" to avoid errors.\n",
    "descriptions = df_text['description'].fillna(\"\").tolist()\n",
    "\n",
    "# 2. Parameters\n",
    "BATCH_SIZE = 32 # We process 32 listings at a time\n",
    "embeddings_list = []\n",
    "\n",
    "print(f\"Starting BERT embedding generation for {len(descriptions)} listings...\")\n",
    "\n",
    "# 3. Loop through data in batches\n",
    "# range(start, stop, step)\n",
    "for i in range(0, len(descriptions), BATCH_SIZE):\n",
    "    # Select the batch\n",
    "    batch_texts = descriptions[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # Tokenize\n",
    "    # padding=True: pad to the longest sentence in the batch\n",
    "    # truncation=True: cut texts longer than 128 tokens (saves memory)\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                      max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the device (CPU or GPU)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Generate Embeddings\n",
    "    with torch.no_grad(): # We do not need gradients for inference (saves RAM)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token (The vector representing the whole sentence)\n",
    "    # It is the first token (index 0) of the last hidden state\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Add to our list\n",
    "    embeddings_list.extend(batch_embeddings)\n",
    "    \n",
    "    # Print progress every 100 batches (approx. every 3200 listings)\n",
    "    if (i // BATCH_SIZE) % 100 == 0:\n",
    "        print(f\"Processed {i} / {len(descriptions)} listings...\")\n",
    "\n",
    "print(\"Embedding generation complete!\")\n",
    "\n",
    "# 4. Create DataFrame\n",
    "# Convert the list of arrays into a Pandas DataFrame\n",
    "df_bert = pd.DataFrame(embeddings_list)\n",
    "\n",
    "# Rename columns to 'bert_0', 'bert_1', ... 'bert_767'\n",
    "df_bert.columns = [f'bert_{i}' for i in range(df_bert.shape[1])]\n",
    "\n",
    "# Add the ID column for merging later\n",
    "df_bert['id'] = df_text['id'].values\n",
    "\n",
    "print(f\"BERT DataFrame Shape: {df_bert.shape}\")\n",
    "print(df_bert.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Interpretation of Step 3 (BERT Embeddings)\n",
    "\n",
    "We have successfully converted 20,942 listing descriptions into high-dimensional vectors.\n",
    "\n",
    "**Understanding the Output (`df_bert`):**\n",
    "* **Rows (20,942):** Each row represents one Airbnb listing.\n",
    "* **Columns (769):**\n",
    "    * **`id`:** The key to match these numbers back to the original house.\n",
    "    * **`bert_0` ... `bert_767`:** These **768 numbers** are the \"Deep Learning features.\"\n",
    "    * Unlike VADER (which gave us just 1 score: Positive/Negative), BERT gives us **768 dimensions** of meaning (e.g., one number might represent \"luxury,\" another \"location,\" another \"coziness\").\n",
    "\n",
    "**Next Step:**\n",
    "Now that we have these valuable numbers, we must **save** them immediately so we don't have to wait a long time again. Then, we will merge them with our VADER scores to prepare for the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Save and Merge Data\n",
    "\n",
    "**What will we do?**\n",
    "1.  **Save to CSV:** We will save the new BERT features to a file (`bert_embeddings.csv`).\n",
    "    * **Why?** Generating these numbers took a long time. We save them immediately to avoid doing it again if the computer crashes.\n",
    "2.  **Merge:** We will combine the **BERT features** (768 columns) with our **Baseline Features** (VADER scores + Word Counts).\n",
    "    * **Goal:** Create a single \"Dataset\" that allows us to compare the old method vs. the new method side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Paths\n",
    "BERT_FILE = os.path.join(DATA_DIR, \"bert_embeddings.csv\")\n",
    "FINAL_TASK_FILE = os.path.join(DATA_DIR, \"bert_prepared.csv\")\n",
    "\n",
    "# 2. Save BERT Embeddings (Checkpoint)\n",
    "# We save this immediately so we don't lose the calculated data.\n",
    "if 'df_bert' in locals():\n",
    "    df_bert.to_csv(BERT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved: {BERT_FILE}\")\n",
    "else:\n",
    "    print(\"Warning: df_bert not found. Make sure Step 3 ran successfully.\")\n",
    "\n",
    "# 3. Merge with Baseline Features\n",
    "# We combine:\n",
    "# - df_baseline: ID + VADER Scores + Word Counts (Old features)\n",
    "# - df_bert: ID + 768 BERT Vectors (New Deep Learning features)\n",
    "if 'df_baseline' in locals() and 'df_bert' in locals():\n",
    "    # Merge on 'id'\n",
    "    df_task3_1 = pd.merge(df_baseline, df_bert, on='id', how='inner')\n",
    "    \n",
    "    # 4. Save Final Dataset\n",
    "    df_task3_1.to_csv(FINAL_TASK_FILE, index=False)\n",
    "    \n",
    "    print(\"\\nMerge Complete!\")\n",
    "    print(f\"Final Dataset Shape: {df_task3_1.shape}\")\n",
    "    print(f\"Saved to: {FINAL_TASK_FILE}\")\n",
    "    \n",
    "    # Display first few rows to confirm\n",
    "    print(df_task3_1.head(3))\n",
    "else:\n",
    "    print(\"Error: Could not merge. Check if df_baseline and df_bert exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
