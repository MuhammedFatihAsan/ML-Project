{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Milestone 3: Advanced NLP and Deep Learning Models\n",
    "\n",
    "**Objective**\n",
    "In this notebook, we will improve our text analysis using Deep Learning.\n",
    "1. **BERT Analysis:** Use pre-trained transformers to understand listing descriptions.\n",
    "2. **Word Embeddings:** Create Word2Vec vectors to capture semantic meaning.\n",
    "3. **Deep Learning Model:** Train a Neural Network to predict values.\n",
    "\n",
    "**Final Goal: Integration Strategy (The \"Meta-Feature\")**\n",
    "After building these complex models, we will not just stop at comparisons.\n",
    "We will **condense** all our Deep Learning insights into **1 or 2 simple numeric features** (like a \"BERT_Score\").\n",
    "We will save these new features so they can be easily used by other models (like XGBoost or Random Forest) to improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "**What will we do?**\n",
    "In this step, we will prepare our environment and load the data.\n",
    "\n",
    "1.  **Import Libraries:** We will import Pandas, NumPy, and the Transformers library for BERT.\n",
    "2.  **Load Text Data (`listings_text_cleaned.csv`):**\n",
    "    * We created this file in milestone 1.\n",
    "    * It contains **two versions** of the text for different future tasks:\n",
    "        * **Raw Text:** The original text with punctuation. We need this for **BERT** to understand the context.\n",
    "        * **Clean Text:** The processed text without stopwords. We used this for TF-IDF before.\n",
    "3.  **Load Baseline Features (`nlp_master_features.csv`):**\n",
    "    * This file contains our old VADER sentiment scores.\n",
    "    * We will keep these scores to compare them with our new Deep Learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# We ignore warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import Transformers (for BERT)\n",
    "try:\n",
    "    from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "    print(\"Transformers library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Transformers library is not found. Please install it.\")\n",
    "\n",
    "# 2. Define File Paths\n",
    "# We assume the data is in the processed folder\n",
    "DATA_DIR = \"../../data/processed/\"\n",
    "TEXT_FILE = os.path.join(DATA_DIR, \"listings_text_cleaned.csv\")\n",
    "FEATURES_FILE = os.path.join(DATA_DIR, \"nlp_master_features.csv\")\n",
    "\n",
    "# 3. Load Data\n",
    "if os.path.exists(TEXT_FILE) and os.path.exists(FEATURES_FILE):\n",
    "    # Load text data (contains 'description' and 'description_clean')\n",
    "    df_text = pd.read_csv(TEXT_FILE)\n",
    "    \n",
    "    # Load old features (contains VADER scores)\n",
    "    df_features = pd.read_csv(FEATURES_FILE)\n",
    "    \n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Text Data Shape: {df_text.shape}\")\n",
    "    print(f\"Features Data Shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"Error: Files not found. Please check your paths.\")\n",
    "\n",
    "# 4. Prepare Baseline Features\n",
    "# Strategy: We keep VADER scores and Counts. We DROP old TF-IDF columns.\n",
    "# We will use these to compare with our new Deep Learning model later.\n",
    "keep_columns = [\n",
    "    'id', \n",
    "    'description_sentiment', \n",
    "    'host_about_sentiment', \n",
    "    'desc_word_count', \n",
    "    'desc_length', \n",
    "    'name_length'\n",
    "]\n",
    "\n",
    "# Create the baseline dataframe\n",
    "if 'df_features' in locals():\n",
    "    df_baseline = df_features[keep_columns].copy()\n",
    "    print(\"\\nBaseline Features Selected (VADER + Structure):\")\n",
    "    print(df_baseline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Interpretation of Step 1\n",
    "\n",
    "We successfully loaded the data and created three dataframes:\n",
    "\n",
    "1.  **df_text**:\n",
    "    * **Source:** `listings_text_cleaned.csv`\n",
    "    * **Content:** Contains the raw `description` text.\n",
    "    * **Why?** We will feed this raw text into the BERT model to understand the context.<br><br>\n",
    "\n",
    "2.  **df_features**:\n",
    "    * **Source:** `nlp_master_features.csv`\n",
    "    * **Content:** Contains all 107 NLP features from Milestone 1 (including VADER scores and TF-IDF).\n",
    "    * **Why?** We loaded this to extract the specific columns we need.<br><br>\n",
    "\n",
    "3.  **df_baseline**:\n",
    "    * **Source:** Selected columns from `df_features`.\n",
    "    * **Content:** Contains only `id`, VADER sentiment scores, and word counts.\n",
    "    * **Why?** These are our \"Baseline\" features. Later, we will compare these old scores with the new BERT scores to see if Deep Learning is better.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Initialize BERT Tokenizer and Model\n",
    "\n",
    "**What will we do?**\n",
    "Computers cannot read words. They only understand numbers.\n",
    "1.  **Tokenizer:** We will load a tool that converts our text into numbers (tokens).\n",
    "2.  **Model:** We will download the **DistilBERT** model.\n",
    "    * **Why DistilBERT?** It is a smaller, faster, and lighter version of BERT. It gives 95% of the performance but runs 60% faster.\n",
    "    * **Pre-trained:** The model already \"knows\" English because it was trained on Wikipedia and Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import PyTorch and Transformers\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Check device (Use GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Tokenizer\n",
    "# We use 'distilbert-base-uncased' (Standard English model)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 3. Initialize Model (PyTorch Version)\n",
    "# We switched to PyTorch (DistilBertModel) to avoid Keras errors.\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move the model to the active device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "print(\"DistilBERT Model (PyTorch) loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Interpretation of Step 2\n",
    "\n",
    "We successfully loaded the model.\n",
    "\n",
    "**Model Status:** The DistilBERT model is ready to process our text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 3: Generating BERT Embeddings\n",
    "\n",
    "**What will we do?**\n",
    "We will now convert the listing descriptions into numbers (vectors).\n",
    "\n",
    "**How will we do it?**\n",
    "1.  **Batch Processing:** We cannot process all 20,000 listings at once. It would crash the computer's memory (RAM).\n",
    "2.  **Loop:** We will take small groups (e.g., 32 listings at a time).\n",
    "3.  **Tokenize & Encode:**\n",
    "    * First, we convert words to tokens.\n",
    "    * Then, we feed them into the DistilBERT model.\n",
    "    * The model gives us a **vector of size 768** for each listing. This vector represents the \"meaning\" of the description.\n",
    "\n",
    "**Note:** This process might take some time (10-20 minutes on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data\n",
    "# We fill empty descriptions with \" \" to avoid errors.\n",
    "descriptions = df_text['description'].fillna(\"\").tolist()\n",
    "\n",
    "# 2. Parameters\n",
    "BATCH_SIZE = 32 # We process 32 listings at a time\n",
    "embeddings_list = []\n",
    "\n",
    "print(f\"Starting BERT embedding generation for {len(descriptions)} listings...\")\n",
    "\n",
    "# 3. Loop through data in batches\n",
    "# range(start, stop, step)\n",
    "for i in range(0, len(descriptions), BATCH_SIZE):\n",
    "    # Select the batch\n",
    "    batch_texts = descriptions[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # Tokenize\n",
    "    # padding=True: pad to the longest sentence in the batch\n",
    "    # truncation=True: cut texts longer than 128 tokens (saves memory)\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                      max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the device (CPU or GPU)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Generate Embeddings\n",
    "    with torch.no_grad(): # We do not need gradients for inference (saves RAM)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token (The vector representing the whole sentence)\n",
    "    # It is the first token (index 0) of the last hidden state\n",
    "    batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    # Add to our list\n",
    "    embeddings_list.extend(batch_embeddings)\n",
    "    \n",
    "    # Print progress every 100 batches (approx. every 3200 listings)\n",
    "    if (i // BATCH_SIZE) % 100 == 0:\n",
    "        print(f\"Processed {i} / {len(descriptions)} listings...\")\n",
    "\n",
    "print(\"Embedding generation complete!\")\n",
    "\n",
    "# 4. Create DataFrame\n",
    "# Convert the list of arrays into a Pandas DataFrame\n",
    "df_bert = pd.DataFrame(embeddings_list)\n",
    "\n",
    "# Rename columns to 'bert_0', 'bert_1', ... 'bert_767'\n",
    "df_bert.columns = [f'bert_{i}' for i in range(df_bert.shape[1])]\n",
    "\n",
    "# Add the ID column for merging later\n",
    "df_bert['id'] = df_text['id'].values\n",
    "\n",
    "print(f\"BERT DataFrame Shape: {df_bert.shape}\")\n",
    "print(df_bert.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Interpretation of Step 3 (BERT Embeddings)\n",
    "\n",
    "We have successfully converted 20,942 listing descriptions into high-dimensional vectors.\n",
    "\n",
    "**Understanding the Output (`df_bert`):**\n",
    "* **Rows (20,942):** Each row represents one Airbnb listing.\n",
    "* **Columns (769):**\n",
    "    * **`id`:** The key to match these numbers back to the original house.\n",
    "    * **`bert_0` ... `bert_767`:** These **768 numbers** are the \"Deep Learning features.\"\n",
    "    * Unlike VADER (which gave us just 1 score: Positive/Negative), BERT gives us **768 dimensions** of meaning (e.g., one number might represent \"luxury,\" another \"location,\" another \"coziness\").\n",
    "\n",
    "**Next Step:**\n",
    "Now that we have these valuable numbers, we must **save** them immediately so we don't have to wait a long time again. Then, we will merge them with our VADER scores to prepare for the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Save and Merge Data\n",
    "\n",
    "**What will we do?**\n",
    "1.  **Save to CSV:** We will save the new BERT features to a file (`bert_embeddings.csv`).\n",
    "    * **Why?** Generating these numbers took a long time. We save them immediately to avoid doing it again if the computer crashes.\n",
    "2.  **Merge:** We will combine the **BERT features** (768 columns) with our **Baseline Features** (VADER scores + Word Counts).\n",
    "    * **Goal:** Create a single \"Dataset\" that allows us to compare the old method vs. the new method side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Paths\n",
    "BERT_FILE = os.path.join(DATA_DIR, \"bert_embeddings.csv\")\n",
    "FINAL_TASK_FILE = os.path.join(DATA_DIR, \"bert_prepared.csv\")\n",
    "\n",
    "# 2. Save BERT Embeddings (Checkpoint)\n",
    "# We save this immediately so we don't lose the calculated data.\n",
    "if 'df_bert' in locals():\n",
    "    df_bert.to_csv(BERT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved: {BERT_FILE}\")\n",
    "else:\n",
    "    print(\"Warning: df_bert not found. Make sure Step 3 ran successfully.\")\n",
    "\n",
    "# 3. Merge with Baseline Features\n",
    "# We combine:\n",
    "# - df_baseline: ID + VADER Scores + Word Counts (Old features)\n",
    "# - df_bert: ID + 768 BERT Vectors (New Deep Learning features)\n",
    "if 'df_baseline' in locals() and 'df_bert' in locals():\n",
    "    # Merge on 'id'\n",
    "    df_task3_1 = pd.merge(df_baseline, df_bert, on='id', how='inner')\n",
    "    \n",
    "    # 4. Save Final Dataset\n",
    "    df_task3_1.to_csv(FINAL_TASK_FILE, index=False)\n",
    "    \n",
    "    print(\"\\nMerge Complete!\")\n",
    "    print(f\"Final Dataset Shape: {df_task3_1.shape}\")\n",
    "    print(f\"Saved to: {FINAL_TASK_FILE}\")\n",
    "    \n",
    "    # Display first few rows to confirm\n",
    "    print(df_task3_1.head(3))\n",
    "else:\n",
    "    print(\"Error: Could not merge. Check if df_baseline and df_bert exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Word Embeddings (Word2Vec)\n",
    "\n",
    "**What will we do?**\n",
    "We will train a **Word2Vec** model specifically on our Airbnb descriptions.\n",
    "* **BERT vs Word2Vec:**\n",
    "    * **BERT** is pre-trained on Wikipedia. It knows general English perfectly.\n",
    "    * **Word2Vec** will be trained *only* on our data. It will learn the specific jargon of Airbnb (e.g., that \"Ocean\" and \"Beach\" are mathematically very close).\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Input:** We will use the **Cleaned Text** (`description_clean`) this time.\n",
    "    * *Why?* Word2Vec doesn't need punctuation or stopwords. It just needs the core words.\n",
    "2.  **Training:** We will create a model that represents every word as a vector.\n",
    "3.  **Averaging:** Since a house has many words, we will take the **average** of all word vectors to get a single \"House Vector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Library\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    print(\"Gensim library is ready.\")\n",
    "except ImportError:\n",
    "    print(\"Gensim not found. Please install it using: !pip install gensim\")\n",
    "\n",
    "# 2. Prepare Data\n",
    "# Word2Vec expects a list of words, not a full sentence string.\n",
    "# We use 'description_clean' because stopwords/punctuation are already removed.\n",
    "# We convert \"sunny room wifi\" -> ['sunny', 'room', 'wifi']\n",
    "sentences = df_text['description_clean'].fillna(\"\").apply(lambda x: str(x).split()).tolist()\n",
    "\n",
    "print(f\"Training Word2Vec model on {len(sentences)} listings...\")\n",
    "\n",
    "# 3. Train Word2Vec Model\n",
    "# vector_size=100: Each word/house will be represented by 100 numbers.\n",
    "# window=5: The model looks at 5 words before and after the target word.\n",
    "# min_count=5: We ignore rare words (words that appear less than 5 times).\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "print(\"Word2Vec Model trained successfully.\")\n",
    "\n",
    "# 4. Generate Document Vectors (Averaging)\n",
    "# Since a house description has many words, we take the AVERAGE of all word vectors\n",
    "# to get a single vector representing the house.\n",
    "\n",
    "def get_mean_vector(word_list):\n",
    "    # Filter words that exist in our trained model\n",
    "    valid_words = [word for word in word_list if word in w2v_model.wv]\n",
    "    \n",
    "    if len(valid_words) > 0:\n",
    "        # Calculate mean (average)\n",
    "        return np.mean(w2v_model.wv[valid_words], axis=0)\n",
    "    else:\n",
    "        # If no valid words, return a list of zeros\n",
    "        return np.zeros(100)\n",
    "\n",
    "# Apply the function to all listings\n",
    "w2v_vectors = [get_mean_vector(doc) for doc in sentences]\n",
    "\n",
    "# 5. Create DataFrame\n",
    "df_w2v = pd.DataFrame(w2v_vectors)\n",
    "\n",
    "# Rename columns to w2v_0, w2v_1, ... w2v_99\n",
    "df_w2v.columns = [f'w2v_{i}' for i in range(df_w2v.shape[1])]\n",
    "\n",
    "# Add ID for merging\n",
    "df_w2v['id'] = df_text['id'].values\n",
    "\n",
    "print(f\"Word2Vec DataFrame Shape: {df_w2v.shape}\")\n",
    "print(df_w2v.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Interpretation of Step 5 (Word2Vec)\n",
    "\n",
    "We successfully trained a custom Word2Vec model on our data.\n",
    "* **Warning Note:** The `Exception ignored` message in the output is a harmless warning related to the library's internal threads. It did not stop the process.\n",
    "* **Result:** We now have a dataframe (`df_w2v`) with **100 new columns** (`w2v_0` to `w2v_99`)(and +1 id for represent).\n",
    "* **Meaning:** Each row represents the \"average meaning\" of a house description, distilled into 100 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Save Word2Vec Data\n",
    "\n",
    "**What will we do?**\n",
    "We will save these new features to a CSV file (`word2vec_embeddings.csv`).\n",
    "\n",
    "**Why?**\n",
    "1.  **Safety:** Just like BERT, we want to save our work so we don't have to retrain the model later.\n",
    "2.  **Usage:** Later, we will use this file to train a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Path\n",
    "W2V_FILE = os.path.join(DATA_DIR, \"word2vec_embeddings.csv\")\n",
    "\n",
    "# 2. Save Word2Vec Features\n",
    "if 'df_w2v' in locals():\n",
    "    df_w2v.to_csv(W2V_FILE, index=False)\n",
    "    \n",
    "    print(\"Save Complete!\")\n",
    "    print(f\"File saved to: {W2V_FILE}\")\n",
    "else:\n",
    "    print(\"Error: df_w2v not found. Please check Step 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb-Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
