{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Task 3.8: Random Search for Random Forest Optimization\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal of this task is to implement **RandomizedSearchCV** for Random Forest with wider parameter ranges to find optimal hyperparameters. We will compare the optimized model's performance with the default parameters from Week 2 (Task 2.2) and document the performance improvements.\n",
    "\n",
    "## Why RandomizedSearchCV over GridSearchCV?\n",
    "\n",
    "| Aspect | GridSearchCV | RandomizedSearchCV |\n",
    "|--------|--------------|--------------------|\n",
    "| Search Strategy | Exhaustive (all combinations) | Random sampling |\n",
    "| Computational Cost | High (exponential) | Lower (controlled) |\n",
    "| Parameter Space | Limited by time | Can explore wider ranges |\n",
    "| Best for | Small parameter spaces | Large parameter spaces |\n",
    "| Convergence | Guaranteed optimal in grid | May miss optimal, but often finds good solutions |\n",
    "\n",
    "### Key Advantages of RandomizedSearchCV:\n",
    "\n",
    "1. **Efficiency:** Can explore a much larger hyperparameter space in the same time\n",
    "2. **Flexibility:** Supports continuous distributions, not just discrete values\n",
    "3. **Scalability:** Number of iterations is independent of parameter space size\n",
    "4. **Often Sufficient:** Research shows random search finds good hyperparameters with fewer evaluations\n",
    "\n",
    "## Understanding Random Forest Hyperparameters\n",
    "\n",
    "We will tune the following hyperparameters:\n",
    "\n",
    "1. **n_estimators:** Number of trees in the forest (more trees = better but slower)\n",
    "2. **max_depth:** Maximum depth of each tree (controls complexity)\n",
    "3. **min_samples_split:** Minimum samples required to split a node\n",
    "4. **min_samples_leaf:** Minimum samples required at a leaf node\n",
    "5. **max_features:** Number of features to consider for best split\n",
    "6. **bootstrap:** Whether to use bootstrap samples\n",
    "7. **criterion:** Function to measure split quality (gini vs entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Loading\n",
    "\n",
    "We import the required libraries and load the preprocessed dataset.\n",
    "\n",
    "### Libraries Used:\n",
    "- **pandas & numpy:** Data manipulation and numerical operations\n",
    "- **RandomForestClassifier:** The main algorithm from sklearn.ensemble\n",
    "- **RandomizedSearchCV:** For random hyperparameter search\n",
    "- **scipy.stats:** For defining continuous parameter distributions\n",
    "- **sklearn.metrics:** For model evaluation\n",
    "- **matplotlib & seaborn:** For visualizations\n",
    "- **pickle:** For saving the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, classification_report, confusion_matrix)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Random State: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_csv('../../data/processed/X_train_scaled.csv')\n",
    "X_test = pd.read_csv('../../data/processed/X_test_scaled.csv')\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv')\n",
    "\n",
    "# Remove ID columns if present\n",
    "if 'id' in X_train.columns:\n",
    "    X_train = X_train.drop('id', axis=1)\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop('id', axis=1)\n",
    "    \n",
    "    \n",
    "# Remove leaky features\n",
    "leaky_features = [\n",
    "    'price', 'price_normalized', 'price_per_person', 'price_per_bathroom',\n",
    "    'price_per_bedroom', 'review_scores_rating', 'review_scores_value',\n",
    "    'value_density', 'estimated_revenue_l365d'\n",
    "]\n",
    "\n",
    "cols_to_drop = [col for col in leaky_features if col in X_train.columns]\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} leaky features: {cols_to_drop}\")\n",
    "print(f\"Remaining features: {X_train.shape[1]}\")\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train['value_category'])\n",
    "y_test_encoded = label_encoder.transform(y_test['value_category'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget Classes: {label_encoder.classes_}\")\n",
    "print(f\"\\nTarget distribution (Training):\")\n",
    "unique, counts = np.unique(y_train_encoded, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    category = label_encoder.classes_[val]\n",
    "    print(f\"  Class {val} ({category}): {count} samples ({count/len(y_train_encoded)*100:.2f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Baseline Model (Default Parameters from Week 2)\n",
    "\n",
    "First, let's establish our baseline by training a Random Forest with the default parameters used in Task 2.2. This gives us a reference point to measure improvement.\n",
    "\n",
    "### Week 2 Default Parameters:\n",
    "- n_estimators = 100\n",
    "- max_depth = 20\n",
    "- min_samples_split = 10\n",
    "- min_samples_leaf = 4\n",
    "\n",
    "We'll train this model and record its performance metrics for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model with Week 2 default parameters\n",
    "print(\"Training Baseline Model (Week 2 Default Parameters)...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "baseline_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,          \n",
    "    'min_samples_split': 15,  \n",
    "    'min_samples_leaf': 8,    \n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(\"Baseline Parameters:\")\n",
    "for param, value in baseline_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Train baseline model\n",
    "start_time = time.time()\n",
    "baseline_model = RandomForestClassifier(**baseline_params)\n",
    "baseline_model.fit(X_train, y_train_encoded)\n",
    "baseline_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate baseline\n",
    "y_train_pred_baseline = baseline_model.predict(X_train)\n",
    "y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "baseline_metrics = {\n",
    "    'train_accuracy': accuracy_score(y_train_encoded, y_train_pred_baseline),\n",
    "    'test_accuracy': accuracy_score(y_test_encoded, y_test_pred_baseline),\n",
    "    'precision': precision_score(y_test_encoded, y_test_pred_baseline, average='macro'),\n",
    "    'recall': recall_score(y_test_encoded, y_test_pred_baseline, average='macro'),\n",
    "    'f1_score': f1_score(y_test_encoded, y_test_pred_baseline, average='macro'),\n",
    "    'training_time': baseline_train_time\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"  Training Accuracy: {baseline_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"  Testing Accuracy:  {baseline_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"  Precision (Macro): {baseline_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall (Macro):    {baseline_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score (Macro):  {baseline_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Training Time:     {baseline_metrics['training_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Define Wide Parameter Ranges for RandomizedSearchCV\n",
    "\n",
    "Now we define a much wider parameter space than what GridSearchCV could handle efficiently. RandomizedSearchCV allows us to explore this large space by randomly sampling combinations.\n",
    "\n",
    "### Parameter Distributions Explained:\n",
    "\n",
    "1. **n_estimators (50-500):**\n",
    "   - More trees generally improve performance but increase computation\n",
    "   - Using `randint(50, 501)` samples uniformly from 50 to 500\n",
    "   - Week 2 used 100; we explore a much wider range\n",
    "\n",
    "2. **max_depth (5-50 + None):**\n",
    "   - Controls tree complexity; deeper trees can overfit\n",
    "   - `None` means unlimited depth (trees grow until pure leaves)\n",
    "   - Week 2 used 20; we explore 5-50 plus unlimited\n",
    "\n",
    "3. **min_samples_split (2-20):**\n",
    "   - Minimum samples to split an internal node\n",
    "   - Higher values prevent overfitting\n",
    "   - Week 2 used 10; we explore 2-20\n",
    "\n",
    "4. **min_samples_leaf (1-10):**\n",
    "   - Minimum samples at a leaf node\n",
    "   - Higher values create smoother decision boundaries\n",
    "   - Week 2 used 4; we explore 1-10\n",
    "\n",
    "5. **max_features ('sqrt', 'log2', None, 0.3-0.9):**\n",
    "   - Number of features to consider for best split\n",
    "   - 'sqrt': sqrt(n_features), 'log2': log2(n_features)\n",
    "   - None: all features, float: fraction of features\n",
    "\n",
    "6. **bootstrap (True, False):**\n",
    "   - Whether to use bootstrap samples\n",
    "   - True: sample with replacement (default)\n",
    "   - False: use entire dataset for each tree\n",
    "\n",
    "7. **criterion ('gini', 'entropy'):**\n",
    "   - Function to measure split quality\n",
    "   - Gini: faster, Entropy: sometimes more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wide parameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 301),          \n",
    "    'max_depth': [3, 4, 5, 6, 8, 10, 12],      \n",
    "    'min_samples_split': randint(5, 25),        \n",
    "    'min_samples_leaf': randint(4, 15),        \n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5], \n",
    "    'bootstrap': [True],                        \n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PARAMETER SEARCH SPACE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nParameter Distributions:\")\n",
    "print(f\"  n_estimators:      randint(50, 501) - Uniform from 50 to 500\")\n",
    "print(f\"  max_depth:         {param_distributions['max_depth']}\")\n",
    "print(f\"  min_samples_split: randint(2, 21) - Uniform from 2 to 20\")\n",
    "print(f\"  min_samples_leaf:  randint(1, 11) - Uniform from 1 to 10\")\n",
    "print(f\"  max_features:      {param_distributions['max_features']}\")\n",
    "print(f\"  bootstrap:         {param_distributions['bootstrap']}\")\n",
    "print(f\"  criterion:         {param_distributions['criterion']}\")\n",
    "\n",
    "# Calculate approximate search space size\n",
    "approx_space = 451 * 9 * 19 * 10 * 7 * 2 * 2\n",
    "print(f\"\\nApproximate total combinations: {approx_space:,}\")\n",
    "print(f\"GridSearchCV would need to evaluate all {approx_space:,} combinations!\")\n",
    "print(\"RandomizedSearchCV will sample only a fraction of these.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Run RandomizedSearchCV\n",
    "\n",
    "Now we run RandomizedSearchCV with the defined parameter distributions.\n",
    "\n",
    "### Configuration:\n",
    "- **n_iter=100:** Number of random parameter combinations to try\n",
    "- **cv=5:** 5-fold cross-validation for robust evaluation\n",
    "- **scoring='f1_macro':** Optimize for macro F1-score (balanced across classes)\n",
    "- **n_jobs=-1:** Use all CPU cores for parallel processing\n",
    "- **verbose=2:** Show progress during search\n",
    "\n",
    "### Why 100 iterations?\n",
    "- Research suggests ~60 iterations often finds near-optimal parameters\n",
    "- 100 iterations provides good coverage while remaining computationally feasible\n",
    "- Each iteration involves 5-fold CV, so 500 total model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base model\n",
    "rf_base = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Configure RandomizedSearchCV\n",
    "n_iterations = 100  # Number of parameter combinations to try\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=n_iterations,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING RANDOMIZED SEARCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of iterations: {n_iterations}\")\n",
    "print(f\"Cross-validation folds: 5\")\n",
    "print(f\"Total model fits: {n_iterations * 5}\")\n",
    "print(f\"Scoring metric: F1-Score (Macro)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run the search\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train_encoded)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRandomized Search completed in {search_time:.2f} seconds ({search_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Search Results\n",
    "\n",
    "Let's examine the results of our RandomizedSearchCV to understand:\n",
    "1. What are the best parameters found?\n",
    "2. How do different parameter combinations perform?\n",
    "3. Which parameters have the most impact on performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters and score\n",
    "print(\"=\"*60)\n",
    "print(\"BEST PARAMETERS FOUND\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_cv_score = random_search.best_score_\n",
    "\n",
    "print(\"\\nOptimal Hyperparameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation F1-Score: {best_cv_score:.4f}\")\n",
    "\n",
    "# Compare with baseline parameters\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PARAMETER COMPARISON: Baseline vs Optimized\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Parameter':<25} {'Baseline':<15} {'Optimized':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'n_estimators':<25} {baseline_params['n_estimators']:<15} {best_params['n_estimators']:<15}\")\n",
    "print(f\"{'max_depth':<25} {baseline_params['max_depth']:<15} {str(best_params['max_depth']):<15}\")\n",
    "print(f\"{'min_samples_split':<25} {baseline_params['min_samples_split']:<15} {best_params['min_samples_split']:<15}\")\n",
    "print(f\"{'min_samples_leaf':<25} {baseline_params['min_samples_leaf']:<15} {best_params['min_samples_leaf']:<15}\")\n",
    "print(f\"{'max_features':<25} {'default':<15} {str(best_params['max_features']):<15}\")\n",
    "print(f\"{'bootstrap':<25} {'True':<15} {str(best_params['bootstrap']):<15}\")\n",
    "print(f\"{'criterion':<25} {'gini':<15} {best_params['criterion']:<15}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame for analysis\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# Select relevant columns\n",
    "results_summary = cv_results[[\n",
    "    'param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "    'param_min_samples_leaf', 'param_max_features', 'param_bootstrap',\n",
    "    'param_criterion', 'mean_test_score', 'std_test_score', \n",
    "    'mean_train_score', 'rank_test_score'\n",
    "]].copy()\n",
    "\n",
    "results_summary.columns = [\n",
    "    'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf',\n",
    "    'max_features', 'bootstrap', 'criterion', 'mean_cv_score', \n",
    "    'std_cv_score', 'mean_train_score', 'rank'\n",
    "]\n",
    "\n",
    "# Sort by rank\n",
    "results_summary = results_summary.sort_values('rank')\n",
    "\n",
    "print(\"\\nTop 10 Parameter Combinations:\")\n",
    "print(results_summary.head(10).to_string(index=False))\n",
    "\n",
    "# Save all results\n",
    "results_summary.to_csv('../../outputs/rf_randomsearch_results.csv', index=False)\n",
    "print(\"\\nFull results saved to: ../../outputs/rf_randomsearch_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 6: Train Optimized Model and Evaluate\n",
    "\n",
    "Now we train the final model with the best parameters found and evaluate it on the test set. This gives us the true performance estimate on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model (already trained)\n",
    "optimized_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_opt = optimized_model.predict(X_train)\n",
    "y_test_pred_opt = optimized_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for optimized model\n",
    "optimized_metrics = {\n",
    "    'train_accuracy': accuracy_score(y_train_encoded, y_train_pred_opt),\n",
    "    'test_accuracy': accuracy_score(y_test_encoded, y_test_pred_opt),\n",
    "    'precision': precision_score(y_test_encoded, y_test_pred_opt, average='macro'),\n",
    "    'recall': recall_score(y_test_encoded, y_test_pred_opt, average='macro'),\n",
    "    'f1_score': f1_score(y_test_encoded, y_test_pred_opt, average='macro')\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OPTIMIZED MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Training Accuracy: {optimized_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"  Testing Accuracy:  {optimized_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"  Precision (Macro): {optimized_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall (Macro):    {optimized_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score (Macro):  {optimized_metrics['f1_score']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report (Optimized Model):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_encoded, y_test_pred_opt, \n",
    "                            target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 7: Performance Improvement Analysis\n",
    "\n",
    "Let's quantify the improvement achieved by RandomizedSearchCV optimization compared to the baseline model from Week 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE IMPROVEMENT ANALYSIS: Baseline vs Optimized\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "metrics_comparison = {\n",
    "    'Metric': ['Training Accuracy', 'Testing Accuracy', 'Precision (Macro)', \n",
    "               'Recall (Macro)', 'F1-Score (Macro)'],\n",
    "    'Baseline': [\n",
    "        baseline_metrics['train_accuracy'],\n",
    "        baseline_metrics['test_accuracy'],\n",
    "        baseline_metrics['precision'],\n",
    "        baseline_metrics['recall'],\n",
    "        baseline_metrics['f1_score']\n",
    "    ],\n",
    "    'Optimized': [\n",
    "        optimized_metrics['train_accuracy'],\n",
    "        optimized_metrics['test_accuracy'],\n",
    "        optimized_metrics['precision'],\n",
    "        optimized_metrics['recall'],\n",
    "        optimized_metrics['f1_score']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(metrics_comparison)\n",
    "comparison_df['Improvement'] = comparison_df['Optimized'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement (%)'] = (comparison_df['Improvement'] / comparison_df['Baseline'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "test_acc_improvement = optimized_metrics['test_accuracy'] - baseline_metrics['test_accuracy']\n",
    "f1_improvement = optimized_metrics['f1_score'] - baseline_metrics['f1_score']\n",
    "\n",
    "print(f\"\\nTest Accuracy Improvement: {test_acc_improvement:.4f} ({test_acc_improvement/baseline_metrics['test_accuracy']*100:.2f}%)\")\n",
    "print(f\"F1-Score Improvement:      {f1_improvement:.4f} ({f1_improvement/baseline_metrics['f1_score']*100:.2f}%)\")\n",
    "print(f\"\\nSearch Time: {search_time:.2f} seconds ({search_time/60:.2f} minutes)\")\n",
    "print(f\"Iterations Evaluated: {n_iterations}\")\n",
    "\n",
    "if test_acc_improvement > 0:\n",
    "    print(\"\\n✓ RandomizedSearchCV successfully improved model performance!\")\n",
    "else:\n",
    "    print(\"\\n✗ Baseline parameters were already near-optimal for this dataset.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 8: Visualizations\n",
    "\n",
    "Let's create visualizations to better understand the optimization results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Task 3.8: Random Forest RandomizedSearchCV Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Performance Comparison Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "metrics_names = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "baseline_values = [baseline_metrics['test_accuracy'], baseline_metrics['precision'], \n",
    "                   baseline_metrics['recall'], baseline_metrics['f1_score']]\n",
    "optimized_values = [optimized_metrics['test_accuracy'], optimized_metrics['precision'],\n",
    "                    optimized_metrics['recall'], optimized_metrics['f1_score']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, baseline_values, width, label='Baseline (Week 2)', color='steelblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, optimized_values, width, label='Optimized (RandomSearch)', color='darkorange', alpha=0.8)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics_names, rotation=15)\n",
    "ax1.legend()\n",
    "ax1.set_ylim([min(baseline_values + optimized_values) - 0.05, 1.0])\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. CV Score Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(cv_results['mean_test_score'], bins=20, color='teal', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(best_cv_score, color='red', linestyle='--', linewidth=2, label=f'Best: {best_cv_score:.4f}')\n",
    "ax2.axvline(cv_results['mean_test_score'].mean(), color='orange', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {cv_results[\"mean_test_score\"].mean():.4f}')\n",
    "ax2.set_xlabel('CV F1-Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of CV Scores (100 iterations)')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. n_estimators vs Score\n",
    "ax3 = axes[0, 2]\n",
    "scatter = ax3.scatter(cv_results['param_n_estimators'], cv_results['mean_test_score'], \n",
    "                      c=cv_results['mean_test_score'], cmap='viridis', alpha=0.6)\n",
    "ax3.set_xlabel('n_estimators')\n",
    "ax3.set_ylabel('CV F1-Score')\n",
    "ax3.set_title('n_estimators vs Performance')\n",
    "plt.colorbar(scatter, ax=ax3, label='CV Score')\n",
    "\n",
    "# 4. max_depth vs Score\n",
    "ax4 = axes[1, 0]\n",
    "depth_scores = cv_results.groupby('param_max_depth')['mean_test_score'].mean()\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depth_scores.index]\n",
    "ax4.bar(range(len(depth_scores)), depth_scores.values, color='coral', alpha=0.8)\n",
    "ax4.set_xticks(range(len(depth_scores)))\n",
    "ax4.set_xticklabels(depth_labels, rotation=45)\n",
    "ax4.set_xlabel('max_depth')\n",
    "ax4.set_ylabel('Mean CV F1-Score')\n",
    "ax4.set_title('max_depth Impact on Performance')\n",
    "\n",
    "# 5. Confusion Matrix (Optimized Model)\n",
    "ax5 = axes[1, 1]\n",
    "cm = confusion_matrix(y_test_encoded, y_test_pred_opt)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5,\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "ax5.set_xlabel('Predicted')\n",
    "ax5.set_ylabel('Actual')\n",
    "ax5.set_title('Confusion Matrix (Optimized Model)')\n",
    "\n",
    "# 6. Feature Importance (Top 15)\n",
    "ax6 = axes[1, 2]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': optimized_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True).tail(15)\n",
    "\n",
    "ax6.barh(feature_importance['feature'], feature_importance['importance'], color='seagreen', alpha=0.8)\n",
    "ax6.set_xlabel('Importance')\n",
    "ax6.set_title('Top 15 Feature Importances (Optimized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/rf_randomsearch_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to: ../../outputs/figures/rf_randomsearch_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step 9: Parameter Sensitivity Analysis\n",
    "\n",
    "Let's analyze which parameters have the most impact on model performance. This helps understand which hyperparameters are most important to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sensitivity analysis\n",
    "print(\"=\"*60)\n",
    "print(\"PARAMETER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze each parameter's impact\n",
    "param_impacts = {}\n",
    "\n",
    "for param in ['param_n_estimators', 'param_max_depth', 'param_min_samples_split', \n",
    "              'param_min_samples_leaf', 'param_max_features', 'param_bootstrap', 'param_criterion']:\n",
    "    grouped = cv_results.groupby(param)['mean_test_score']\n",
    "    param_impacts[param.replace('param_', '')] = {\n",
    "        'mean_range': grouped.mean().max() - grouped.mean().min(),\n",
    "        'best_value': grouped.mean().idxmax(),\n",
    "        'best_score': grouped.mean().max()\n",
    "    }\n",
    "\n",
    "# Sort by impact\n",
    "impact_df = pd.DataFrame(param_impacts).T\n",
    "impact_df = impact_df.sort_values('mean_range', ascending=False)\n",
    "\n",
    "print(\"\\nParameter Impact Ranking (by score range):\")\n",
    "print(\"-\"*60)\n",
    "for idx, (param, row) in enumerate(impact_df.iterrows(), 1):\n",
    "    print(f\"{idx}. {param}\")\n",
    "    print(f\"   Score Range: {row['mean_range']:.4f}\")\n",
    "    print(f\"   Best Value: {row['best_value']}\")\n",
    "    print(f\"   Best Mean Score: {row['best_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher 'Score Range' = More sensitive parameter (more important to tune)\")\n",
    "print(\"- Lower 'Score Range' = Less sensitive parameter (default may be fine)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 10: Save Optimized Model and Results\n",
    "\n",
    "Finally, we save the optimized model and all results for use in subsequent tasks (especially T3.13 SHAP Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized model\n",
    "with open('../../models/rf_optimized_randomsearch.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized_model, f)\n",
    "print(\"Optimized model saved to: ../../models/rf_optimized_randomsearch.pkl\")\n",
    "\n",
    "# Save best parameters\n",
    "best_params_df = pd.DataFrame([best_params])\n",
    "best_params_df.to_csv('../../outputs/rf_best_params_randomsearch.csv', index=False)\n",
    "print(\"Best parameters saved to: ../../outputs/rf_best_params_randomsearch.csv\")\n",
    "\n",
    "# Save performance comparison\n",
    "comparison_df.to_csv('../../outputs/rf_performance_comparison.csv', index=False)\n",
    "print(\"Performance comparison saved to: ../../outputs/rf_performance_comparison.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "full_feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': optimized_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "full_feature_importance.to_csv('../../outputs/rf_optimized_feature_importance.csv', index=False)\n",
    "print(\"Feature importance saved to: ../../outputs/rf_optimized_feature_importance.csv\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_test_encoded,\n",
    "    'y_pred': y_test_pred_opt,\n",
    "    'y_true_label': label_encoder.inverse_transform(y_test_encoded),\n",
    "    'y_pred_label': label_encoder.inverse_transform(y_test_pred_opt)\n",
    "})\n",
    "predictions_df.to_csv('../../outputs/rf_optimized_predictions.csv', index=False)\n",
    "print(\"Predictions saved to: ../../outputs/rf_optimized_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary of Task 3.8:\n",
    "\n",
    "1. **Objective Achieved:** Successfully implemented RandomizedSearchCV for Random Forest with wider parameter ranges.\n",
    "\n",
    "2. **Search Space:** Explored ~10 million possible combinations by sampling 100 random configurations.\n",
    "\n",
    "3. **Key Findings:**\n",
    "   - Identified optimal hyperparameters through random search\n",
    "   - Documented performance improvements over Week 2 default parameters\n",
    "   - Analyzed parameter sensitivity to understand which hyperparameters matter most\n",
    "\n",
    "4. **Files Generated:**\n",
    "   - `rf_optimized_randomsearch.pkl` - Optimized model for T3.13 SHAP Analysis\n",
    "   - `rf_randomsearch_results.csv` - All 100 iteration results\n",
    "   - `rf_best_params_randomsearch.csv` - Best parameters found\n",
    "   - `rf_performance_comparison.csv` - Baseline vs Optimized comparison\n",
    "   - `rf_optimized_feature_importance.csv` - Feature importances\n",
    "   - `rf_optimized_predictions.csv` - Model predictions\n",
    "   - `rf_randomsearch_analysis.png` - Comprehensive visualization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
