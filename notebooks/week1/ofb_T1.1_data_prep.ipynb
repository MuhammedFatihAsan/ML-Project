{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load both datasets\n",
    "sf_df = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_df = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T1.1: INITIAL DATA EXPLORATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä San Francisco Dataset:\")\n",
    "print(f\"   - Rows: {sf_df.shape[0]:,}\")\n",
    "print(f\"   - Columns: {sf_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüìä San Diego Dataset:\")\n",
    "print(f\"   - Rows: {sd_df.shape[0]:,}\")\n",
    "print(f\"   - Columns: {sd_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüìä Combined Dataset (if merged):\")\n",
    "print(f\"   - Total Rows: {sf_df.shape[0] + sd_df.shape[0]:,}\")\n",
    "\n",
    "# Check if columns match\n",
    "sf_cols = set(sf_df.columns)\n",
    "sd_cols = set(sd_df.columns)\n",
    "print(f\"\\n‚úì Column names match: {sf_cols == sd_cols}\")\n",
    "\n",
    "if sf_cols != sd_cols:\n",
    "    print(f\"   - Columns only in SF: {sf_cols - sd_cols}\")\n",
    "    print(f\"   - Columns only in SD: {sd_cols - sf_cols}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. COLUMN STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Columns: {sf_df.shape[1]}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(sf_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. DATA TYPES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze data types\n",
    "sf_dtypes = sf_df.dtypes.value_counts()\n",
    "print(\"\\nüìã San Francisco Data Types:\")\n",
    "for dtype, count in sf_dtypes.items():\n",
    "    print(f\"   - {dtype}: {count} columns\")\n",
    "\n",
    "sd_dtypes = sd_df.dtypes.value_counts()\n",
    "print(\"\\nüìã San Diego Data Types:\")\n",
    "for dtype, count in sd_dtypes.items():\n",
    "    print(f\"   - {dtype}: {count} columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. MISSING VALUES ANALYSIS - SAN FRANCISCO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sf_missing = sf_df.isnull().sum()\n",
    "sf_missing_pct = (sf_missing / len(sf_df) * 100).round(2)\n",
    "sf_missing_df = pd.DataFrame({\n",
    "    'Column': sf_missing.index,\n",
    "    'Missing_Count': sf_missing.values,\n",
    "    'Missing_Percentage': sf_missing_pct.values\n",
    "})\n",
    "sf_missing_df = sf_missing_df[sf_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with Missing Values: {len(sf_missing_df)}/{len(sf_df.columns)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Values:\")\n",
    "print(sf_missing_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. MISSING VALUES ANALYSIS - SAN DIEGO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sd_missing = sd_df.isnull().sum()\n",
    "sd_missing_pct = (sd_missing / len(sd_df) * 100).round(2)\n",
    "sd_missing_df = pd.DataFrame({\n",
    "    'Column': sd_missing.index,\n",
    "    'Missing_Count': sd_missing.values,\n",
    "    'Missing_Percentage': sd_missing_pct.values\n",
    "})\n",
    "sd_missing_df = sd_missing_df[sd_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with Missing Values: {len(sd_missing_df)}/{len(sd_df.columns)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Values:\")\n",
    "print(sd_missing_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. KEY NUMERICAL FEATURES SUMMARY - SAN FRANCISCO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key numerical columns to analyze\n",
    "key_numerical = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms', \n",
    "                 'minimum_nights', 'maximum_nights', 'number_of_reviews',\n",
    "                 'review_scores_rating', 'review_scores_accuracy', \n",
    "                 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                 'review_scores_communication', 'review_scores_location',\n",
    "                 'review_scores_value']\n",
    "\n",
    "# Check which columns exist\n",
    "existing_numerical = [col for col in key_numerical if col in sf_df.columns]\n",
    "\n",
    "print(\"\\nüìä San Francisco - Key Numerical Features:\")\n",
    "sf_summary = sf_df[existing_numerical].describe().T\n",
    "sf_summary['missing'] = sf_df[existing_numerical].isnull().sum()\n",
    "sf_summary['missing_pct'] = (sf_summary['missing'] / len(sf_df) * 100).round(2)\n",
    "print(sf_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'missing', 'missing_pct']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. KEY NUMERICAL FEATURES SUMMARY - SAN DIEGO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä San Diego - Key Numerical Features:\")\n",
    "sd_summary = sd_df[existing_numerical].describe().T\n",
    "sd_summary['missing'] = sd_df[existing_numerical].isnull().sum()\n",
    "sd_summary['missing_pct'] = (sd_summary['missing'] / len(sd_df) * 100).round(2)\n",
    "print(sd_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'missing', 'missing_pct']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. KEY CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "key_categorical = ['property_type', 'room_type', 'neighbourhood_cleansed', \n",
    "                   'host_is_superhost', 'instant_bookable']\n",
    "\n",
    "existing_categorical = [col for col in key_categorical if col in sf_df.columns]\n",
    "\n",
    "print(\"\\nüìä San Francisco - Categorical Features:\")\n",
    "for col in existing_categorical:\n",
    "    unique_count = sf_df[col].nunique()\n",
    "    missing = sf_df[col].isnull().sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   - Unique values: {unique_count}\")\n",
    "    print(f\"   - Missing: {missing} ({missing/len(sf_df)*100:.2f}%)\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"   - Value counts:\")\n",
    "        print(sf_df[col].value_counts().head(10).to_string())\n",
    "\n",
    "print(\"\\nüìä San Diego - Categorical Features:\")\n",
    "for col in existing_categorical:\n",
    "    unique_count = sd_df[col].nunique()\n",
    "    missing = sd_df[col].isnull().sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   - Unique values: {unique_count}\")\n",
    "    print(f\"   - Missing: {missing} ({missing/len(sd_df)*100:.2f}%)\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"   - Value counts:\")\n",
    "        print(sd_df[col].value_counts().head(10).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. PRICE ANALYSIS (NEEDS CLEANING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä San Francisco - Price Column:\")\n",
    "print(f\"   - Data type: {sf_df['price'].dtype}\")\n",
    "print(f\"   - Sample values: {sf_df['price'].head(10).tolist()}\")\n",
    "print(f\"   - Missing: {sf_df['price'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nüìä San Diego - Price Column:\")\n",
    "print(f\"   - Data type: {sd_df['price'].dtype}\")\n",
    "print(f\"   - Sample values: {sd_df['price'].head(10).tolist()}\")\n",
    "print(f\"   - Missing: {sd_df['price'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: Price column is stored as string with '$' and ',' - needs cleaning in T1.2\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. DATA QUALITY ISSUES IDENTIFIED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check for duplicates\n",
    "sf_dupes = sf_df.duplicated().sum()\n",
    "sd_dupes = sd_df.duplicated().sum()\n",
    "if sf_dupes > 0 or sd_dupes > 0:\n",
    "    issues.append(f\"Duplicate rows: SF={sf_dupes}, SD={sd_dupes}\")\n",
    "\n",
    "# Check price format\n",
    "if sf_df['price'].dtype == 'object':\n",
    "    issues.append(\"Price column needs cleaning (contains '$' and ',')\")\n",
    "\n",
    "# Check high missing value columns\n",
    "high_missing_sf = sf_missing_df[sf_missing_df['Missing_Percentage'] > 50]\n",
    "high_missing_sd = sd_missing_df[sd_missing_df['Missing_Percentage'] > 50]\n",
    "issues.append(f\"Columns with >50% missing: SF={len(high_missing_sf)}, SD={len(high_missing_sd)}\")\n",
    "\n",
    "# Check for columns with all missing\n",
    "all_missing_sf = sf_missing_df[sf_missing_df['Missing_Percentage'] == 100]\n",
    "all_missing_sd = sd_missing_df[sd_missing_df['Missing_Percentage'] == 100]\n",
    "if len(all_missing_sf) > 0 or len(all_missing_sd) > 0:\n",
    "    issues.append(f\"Columns with 100% missing: SF={len(all_missing_sf)}, SD={len(all_missing_sd)}\")\n",
    "\n",
    "print(\"\\nüîç Issues Found:\")\n",
    "for i, issue in enumerate(issues, 1):\n",
    "    print(f\"{i}. {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"T1.1 SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Task T1.1 Completed: Initial Data Exploration\n",
    "\n",
    "üìä Dataset Overview:\n",
    "   - San Francisco: {sf_df.shape[0]:,} rows √ó {sf_df.shape[1]} columns\n",
    "   - San Diego: {sd_df.shape[0]:,} rows √ó {sd_df.shape[1]} columns\n",
    "   - Combined: {sf_df.shape[0] + sd_df.shape[0]:,} rows\n",
    "\n",
    "üîç Key Findings:\n",
    "   1. Both datasets have {sf_df.shape[1]} columns with matching structure\n",
    "   2. Price column needs cleaning (stored as string with '$' and ',')\n",
    "   3. {len(sf_missing_df)} columns in SF and {len(sd_missing_df)} columns in SD have missing values\n",
    "   4. Review scores have significant missing values (~30-40%)\n",
    "   5. Text columns (description, host_about) need NLP processing (Member 2's task)\n",
    "   6. Categorical encoding needed for property_type, room_type, neighbourhood\n",
    "\n",
    "üìã Next Steps (T1.2 - Data Cleaning Pipeline):\n",
    "   - Handle missing values\n",
    "   - Clean price column (remove '$' and ',', convert to float)\n",
    "   - Remove duplicates\n",
    "   - Filter invalid entries\n",
    "   - Document all cleaning decisions\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
