{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Loading\n",
    "In this section, we imported the necessary libraries (Pandas, NLTK) and downloaded the required NLTK resources (like stopwords and wordnet).\n",
    "\n",
    "Then, we loaded the raw Airbnb data for **San Diego** and **San Francisco**. We combined these two datasets into a single dataframe to process them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. IMPORT LIBRARIES & SETUP\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Configure pandas to show full text content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DOWNLOAD NLTK RESOURCES\n",
    "# ==========================================\n",
    "print(\"Downloading NLTK resources...\")\n",
    "try:\n",
    "    nltk.download('punkt')       # Sentence/word splitter\n",
    "    nltk.download('stopwords')   # Noise words (the, is, at)\n",
    "    nltk.download('wordnet')     # Dictionary for lemmatization\n",
    "    nltk.download('omw-1.4')     # Open Multilingual Wordnet\n",
    "    print(\"NLTK resources downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD AND COMBINE RAW DATA\n",
    "# ==========================================\n",
    "# Define the paths for your two raw data files.\n",
    "# PLEASE UPDATE THESE FILENAMES to match your actual files in data/raw/\n",
    "base_path = \"../../data/raw/\"\n",
    "FILE_1_NAME = \"san diego.csv\"      # Ornegin: listings_sd.csv\n",
    "FILE_2_NAME = \"san francisco.csv\"   # Ornegin: listings_sf.csv\n",
    "\n",
    "path1 = os.path.join(base_path, FILE_1_NAME)\n",
    "path2 = os.path.join(base_path, FILE_2_NAME)\n",
    "\n",
    "def load_and_combine_data(p1, p2):\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(p1):\n",
    "        print(f\"File not found: {p1}\")\n",
    "        return None\n",
    "    if not os.path.exists(p2):\n",
    "        print(f\"File not found: {p2}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading file 1: {p1}\")\n",
    "    df1 = pd.read_csv(p1)\n",
    "    \n",
    "    print(f\"Loading file 2: {p2}\")\n",
    "    df2 = pd.read_csv(p2)\n",
    "    \n",
    "    # Combine (Concatenate) the dataframes vertically\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    print(f\"Data combined successfully.\")\n",
    "    print(f\"File 1 shape: {df1.shape}\")\n",
    "    print(f\"File 2 shape: {df2.shape}\")\n",
    "    print(f\"Total shape:  {combined_df.shape}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Execute loading\n",
    "df = load_and_combine_data(path1, path2)\n",
    "\n",
    "# Check text columns\n",
    "if df is not None:\n",
    "    text_cols = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "    existing_cols = [c for c in text_cols if c in df.columns]\n",
    "    print(f\"Text columns found: {existing_cols}\")\n",
    "    display(df[existing_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Sanity Check: Duplicate IDs\n",
    "\n",
    "In this step, we check whether there are **duplicate `id` values** in the dataset.\n",
    "\n",
    "Since the data was merged from multiple files, duplicate IDs may cause problems in later steps such as merging or modeling.\n",
    "\n",
    "- If **no duplicate IDs** are found, the data integrity is safe.\n",
    "- If **duplicate IDs** exist, they are removed to avoid conflicts.\n",
    "\n",
    "This check helps ensure that each listing in the dataset has a **unique identifier**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CHECK: DUPLICATE IDS\n",
    "# ==========================================\n",
    "# Since we merged data from different files, we check for ID collisions.\n",
    "duplicate_count = df['id'].duplicated().sum()\n",
    "\n",
    "print(f\"Checking for duplicate IDs...\")\n",
    "if duplicate_count == 0:\n",
    "    print(f\"Test Passed: No duplicate IDs found. Data integrity is safe.\")\n",
    "else:\n",
    "    print(f\"WARNING: Found {duplicate_count} duplicate IDs.\")\n",
    "    # If duplicates exist, we drop them to prevent merge issues later\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    print(f\"Duplicates dropped. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Step 2: Text Preprocessing (T1.8)\n",
    "In this step, we clean the text data to make it ready for analysis. \n",
    "We define a function `preprocess_text` that performs the following operations:\n",
    "1.  **Lowercasing**: Converts all text to small letters.\n",
    "2.  **Noise Removal**: Removes HTML tags (like `<br>`), URLs, and special characters.\n",
    "3.  **Tokenization**: Splits sentences into individual words.\n",
    "4.  **Stopword Removal**: Removes common words (like 'and', 'the') that do not carry specific meaning.\n",
    "5.  **Lemmatization**: Converts words to their root form (e.g., 'running' -> 'run') using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. T1.7: HANDLING MISSING TEXT VALUES\n",
    "# ==========================================\n",
    "\n",
    "# List of text columns to process\n",
    "text_features = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "\n",
    "# Check for missing values before fixing\n",
    "print(\"Missing values BEFORE fixing:\")\n",
    "print(df[text_features].isnull().sum())\n",
    "\n",
    "# Fill NaN (empty) values with an empty string \"\"\n",
    "# We do not use \"No Description\" to avoid adding artificial words to the model.\n",
    "for col in text_features:\n",
    "    df[col] = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for missing values after fixing (Should be 0)\n",
    "print(\"Missing values AFTER fixing:\")\n",
    "print(df[text_features].isnull().sum())\n",
    "\n",
    "# Verify that they are strings\n",
    "print(\"\\nSample check:\")\n",
    "display(df[text_features].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Step 2: Text Preprocessing (T1.8)\n",
    "In this step, we built a cleaning pipeline to prepare the text for analysis.\n",
    "We defined a function `preprocess_text` that performs the following operations:\n",
    "\n",
    "1.  **Lowercasing**: Converted all letters to lowercase to ensure consistency (e.g., \"Home\" becomes \"home\").\n",
    "2.  **Noise Removal**: Removed HTML tags (like `<br>`), URLs, and special characters using Regex.\n",
    "3.  **Tokenization**: Split the text into individual words using the NLTK library.\n",
    "4.  **Stopword Removal**: Removed common words (like 'the', 'is', 'and') that add no specific meaning.\n",
    "5.  **Lemmatization**: Converted words to their root forms (e.g., \"running\" -> \"run\") to group similar concepts.\n",
    "\n",
    "Finally, we applied this function to all text columns (`description`, `host_about`, etc.) and saved the clean versions with a `_clean` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. T1.8: TEXT PREPROCESSING PIPELINE (UPDATED)\n",
    "# ==========================================\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- PRE-CHECK: Download necessary NLTK resources ---\n",
    "# We ensure 'punkt_tab' is available for the new NLTK version\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading missing resource: punkt_tab...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize Lemmatizer and Stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and processes text data:\n",
    "    1. Lowercase\n",
    "    2. Remove HTML tags and URLs\n",
    "    3. Remove non-alphabetic characters\n",
    "    4. Tokenize (Split into words)\n",
    "    5. Remove stopwords\n",
    "    6. Lemmatize (Convert to root form)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove HTML tags (e.g., <br />) using regex\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # 3. Remove URLs (http://...)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    \n",
    "    # 4. Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 5. Tokenization (Split into words)\n",
    "    # Uses the updated punkt_tab logic automatically\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 6. Remove Stopwords & Lemmatization\n",
    "    # We keep words that are NOT in stop_words, and find their root (lemma)\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "# Apply the function to our text columns\n",
    "# We create NEW columns with '_clean' suffix to compare results later\n",
    "print(\"Starting text preprocessing... This might take a minute.\")\n",
    "\n",
    "text_features = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "\n",
    "for col in text_features:\n",
    "    new_col_name = col + \"_clean\"\n",
    "    print(f\"Processing column: {col} -> {new_col_name}\")\n",
    "    df[new_col_name] = df[col].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "\n",
    "# Compare Original vs Cleaned version\n",
    "display(df[['description', 'description_clean']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Step 3: Saving the Optimized NLP Dataset\n",
    "In this final step, we save the processed text data.\n",
    "\n",
    "To maintain a modular project structure and avoid conflicts with other team members' work (who might be cleaning columns like 'price' or 'room_type'), we do **not** save the entire dataset.\n",
    "\n",
    "Instead, we export a specialized **NLP-only dataset** containing:\n",
    "1.  **id**: Essential for merging this data back with the main dataset later.\n",
    "2.  **Original Text Columns**: Required for extracting structural features (e.g., text length, capital letters ratio).\n",
    "3.  **Cleaned Text Columns**: Required for content-based tasks like Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. SAVE CHECKPOINT (NLP PREPROCESSING COMPLETE)\n",
    "# ==========================================\n",
    "import os\n",
    "\n",
    "# Create 'data/processed' folder if it doesn't exist\n",
    "output_folder = \"../../data/processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the output path\n",
    "output_path = os.path.join(output_folder, \"listings_text_cleaned.csv\")\n",
    "\n",
    "# --- OPTIMIZATION: SAVE ONLY RELEVANT COLUMNS ---\n",
    "# We keep 'id' to merge with the main dataset later.\n",
    "# We keep original text columns (for length/structure features).\n",
    "# We keep cleaned text columns (for sentiment/content analysis).\n",
    "\n",
    "relevant_columns = ['id'] + text_features + [col + \"_clean\" for col in text_features]\n",
    "\n",
    "# Create a smaller dataframe with only NLP data\n",
    "df_nlp = df[relevant_columns].copy()\n",
    "\n",
    "# Save only the NLP-related data\n",
    "df_nlp.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"NLP Preprocessing Pipeline (T1.7 & T1.8) completed successfully!\")\n",
    "print(f\"Optimized NLP dataset saved to: {output_path}\")\n",
    "print(f\"Original shape: {df.shape} -> Optimized shape: {df_nlp.shape}\")\n",
    "print(f\"Columns saved: {df_nlp.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
