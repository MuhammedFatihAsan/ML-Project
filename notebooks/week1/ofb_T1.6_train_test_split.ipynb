{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä TASK 1.6: TRAIN-TEST SPLIT & FEATURE SCALING\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Loaded dataset: 19,912 rows √ó 31 columns\n",
      "\n",
      "Dataset columns:\n",
      "   - id: 1 column\n",
      "   - Features: 28 columns\n",
      "   - Targets: 2 columns (value_encoded, value_category)\n",
      "\n",
      "================================================================================\n",
      "üîß SEPARATING FEATURES AND TARGETS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Data Separation Complete:\n",
      "   IDs: 19,912 rows\n",
      "   Features (X): 19,912 rows √ó 28 columns\n",
      "   Target (y_encoded): 19,912 rows\n",
      "   Target (y_category): 19,912 rows\n",
      "\n",
      "üìã Feature List (28 features):\n",
      "    1. fp_score\n",
      "    2. price_normalized\n",
      "    3. price\n",
      "    4. price_per_bathroom\n",
      "    5. price_per_bedroom\n",
      "    6. price_per_person\n",
      "    7. beds\n",
      "    8. accommodates\n",
      "    9. bedrooms\n",
      "   10. estimated_revenue_l365d\n",
      "   11. value_density\n",
      "   12. bathrooms_numeric\n",
      "   13. neighbourhood_target_encoded\n",
      "   14. property_type_frequency\n",
      "   15. review_to_capacity_ratio\n",
      "   16. property_type_label\n",
      "   17. neighbourhood_frequency\n",
      "   18. host_portfolio_intensity\n",
      "   19. host_id\n",
      "   20. space_efficiency\n",
      "   21. host_years\n",
      "   22. estimated_occupancy_l365d\n",
      "   23. space_per_person\n",
      "   24. occupancy_rate\n",
      "   25. availability_rate\n",
      "   26. availability_365\n",
      "   27. review_scores_value\n",
      "   28. booking_flexibility_score\n",
      "\n",
      "üìä Target Distribution:\n",
      "--------------------------------------------------------------------------------\n",
      "   0 (Poor_Value     ):  6,571 (33.00%)\n",
      "   1 (Fair_Value     ):  6,773 (34.01%)\n",
      "   2 (Excellent_Value):  6,568 (32.99%)\n",
      "\n",
      "üîç Data Quality Check:\n",
      "--------------------------------------------------------------------------------\n",
      "   Missing values in features (X): 0\n",
      "   Missing values in target (y): 0\n",
      "   Data types in X: {dtype('float64'): 23, dtype('int64'): 5}\n",
      "   ‚úÖ No missing values - Ready for splitting!\n",
      "\n",
      "================================================================================\n",
      "‚úÇÔ∏è TRAIN-TEST SPLIT (80-20)\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Split Complete:\n",
      "   Training set: 15,929 rows (80.0%)\n",
      "   Test set:     3,983 rows (20.0%)\n",
      "   Features:     28 columns\n",
      "\n",
      "üìä Class Distribution Verification:\n",
      "--------------------------------------------------------------------------------\n",
      "Class                |     Original |        Train |         Test\n",
      "--------------------------------------------------------------------------------\n",
      "Poor_Value           |       33.00% |       33.00% |       32.99%\n",
      "Fair_Value           |       34.01% |       34.01% |       34.02%\n",
      "Excellent_Value      |       32.99% |       32.98% |       32.99%\n",
      "\n",
      "‚úÖ Stratification successful - class distributions maintained!\n",
      "\n",
      "================================================================================\n",
      "üìè FEATURE SCALING (STANDARDSCALER)\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Fitting StandardScaler on training data...\n",
      "   Formula: z = (x - Œº) / œÉ\n",
      "   Where: Œº = mean, œÉ = standard deviation\n",
      "\n",
      "‚úÖ Scaling Complete:\n",
      "   Training set scaled: 15,929 rows √ó 28 columns\n",
      "   Test set scaled:     3,983 rows √ó 28 columns\n",
      "\n",
      "üìä Scaling Statistics (First 5 Features):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature                        |   Original Mean |    Original Std |     Scaled Mean |      Scaled Std\n",
      "----------------------------------------------------------------------------------------------------\n",
      "fp_score                       |          0.7493 |          0.2976 |          0.0000 |          1.0000\n",
      "price_normalized               |          0.2806 |          0.1766 |         -0.0000 |          1.0000\n",
      "price                          |        198.7105 |        118.1462 |          0.0000 |          1.0000\n",
      "price_per_bathroom             |        148.6769 |         78.9496 |          0.0000 |          1.0000\n",
      "price_per_bedroom              |        128.3597 |         67.6534 |          0.0000 |          1.0000\n",
      "\n",
      "‚úÖ All features now have mean ‚âà 0 and std ‚âà 1\n",
      "\n",
      "üíæ Saved scaler: models/standard_scaler.pkl\n",
      "\n",
      "================================================================================\n",
      "üíæ SAVING PROCESSED DATASETS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Saved scaled features:\n",
      "   - data/processed/X_train_scaled.csv (15,929 √ó 29)\n",
      "   - data/processed/X_test_scaled.csv (3,983 √ó 29)\n",
      "\n",
      "‚úÖ Saved targets:\n",
      "   - data/processed/y_train.csv (15,929 √ó 3)\n",
      "   - data/processed/y_test.csv (3,983 √ó 3)\n",
      "\n",
      "‚úÖ Saved unscaled features (for reference):\n",
      "   - data/processed/X_train_unscaled.csv (15,929 √ó 29)\n",
      "   - data/processed/X_test_unscaled.csv (3,983 √ó 29)\n",
      "\n",
      "================================================================================\n",
      "üìä GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Creating before/after scaling comparison...\n",
      "   ‚úì Saved: outputs/figures/scaling_comparison.png\n",
      "\n",
      "2Ô∏è‚É£ Creating train-test split distribution plot...\n",
      "   ‚úì Saved: outputs/figures/train_test_split_distribution.png\n",
      "\n",
      "3Ô∏è‚É£ Creating feature scale comparison for all features...\n",
      "   ‚úì Saved: outputs/figures/feature_scale_comparison.png\n",
      "\n",
      "‚úÖ All 3 visualizations generated successfully!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TASK 1.6: TRAIN-TEST SPLIT & SCALING - COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìã SUMMARY REPORT\n",
      "--------------------------------------------------------------------------------\n",
      "Original dataset: 19,912 rows √ó 31 columns\n",
      "Features: 28\n",
      "Training samples: 15,929 (80.0%)\n",
      "Test samples: 3,983 (20.0%)\n",
      "Split ratio: 80-20 (stratified)\n",
      "Scaling method: StandardScaler (z-score normalization)\n",
      "\n",
      "üìÅ FILES GENERATED\n",
      "--------------------------------------------------------------------------------\n",
      "Data Files (data/processed/):\n",
      "   ‚úì X_train_scaled.csv (15,929 √ó 29)\n",
      "   ‚úì X_test_scaled.csv (3,983 √ó 29)\n",
      "   ‚úì y_train.csv (15,929 √ó 3)\n",
      "   ‚úì y_test.csv (3,983 √ó 3)\n",
      "   ‚úì X_train_unscaled.csv (reference)\n",
      "   ‚úì X_test_unscaled.csv (reference)\n",
      "\n",
      "Model Files (models/):\n",
      "   ‚úì standard_scaler.pkl (fitted StandardScaler)\n",
      "\n",
      "Visualization Files (outputs/figures/):\n",
      "   ‚úì scaling_comparison.png\n",
      "   ‚úì train_test_split_distribution.png\n",
      "   ‚úì feature_scale_comparison.png\n",
      "\n",
      "‚úÖ DATA QUALITY CHECK\n",
      "--------------------------------------------------------------------------------\n",
      "   Missing values in X_train: 0\n",
      "   Missing values in X_test: 0\n",
      "   Missing values in y_train: 0\n",
      "   Missing values in y_test: 0\n",
      "   Stratification maintained: ‚úì\n",
      "   Features scaled (mean‚âà0, std‚âà1): ‚úì\n",
      "   Scaler saved for inference: ‚úì\n",
      "   Ready for model training: ‚úì\n",
      "\n",
      "üéØ SCALING VERIFICATION\n",
      "--------------------------------------------------------------------------------\n",
      "   Mean of scaled features (should be ‚âà0): -0.000000\n",
      "   Std of scaled features (should be ‚âà1): 1.000031\n",
      "\n",
      "================================================================================\n",
      "‚ú® TASK 1.6 SUCCESSFULLY COMPLETED! ‚ú®\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "Path('../../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs/figures').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TASK 1.6: TRAIN-TEST SPLIT & FEATURE SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the final selected features dataset\n",
    "df = pd.read_csv('../../data/processed/listings_final_selected_features.csv')\n",
    "print(f\"\\n‚úÖ Loaded dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset columns:\")\n",
    "print(f\"   - id: 1 column\")\n",
    "print(f\"   - Features: {df.shape[1] - 3} columns\")\n",
    "print(f\"   - Targets: 2 columns (value_encoded, value_category)\")\n",
    "\n",
    "# Separate features and targets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß SEPARATING FEATURES AND TARGETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract ID, features, and targets\n",
    "ids = df['id']\n",
    "X = df.drop(['id', 'value_encoded', 'value_category'], axis=1)\n",
    "y_encoded = df['value_encoded']  # Numeric target (0, 1, 2)\n",
    "y_category = df['value_category']  # Categorical target (Poor_Value, Fair_Value, Excellent_Value)\n",
    "\n",
    "print(f\"\\n‚úÖ Data Separation Complete:\")\n",
    "print(f\"   IDs: {ids.shape[0]:,} rows\")\n",
    "print(f\"   Features (X): {X.shape[0]:,} rows √ó {X.shape[1]} columns\")\n",
    "print(f\"   Target (y_encoded): {y_encoded.shape[0]:,} rows\")\n",
    "print(f\"   Target (y_category): {y_category.shape[0]:,} rows\")\n",
    "\n",
    "# Display feature names\n",
    "print(f\"\\nüìã Feature List ({X.shape[1]} features):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Display target distribution\n",
    "print(f\"\\nüìä Target Distribution:\")\n",
    "print(\"-\" * 80)\n",
    "for val in sorted(y_encoded.unique()):\n",
    "    count = (y_encoded == val).sum()\n",
    "    pct = (count / len(y_encoded)) * 100\n",
    "    label = ['Poor_Value', 'Fair_Value', 'Excellent_Value'][int(val)]\n",
    "    print(f\"   {val} ({label:15s}): {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüîç Data Quality Check:\")\n",
    "print(\"-\" * 80)\n",
    "missing_X = X.isnull().sum().sum()\n",
    "missing_y = y_encoded.isnull().sum()\n",
    "print(f\"   Missing values in features (X): {missing_X}\")\n",
    "print(f\"   Missing values in target (y): {missing_y}\")\n",
    "print(f\"   Data types in X: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "if missing_X > 0 or missing_y > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Missing values detected!\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No missing values - Ready for splitting!\")\n",
    "\n",
    "# Train-Test Split\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÇÔ∏è TRAIN-TEST SPLIT (80-20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Perform stratified split to maintain class distribution\n",
    "X_train, X_test, y_train_encoded, y_test_encoded, y_train_category, y_test_category, ids_train, ids_test = train_test_split(\n",
    "    X, y_encoded, y_category, ids,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Stratified split to maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Split Complete:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} rows ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set:     {X_test.shape[0]:,} rows ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Features:     {X_train.shape[1]} columns\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nüìä Class Distribution Verification:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Class':<20s} | {'Original':>12s} | {'Train':>12s} | {'Test':>12s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for val in sorted(y_encoded.unique()):\n",
    "    label = ['Poor_Value', 'Fair_Value', 'Excellent_Value'][int(val)]\n",
    "    orig_pct = (y_encoded == val).sum() / len(y_encoded) * 100\n",
    "    train_pct = (y_train_encoded == val).sum() / len(y_train_encoded) * 100\n",
    "    test_pct = (y_test_encoded == val).sum() / len(y_test_encoded) * 100\n",
    "    print(f\"{label:<20s} | {orig_pct:11.2f}% | {train_pct:11.2f}% | {test_pct:11.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Stratification successful - class distributions maintained!\")\n",
    "\n",
    "# Feature Scaling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìè FEATURE SCALING (STANDARDSCALER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚è≥ Fitting StandardScaler on training data...\")\n",
    "print(f\"   Formula: z = (x - Œº) / œÉ\")\n",
    "print(f\"   Where: Œº = mean, œÉ = standard deviation\")\n",
    "\n",
    "# Initialize and fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use training statistics\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\n‚úÖ Scaling Complete:\")\n",
    "print(f\"   Training set scaled: {X_train_scaled.shape[0]:,} rows √ó {X_train_scaled.shape[1]} columns\")\n",
    "print(f\"   Test set scaled:     {X_test_scaled.shape[0]:,} rows √ó {X_test_scaled.shape[1]} columns\")\n",
    "\n",
    "# Display scaling statistics for first 5 features\n",
    "print(f\"\\nüìä Scaling Statistics (First 5 Features):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Feature':<30s} | {'Original Mean':>15s} | {'Original Std':>15s} | {'Scaled Mean':>15s} | {'Scaled Std':>15s}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, col in enumerate(X.columns[:5]):\n",
    "    orig_mean = X_train[col].mean()\n",
    "    orig_std = X_train[col].std()\n",
    "    scaled_mean = X_train_scaled[col].mean()\n",
    "    scaled_std = X_train_scaled[col].std()\n",
    "    print(f\"{col:<30s} | {orig_mean:15.4f} | {orig_std:15.4f} | {scaled_mean:15.4f} | {scaled_std:15.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All features now have mean ‚âà 0 and std ‚âà 1\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_path = '../../models/standard_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\nüíæ Saved scaler: models/standard_scaler.pkl\")\n",
    "\n",
    "# Save Processed Datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING PROCESSED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create train and test DataFrames with IDs\n",
    "train_data = pd.DataFrame({\n",
    "    'id': ids_train.values\n",
    "})\n",
    "train_data = pd.concat([train_data, X_train_scaled.reset_index(drop=True)], axis=1)\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'id': ids_test.values\n",
    "})\n",
    "test_data = pd.concat([test_data, X_test_scaled.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save features (scaled)\n",
    "train_data.to_csv('../../data/processed/X_train_scaled.csv', index=False)\n",
    "test_data.to_csv('../../data/processed/X_test_scaled.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved scaled features:\")\n",
    "print(f\"   - data/processed/X_train_scaled.csv ({X_train_scaled.shape[0]:,} √ó {train_data.shape[1]})\")\n",
    "print(f\"   - data/processed/X_test_scaled.csv ({X_test_scaled.shape[0]:,} √ó {test_data.shape[1]})\")\n",
    "\n",
    "# Save targets (encoded)\n",
    "y_train_df = pd.DataFrame({\n",
    "    'id': ids_train.values,\n",
    "    'value_encoded': y_train_encoded.values,\n",
    "    'value_category': y_train_category.values\n",
    "})\n",
    "y_test_df = pd.DataFrame({\n",
    "    'id': ids_test.values,\n",
    "    'value_encoded': y_test_encoded.values,\n",
    "    'value_category': y_test_category.values\n",
    "})\n",
    "\n",
    "y_train_df.to_csv('../../data/processed/y_train.csv', index=False)\n",
    "y_test_df.to_csv('../../data/processed/y_test.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved targets:\")\n",
    "print(f\"   - data/processed/y_train.csv ({y_train_df.shape[0]:,} √ó {y_train_df.shape[1]})\")\n",
    "print(f\"   - data/processed/y_test.csv ({y_test_df.shape[0]:,} √ó {y_test_df.shape[1]})\")\n",
    "\n",
    "# Also save unscaled versions for reference\n",
    "X_train_unscaled = pd.DataFrame({'id': ids_train.values})\n",
    "X_train_unscaled = pd.concat([X_train_unscaled, X_train.reset_index(drop=True)], axis=1)\n",
    "X_test_unscaled = pd.DataFrame({'id': ids_test.values})\n",
    "X_test_unscaled = pd.concat([X_test_unscaled, X_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_train_unscaled.to_csv('../../data/processed/X_train_unscaled.csv', index=False)\n",
    "X_test_unscaled.to_csv('../../data/processed/X_test_unscaled.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved unscaled features (for reference):\")\n",
    "print(f\"   - data/processed/X_train_unscaled.csv ({X_train_unscaled.shape[0]:,} √ó {X_train_unscaled.shape[1]})\")\n",
    "print(f\"   - data/processed/X_test_unscaled.csv ({X_test_unscaled.shape[0]:,} √ó {X_test_unscaled.shape[1]})\")\n",
    "\n",
    "# Generate Visualizations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# VISUALIZATION 1: Before vs After Scaling (4 features)\n",
    "print(\"\\n1Ô∏è‚É£ Creating before/after scaling comparison...\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Select 4 features with different scales for visualization\n",
    "viz_features = X.columns[:4]\n",
    "\n",
    "for i, feature in enumerate(viz_features):\n",
    "    # Before scaling\n",
    "    axes[0, i].hist(X_train[feature], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, i].set_title(f'Before Scaling\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Value', fontsize=10)\n",
    "    axes[0, i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[0, i].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = X_train[feature].mean()\n",
    "    std_val = X_train[feature].std()\n",
    "    axes[0, i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Œº={mean_val:.2f}')\n",
    "    axes[0, i].legend(fontsize=9)\n",
    "    \n",
    "    # After scaling\n",
    "    axes[1, i].hist(X_train_scaled[feature], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1, i].set_title(f'After Scaling\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    axes[1, i].set_xlabel('Standardized Value', fontsize=10)\n",
    "    axes[1, i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[1, i].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = X_train_scaled[feature].mean()\n",
    "    std_val = X_train_scaled[feature].std()\n",
    "    axes[1, i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Œº‚âà{mean_val:.2f}')\n",
    "    axes[1, i].axvline(mean_val + std_val, color='green', linestyle=':', linewidth=2, label=f'œÉ‚âà{std_val:.2f}')\n",
    "    axes[1, i].axvline(mean_val - std_val, color='green', linestyle=':', linewidth=2)\n",
    "    axes[1, i].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Feature Scaling Comparison: Before vs After StandardScaler', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/scaling_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"   ‚úì Saved: outputs/figures/scaling_comparison.png\")\n",
    "\n",
    "# VISUALIZATION 2: Train-Test Split Distribution\n",
    "print(\"\\n2Ô∏è‚É£ Creating train-test split distribution plot...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Class distribution\n",
    "class_labels = ['Poor_Value', 'Fair_Value', 'Excellent_Value']\n",
    "train_counts = [sum(y_train_encoded == i) for i in range(3)]\n",
    "test_counts = [sum(y_test_encoded == i) for i in range(3)]\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, train_counts, width, label='Train', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, test_counts, width, label='Test', color='coral', alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Value Category', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Train-Test Split: Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_labels, fontsize=11)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Split size comparison\n",
    "split_labels = ['Training Set\\n(80%)', 'Test Set\\n(20%)']\n",
    "split_counts = [len(X_train), len(X_test)]\n",
    "colors_split = ['steelblue', 'coral']\n",
    "\n",
    "bars = axes[1].bar(split_labels, split_counts, color=colors_split, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Train-Test Split: Dataset Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and percentages\n",
    "for bar, count in zip(bars, split_counts):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / len(X)) * 100\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Train-Test Split Analysis (80-20 Stratified Split)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/train_test_split_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"   ‚úì Saved: outputs/figures/train_test_split_distribution.png\")\n",
    "\n",
    "# VISUALIZATION 3: Feature Scale Comparison (All Features)\n",
    "print(\"\\n3Ô∏è‚É£ Creating feature scale comparison for all features...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Before scaling - show range of values\n",
    "feature_ranges_before = []\n",
    "feature_names_short = []\n",
    "for col in X.columns:\n",
    "    feature_ranges_before.append([X_train[col].min(), X_train[col].max()])\n",
    "    # Shorten feature names for display\n",
    "    short_name = col[:25] + '...' if len(col) > 25 else col\n",
    "    feature_names_short.append(short_name)\n",
    "\n",
    "feature_ranges_before = np.array(feature_ranges_before)\n",
    "\n",
    "# Plot before scaling\n",
    "y_pos = np.arange(len(X.columns))\n",
    "axes[0].barh(y_pos, feature_ranges_before[:, 1] - feature_ranges_before[:, 0], \n",
    "            left=feature_ranges_before[:, 0], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(feature_names_short, fontsize=8)\n",
    "axes[0].set_xlabel('Value Range', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Feature Ranges BEFORE Scaling', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# After scaling - show range of values\n",
    "feature_ranges_after = []\n",
    "for col in X.columns:\n",
    "    feature_ranges_after.append([X_train_scaled[col].min(), X_train_scaled[col].max()])\n",
    "\n",
    "feature_ranges_after = np.array(feature_ranges_after)\n",
    "\n",
    "# Plot after scaling\n",
    "axes[1].barh(y_pos, feature_ranges_after[:, 1] - feature_ranges_after[:, 0], \n",
    "            left=feature_ranges_after[:, 0], color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_yticks(y_pos)\n",
    "axes[1].set_yticklabels(feature_names_short, fontsize=8)\n",
    "axes[1].set_xlabel('Standardized Value Range', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Feature Ranges AFTER Scaling', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle(f'Feature Scale Comparison: All {len(X.columns)} Features', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/feature_scale_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"   ‚úì Saved: outputs/figures/feature_scale_comparison.png\")\n",
    "\n",
    "print(\"\\n‚úÖ All 3 visualizations generated successfully!\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TASK 1.6: TRAIN-TEST SPLIT & SCALING - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã SUMMARY REPORT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Original dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test samples: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Split ratio: 80-20 (stratified)\")\n",
    "print(f\"Scaling method: StandardScaler (z-score normalization)\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES GENERATED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Data Files (data/processed/):\")\n",
    "print(f\"   ‚úì X_train_scaled.csv ({X_train_scaled.shape[0]:,} √ó {train_data.shape[1]})\")\n",
    "print(f\"   ‚úì X_test_scaled.csv ({X_test_scaled.shape[0]:,} √ó {test_data.shape[1]})\")\n",
    "print(f\"   ‚úì y_train.csv ({y_train_df.shape[0]:,} √ó {y_train_df.shape[1]})\")\n",
    "print(f\"   ‚úì y_test.csv ({y_test_df.shape[0]:,} √ó {y_test_df.shape[1]})\")\n",
    "print(f\"   ‚úì X_train_unscaled.csv (reference)\")\n",
    "print(f\"   ‚úì X_test_unscaled.csv (reference)\")\n",
    "print(\"\\nModel Files (models/):\")\n",
    "print(\"   ‚úì standard_scaler.pkl (fitted StandardScaler)\")\n",
    "print(\"\\nVisualization Files (outputs/figures/):\")\n",
    "print(\"   ‚úì scaling_comparison.png\")\n",
    "print(\"   ‚úì train_test_split_distribution.png\")\n",
    "print(\"   ‚úì feature_scale_comparison.png\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA QUALITY CHECK\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Missing values in X_train: {X_train_scaled.isnull().sum().sum()}\")\n",
    "print(f\"   Missing values in X_test: {X_test_scaled.isnull().sum().sum()}\")\n",
    "print(f\"   Missing values in y_train: {y_train_df.isnull().sum().sum()}\")\n",
    "print(f\"   Missing values in y_test: {y_test_df.isnull().sum().sum()}\")\n",
    "print(f\"   Stratification maintained: ‚úì\")\n",
    "print(f\"   Features scaled (mean‚âà0, std‚âà1): ‚úì\")\n",
    "print(f\"   Scaler saved for inference: ‚úì\")\n",
    "print(f\"   Ready for model training: ‚úì\")\n",
    "\n",
    "print(f\"\\nüéØ SCALING VERIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Mean of scaled features (should be ‚âà0): {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Std of scaled features (should be ‚âà1): {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ú® TASK 1.6 SUCCESSFULLY COMPLETED! ‚ú®\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2fdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
