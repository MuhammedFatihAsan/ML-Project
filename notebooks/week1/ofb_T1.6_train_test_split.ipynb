{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#  IMPORTANT NOTE\n",
    "\n",
    "**This task has been updated to work with landlord-controlled features only.**\n",
    "\n",
    "## What Changed?\n",
    "\n",
    "**Previous Version (DEPRECATED):**\n",
    "- Used `listings_final_selected_features.csv` with review-based features \n",
    "- Included review_scores_*, number_of_reviews, etc. (data leakage) \n",
    "- Achieved unrealistic ~99% accuracy \n",
    "\n",
    "**Current Version (UPDATED):**\n",
    "- Uses `listings_landlord_features_only.csv` from updated Task 1.5 \n",
    "- Only landlord-controlled features (no review data) \n",
    "- Realistic accuracy expected (~65-75%) \n",
    "- Model can predict value for NEW listings \n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "The original feature set included review scores and booking history that don't exist when a landlord first posts a listing. This created **circular logic** - using reviews to predict labels derived from reviews.\n",
    "\n",
    "The updated pipeline ensures our model can make **realistic predictions** for new listings using only information available at posting time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Task 1.6: Train-Test Split & Feature Scaling (Updated)\n",
    "\n",
    "The final step of Week 1 prepares our **landlord-controlled features** for modeling. We implement an 80-20 stratified split, ensuring each value category (Poor, Fair, Excellent) maintains its proportional representation in both training and test sets—critical for unbiased model evaluation. StandardScaler transforms our features to zero mean and unit variance, a prerequisite for distance-based algorithms like SVM and gradient-sensitive methods like neural networks. Crucially, we fit the scaler exclusively on training data before transforming the test set, preventing data leakage. Both scaled and unscaled datasets are preserved alongside the serialized scaler, giving us flexibility for tree-based models that don't require scaling while keeping everything reproducible for deployment.\n",
    "\n",
    "**Key Update:** This task now processes only landlord-controlled features (no review data), ensuring our model can predict value for NEW listings where reviews don't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "Path('../../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs/figures').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../models').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" TASK 1.6: TRAIN-TEST SPLIT & FEATURE SCALING (UPDATED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n Using landlord-controlled features only (no review data)\")\n",
    "\n",
    "# Load the landlord-controlled features dataset from updated Task 1.5\n",
    "df = pd.read_csv('../../data/processed/listings_landlord_features_only.csv')\n",
    "print(f\"\\n Loaded dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"   Source: listings_landlord_features_only.csv (from Task 1.5)\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset composition:\")\n",
    "id_cols = ['id', 'host_id']\n",
    "available_ids = [col for col in id_cols if col in df.columns]\n",
    "print(f\"   - ID columns: {len(available_ids)} ({', '.join(available_ids)})\")\n",
    "print(f\"   - Features: {df.shape[1] - len(available_ids) - 1} columns\")\n",
    "print(f\"   - Target: 1 column (value_category)\")\n",
    "\n",
    "# Separate features and targets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SEPARATING FEATURES AND TARGETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract ID, features, and target\n",
    "ids = df[available_ids] if len(available_ids) > 0 else None\n",
    "X = df.drop(columns=available_ids + ['value_category'])\n",
    "y_category = df['value_category']  # Categorical target\n",
    "\n",
    "# Create encoded version for stratification\n",
    "category_mapping = {'Excellent_Value': 0, 'Fair_Value': 1, 'Poor_Value': 2}\n",
    "y_encoded = y_category.map(category_mapping)\n",
    "\n",
    "print(f\"\\n Data Separation Complete:\")\n",
    "if ids is not None:\n",
    "    print(f\"   IDs: {ids.shape[0]:,} rows × {ids.shape[1]} columns\")\n",
    "print(f\"   Features (X): {X.shape[0]:,} rows × {X.shape[1]} columns\")\n",
    "print(f\"   Target (y_category): {y_category.shape[0]:,} rows\")\n",
    "print(f\"   Target (y_encoded): {y_encoded.shape[0]:,} rows\")\n",
    "\n",
    "# Display feature names\n",
    "print(f\"\\n Feature List ({X.shape[1]} landlord-controlled features):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Display target distribution\n",
    "print(f\"\\n Target Distribution:\")\n",
    "print(\"-\" * 80)\n",
    "for category in sorted(y_category.unique()):\n",
    "    count = (y_category == category).sum()\n",
    "    pct = (count / len(y_category)) * 100\n",
    "    encoded_val = category_mapping[category]\n",
    "    print(f\"   {encoded_val} ({category:15s}): {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n Data Quality Check:\")\n",
    "print(\"-\" * 80)\n",
    "missing_X = X.isnull().sum().sum()\n",
    "missing_y = y_category.isnull().sum()\n",
    "print(f\"   Missing values in features (X): {missing_X}\")\n",
    "print(f\"   Missing values in target (y): {missing_y}\")\n",
    "print(f\"   Data types in X: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "if missing_X > 0 or missing_y > 0:\n",
    "    print(f\"\\n  Warning: Missing values detected!\")\n",
    "else:\n",
    "    print(f\"\\n No missing values - Ready for splitting!\")\n",
    "\n",
    "# Train-Test Split\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  TRAIN-TEST SPLIT (80-20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Perform stratified split to maintain class distribution\n",
    "if ids is not None:\n",
    "    X_train, X_test, y_train_encoded, y_test_encoded, y_train_category, y_test_category, ids_train, ids_test = train_test_split(\n",
    "        X, y_encoded, y_category, ids,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded  # Stratified split to maintain class balance\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train_encoded, y_test_encoded, y_train_category, y_test_category = train_test_split(\n",
    "        X, y_encoded, y_category,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "    ids_train = ids_test = None\n",
    "\n",
    "print(f\"\\n Split Complete:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} rows ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set:     {X_test.shape[0]:,} rows ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Features:     {X_train.shape[1]} columns\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\n Class Distribution Verification:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Class':<20s} | {'Original':>12s} | {'Train':>12s} | {'Test':>12s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "reverse_mapping = {v: k for k, v in category_mapping.items()}\n",
    "for val in sorted(y_encoded.unique()):\n",
    "    label = reverse_mapping[val]\n",
    "    orig_pct = (y_encoded == val).sum() / len(y_encoded) * 100\n",
    "    train_pct = (y_train_encoded == val).sum() / len(y_train_encoded) * 100\n",
    "    test_pct = (y_test_encoded == val).sum() / len(y_test_encoded) * 100\n",
    "    print(f\"{label:<20s} | {orig_pct:11.2f}% | {train_pct:11.2f}% | {test_pct:11.2f}%\")\n",
    "\n",
    "print(f\"\\n Stratification successful - class distributions maintained!\")\n",
    "\n",
    "# Feature Scaling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE SCALING (STANDARDSCALER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Fitting StandardScaler on training data...\")\n",
    "print(f\"   Formula: z = (x - μ) / σ\")\n",
    "print(f\"   Where: μ = mean, σ = standard deviation\")\n",
    "\n",
    "# Initialize and fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use training statistics\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\n Scaling Complete:\")\n",
    "print(f\"   Training set scaled: {X_train_scaled.shape[0]:,} rows × {X_train_scaled.shape[1]} columns\")\n",
    "print(f\"   Test set scaled:     {X_test_scaled.shape[0]:,} rows × {X_test_scaled.shape[1]} columns\")\n",
    "\n",
    "# Display scaling statistics for first 5 features\n",
    "print(f\"\\n Scaling Statistics (First 5 Features):\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Feature':<30s} | {'Original Mean':>15s} | {'Original Std':>15s} | {'Scaled Mean':>15s} | {'Scaled Std':>15s}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, col in enumerate(X.columns[:5]):\n",
    "    orig_mean = X_train[col].mean()\n",
    "    orig_std = X_train[col].std()\n",
    "    scaled_mean = X_train_scaled[col].mean()\n",
    "    scaled_std = X_train_scaled[col].std()\n",
    "    print(f\"{col:<30s} | {orig_mean:15.4f} | {orig_std:15.4f} | {scaled_mean:15.4f} | {scaled_std:15.4f}\")\n",
    "\n",
    "print(f\"\\n✅ All features now have mean ≈ 0 and std ≈ 1\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_path = '../../models/scaler_landlord.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n Saved scaler: models/scaler_landlord.pkl\")\n",
    "\n",
    "# Save Processed Datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SAVING PROCESSED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save unscaled features\n",
    "X_train.to_csv('../../data/processed/X_train_landlord.csv', index=False)\n",
    "X_test.to_csv('../../data/processed/X_test_landlord.csv', index=False)\n",
    "print(f\"\\n Saved unscaled features:\")\n",
    "print(f\"   - data/processed/X_train_landlord.csv ({X_train.shape[0]:,} × {X_train.shape[1]})\")\n",
    "print(f\"   - data/processed/X_test_landlord.csv ({X_test.shape[0]:,} × {X_test.shape[1]})\")\n",
    "\n",
    "# Save scaled features\n",
    "X_train_scaled.to_csv('../../data/processed/X_train_scaled_landlord.csv', index=False)\n",
    "X_test_scaled.to_csv('../../data/processed/X_test_scaled_landlord.csv', index=False)\n",
    "print(f\"\\n Saved scaled features:\")\n",
    "print(f\"   - data/processed/X_train_scaled_landlord.csv ({X_train_scaled.shape[0]:,} × {X_train_scaled.shape[1]})\")\n",
    "print(f\"   - data/processed/X_test_scaled_landlord.csv ({X_test_scaled.shape[0]:,} × {X_test_scaled.shape[1]})\")\n",
    "\n",
    "# Save targets\n",
    "y_train_df = pd.DataFrame({\n",
    "    'value_encoded': y_train_encoded.values,\n",
    "    'value_category': y_train_category.values\n",
    "})\n",
    "y_test_df = pd.DataFrame({\n",
    "    'value_encoded': y_test_encoded.values,\n",
    "    'value_category': y_test_category.values\n",
    "})\n",
    "\n",
    "# Add IDs if available\n",
    "if ids_train is not None:\n",
    "    for col in ids_train.columns:\n",
    "        y_train_df.insert(0, col, ids_train[col].values)\n",
    "        y_test_df.insert(0, col, ids_test[col].values)\n",
    "\n",
    "y_train_df.to_csv('../../data/processed/y_train_landlord.csv', index=False)\n",
    "y_test_df.to_csv('../../data/processed/y_test_landlord.csv', index=False)\n",
    "print(f\"\\n Saved targets:\")\n",
    "print(f\"   - data/processed/y_train_landlord.csv ({y_train_df.shape[0]:,} × {y_train_df.shape[1]})\")\n",
    "print(f\"   - data/processed/y_test_landlord.csv ({y_test_df.shape[0]:,} × {y_test_df.shape[1]})\")\n",
    "\n",
    "# Save IDs separately for NLP merging\n",
    "if ids_train is not None:\n",
    "    ids_train.to_csv('../../data/processed/train_ids.csv', index=False)\n",
    "    ids_test.to_csv('../../data/processed/test_ids.csv', index=False)\n",
    "    print(f\"\\n Saved IDs for NLP merging:\")\n",
    "    print(f\"   - data/processed/train_ids.csv\")\n",
    "    print(f\"   - data/processed/test_ids.csv\")\n",
    "\n",
    "# Generate Visualizations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# VISUALIZATION 1: Before vs After Scaling (4 features)\n",
    "print(\"\\n Creating before/after scaling comparison...\")\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Select 4 features with different scales for visualization\n",
    "viz_features = X.columns[:4]\n",
    "\n",
    "for i, feature in enumerate(viz_features):\n",
    "    # Before scaling\n",
    "    axes[0, i].hist(X_train[feature], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0, i].set_title(f'Before Scaling\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Value', fontsize=10)\n",
    "    axes[0, i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[0, i].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = X_train[feature].mean()\n",
    "    std_val = X_train[feature].std()\n",
    "    axes[0, i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'μ={mean_val:.2f}')\n",
    "    axes[0, i].legend(fontsize=9)\n",
    "    \n",
    "    # After scaling\n",
    "    axes[1, i].hist(X_train_scaled[feature], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1, i].set_title(f'After Scaling\\n{feature}', fontsize=11, fontweight='bold')\n",
    "    axes[1, i].set_xlabel('Standardized Value', fontsize=10)\n",
    "    axes[1, i].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[1, i].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = X_train_scaled[feature].mean()\n",
    "    std_val = X_train_scaled[feature].std()\n",
    "    axes[1, i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'μ≈{mean_val:.2f}')\n",
    "    axes[1, i].axvline(mean_val + std_val, color='green', linestyle=':', linewidth=2, label=f'σ≈{std_val:.2f}')\n",
    "    axes[1, i].axvline(mean_val - std_val, color='green', linestyle=':', linewidth=2)\n",
    "    axes[1, i].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Feature Scaling Comparison: Before vs After StandardScaler (Landlord Features)', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/scaling_comparison_landlord.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/scaling_comparison_landlord.png\")\n",
    "\n",
    "# VISUALIZATION 2: Train-Test Split Distribution\n",
    "print(\"\\n Creating train-test split distribution plot...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Class distribution\n",
    "class_labels = ['Excellent_Value', 'Fair_Value', 'Poor_Value']\n",
    "train_counts = [sum(y_train_encoded == i) for i in range(3)]\n",
    "test_counts = [sum(y_test_encoded == i) for i in range(3)]\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, train_counts, width, label='Train', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, test_counts, width, label='Test', color='coral', alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Value Category', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Train-Test Split: Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_labels, fontsize=11)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height):,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Split size comparison\n",
    "split_labels = ['Training Set\\n(80%)', 'Test Set\\n(20%)']\n",
    "split_counts = [len(X_train), len(X_test)]\n",
    "colors_split = ['steelblue', 'coral']\n",
    "\n",
    "bars = axes[1].bar(split_labels, split_counts, color=colors_split, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Train-Test Split: Dataset Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and percentages\n",
    "for bar, count in zip(bars, split_counts):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / len(X)) * 100\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Train-Test Split Analysis (80-20 Stratified, Landlord Features Only)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/train_test_split_distribution_landlord.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/train_test_split_distribution_landlord.png\")\n",
    "\n",
    "# VISUALIZATION 3: Feature Scale Comparison (All Features)\n",
    "print(\"\\n Creating feature scale comparison for all features...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Before scaling - show range of values\n",
    "feature_ranges_before = []\n",
    "feature_names_short = []\n",
    "for col in X.columns:\n",
    "    feature_ranges_before.append([X_train[col].min(), X_train[col].max()])\n",
    "    # Shorten feature names for display\n",
    "    short_name = col[:25] + '...' if len(col) > 25 else col\n",
    "    feature_names_short.append(short_name)\n",
    "\n",
    "feature_ranges_before = np.array(feature_ranges_before)\n",
    "\n",
    "# Plot before scaling\n",
    "y_pos = np.arange(len(X.columns))\n",
    "axes[0].barh(y_pos, feature_ranges_before[:, 1] - feature_ranges_before[:, 0], \n",
    "            left=feature_ranges_before[:, 0], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(feature_names_short, fontsize=8)\n",
    "axes[0].set_xlabel('Value Range', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Feature Ranges BEFORE Scaling', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# After scaling - show range of values\n",
    "feature_ranges_after = []\n",
    "for col in X.columns:\n",
    "    feature_ranges_after.append([X_train_scaled[col].min(), X_train_scaled[col].max()])\n",
    "\n",
    "feature_ranges_after = np.array(feature_ranges_after)\n",
    "\n",
    "# Plot after scaling\n",
    "axes[1].barh(y_pos, feature_ranges_after[:, 1] - feature_ranges_after[:, 0], \n",
    "            left=feature_ranges_after[:, 0], color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_yticks(y_pos)\n",
    "axes[1].set_yticklabels(feature_names_short, fontsize=8)\n",
    "axes[1].set_xlabel('Standardized Value Range', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Feature Ranges AFTER Scaling', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.suptitle(f'Feature Scale Comparison: All {len(X.columns)} Landlord-Controlled Features', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/feature_scale_comparison_landlord.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/feature_scale_comparison_landlord.png\")\n",
    "\n",
    "print(\"\\n All 3 visualizations generated successfully!\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TASK 1.6: TRAIN-TEST SPLIT & SCALING - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n SUMMARY REPORT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Original dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Features: {X.shape[1]} (landlord-controlled only)\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test samples: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Split ratio: 80-20 (stratified)\")\n",
    "print(f\"Scaling method: StandardScaler (z-score normalization)\")\n",
    "\n",
    "print(f\"\\n FILES GENERATED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Data Files (data/processed/):\")\n",
    "print(f\"    X_train_landlord.csv ({X_train.shape[0]:,} × {X_train.shape[1]})\")\n",
    "print(f\"    X_test_landlord.csv ({X_test.shape[0]:,} × {X_test.shape[1]})\")\n",
    "print(f\"    X_train_scaled_landlord.csv ({X_train_scaled.shape[0]:,} × {X_train_scaled.shape[1]})\")\n",
    "print(f\"    X_test_scaled_landlord.csv ({X_test_scaled.shape[0]:,} × {X_test_scaled.shape[1]})\")\n",
    "print(f\"    y_train_landlord.csv ({y_train_df.shape[0]:,} × {y_train_df.shape[1]})\")\n",
    "print(f\"    y_test_landlord.csv ({y_test_df.shape[0]:,} × {y_test_df.shape[1]})\")\n",
    "if ids_train is not None:\n",
    "    print(f\"    train_ids.csv (for NLP merging)\")\n",
    "    print(f\"    test_ids.csv (for NLP merging)\")\n",
    "print(\"\\nModel Files (models/):\")\n",
    "print(\"    scaler_landlord.pkl (fitted StandardScaler)\")\n",
    "print(\"\\nVisualization Files (outputs/figures/):\")\n",
    "print(\"    scaling_comparison_landlord.png\")\n",
    "print(\"    train_test_split_distribution_landlord.png\")\n",
    "print(\"    feature_scale_comparison_landlord.png\")\n",
    "\n",
    "print(f\"\\n DATA QUALITY CHECK\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"    Missing values in X_train: {X_train_scaled.isnull().sum().sum()}\")\n",
    "print(f\"    Missing values in X_test: {X_test_scaled.isnull().sum().sum()}\")\n",
    "print(f\"    Missing values in y_train: {y_train_df.isnull().sum().sum()}\")\n",
    "print(f\"    Missing values in y_test: {y_test_df.isnull().sum().sum()}\")\n",
    "print(f\"    Stratification maintained\")\n",
    "print(f\"    Features scaled (mean≈0, std≈1)\")\n",
    "print(f\"    Scaler saved for inference\")\n",
    "print(f\"    Ready for model training\")\n",
    "\n",
    "print(f\"\\n SCALING VERIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Mean of scaled features (should be ≈0): {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"   Std of scaled features (should be ≈1): {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\n  IMPORTANT NOTE\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   This dataset contains ONLY landlord-controlled features.\")\n",
    "print(\"   Review-based features have been removed to prevent data leakage.\")\n",
    "print(\"   Expected model accuracy: ~65-75% (realistic for new listings)\")\n",
    "print(\"   Previous accuracy with reviews: ~99% (unrealistic, circular logic)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  TASK 1.6 SUCCESSFULLY COMPLETED! \")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
