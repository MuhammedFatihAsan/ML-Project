{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T1.2: DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "sf_df = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_df = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "# Add city identifier\n",
    "sf_df['city'] = 'San Francisco'\n",
    "sd_df['city'] = 'San Diego'\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([sf_df, sd_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š Combined Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š Columns with Missing Values: {len(missing_df)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Data:\")\n",
    "print(missing_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_missing = missing_df.head(20)\n",
    "plt.barh(top_missing['Column'], top_missing['Missing_Percentage'])\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.title('Top 20 Columns with Missing Values')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization saved: outputs/figures/missing_values_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection - Remove Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define columns to drop (URLs, IDs, text descriptions, etc.)\n",
    "columns_to_drop = [\n",
    "    # URLs and IDs\n",
    "    'listing_url', 'scrape_id', 'picture_url', 'host_url', \n",
    "    'host_thumbnail_url', 'host_picture_url',\n",
    "    \n",
    "    # Text descriptions (too noisy for initial model)\n",
    "    'description', 'neighborhood_overview', 'host_about', 'name',\n",
    "    \n",
    "    # Redundant or highly specific\n",
    "    'source', 'calendar_updated', 'last_scraped', 'calendar_last_scraped',\n",
    "    \n",
    "    # License (mostly missing or not useful)\n",
    "    'license',\n",
    "    \n",
    "    # Neighbourhood group (if empty)\n",
    "    'neighbourhood_group_cleansed',\n",
    "    \n",
    "    # Bathrooms (we'll use bathrooms_text instead)\n",
    "    'bathrooms',\n",
    "    \n",
    "    # Host verifications (complex nested data)\n",
    "    'host_verifications',\n",
    "    \n",
    "    # Amenities (complex nested data - can be processed later)\n",
    "    'amenities'\n",
    "]\n",
    "\n",
    "# Drop columns that exist in the dataframe\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dropped {len(columns_to_drop)} columns\")\n",
    "print(f\"   - Original: {df.shape[1]} columns\")\n",
    "print(f\"   - After dropping: {df_cleaned.shape[1]} columns\")\n",
    "print(f\"\\nâœ“ Remaining columns: {df_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversions and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DATA TYPE CONVERSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4.1 Clean price column (remove $ and commas)\n",
    "if 'price' in df_cleaned.columns:\n",
    "    df_cleaned['price'] = df_cleaned['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "    print(\"\\nâœ“ Cleaned 'price' column (removed $ and commas)\")\n",
    "\n",
    "# 4.2 Convert percentage columns\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].str.rstrip('%').astype(float) / 100\n",
    "        print(f\"âœ“ Converted '{col}' to decimal\")\n",
    "\n",
    "# 4.3 Convert boolean columns\n",
    "boolean_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', \n",
    "                'has_availability', 'instant_bookable']\n",
    "for col in boolean_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].map({'t': 1, 'f': 0})\n",
    "        print(f\"âœ“ Converted '{col}' to binary (0/1)\")\n",
    "\n",
    "# 4.4 Convert date columns\n",
    "date_cols = ['host_since', 'first_review', 'last_review']\n",
    "for col in date_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "        print(f\"âœ“ Converted '{col}' to datetime\")\n",
    "\n",
    "# 4.5 Extract number from bathrooms_text\n",
    "if 'bathrooms_text' in df_cleaned.columns:\n",
    "    df_cleaned['bathrooms_numeric'] = df_cleaned['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    print(\"\\nâœ“ Extracted numeric bathrooms from 'bathrooms_text'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1 Host experience (years as host)\n",
    "if 'host_since' in df_cleaned.columns:\n",
    "    df_cleaned['host_years'] = (pd.Timestamp.now() - df_cleaned['host_since']).dt.days / 365.25\n",
    "    print(\"\\nâœ“ Created 'host_years' feature\")\n",
    "\n",
    "# 5.2 Days since first review\n",
    "if 'first_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_first_review'] = (pd.Timestamp.now() - df_cleaned['first_review']).dt.days\n",
    "    print(\"âœ“ Created 'days_since_first_review' feature\")\n",
    "\n",
    "# 5.3 Days since last review\n",
    "if 'last_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_last_review'] = (pd.Timestamp.now() - df_cleaned['last_review']).dt.days\n",
    "    print(\"âœ“ Created 'days_since_last_review' feature\")\n",
    "\n",
    "# 5.4 Price per person\n",
    "if 'price' in df_cleaned.columns and 'accommodates' in df_cleaned.columns:\n",
    "    df_cleaned['price_per_person'] = df_cleaned['price'] / df_cleaned['accommodates']\n",
    "    print(\"âœ“ Created 'price_per_person' feature\")\n",
    "\n",
    "# 5.5 Reviews per month (if not already present)\n",
    "if 'reviews_per_month' not in df_cleaned.columns:\n",
    "    if 'number_of_reviews' in df_cleaned.columns and 'days_since_first_review' in df_cleaned.columns:\n",
    "        df_cleaned['reviews_per_month'] = (df_cleaned['number_of_reviews'] / \n",
    "                                           (df_cleaned['days_since_first_review'] / 30.44))\n",
    "        print(\"âœ“ Created 'reviews_per_month' feature\")\n",
    "\n",
    "# 5.6 Availability rate\n",
    "if 'availability_365' in df_cleaned.columns:\n",
    "    df_cleaned['availability_rate'] = df_cleaned['availability_365'] / 365\n",
    "    print(\"âœ“ Created 'availability_rate' feature\")\n",
    "\n",
    "# 5.7 Average review score (if multiple review scores exist)\n",
    "review_score_cols = [col for col in df_cleaned.columns if 'review_scores_' in col and col != 'review_scores_rating']\n",
    "if review_score_cols:\n",
    "    df_cleaned['avg_review_score'] = df_cleaned[review_score_cols].mean(axis=1)\n",
    "    print(\"âœ“ Created 'avg_review_score' feature\")\n",
    "\n",
    "# 5.8 Has reviews flag\n",
    "if 'number_of_reviews' in df_cleaned.columns:\n",
    "    df_cleaned['has_reviews'] = (df_cleaned['number_of_reviews'] > 0).astype(int)\n",
    "    print(\"âœ“ Created 'has_reviews' feature\")\n",
    "\n",
    "print(f\"\\nâœ“ Total features after engineering: {df_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 6.1 Drop columns with >50% missing values\n",
    "missing_threshold = 0.5\n",
    "missing_pct = df_cleaned.isnull().sum() / len(df_cleaned)\n",
    "cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "    print(f\"\\nâœ“ Dropped {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values\")\n",
    "    print(f\"   Columns dropped: {cols_to_drop}\")\n",
    "\n",
    "# 6.2 Fill missing values for specific columns\n",
    "# Numeric columns - fill with median\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\nâœ“ Filled missing numeric values with median\")\n",
    "\n",
    "# Categorical columns - fill with mode or 'Unknown'\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        mode_val = df_cleaned[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_cleaned[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            df_cleaned[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(f\"âœ“ Filled missing categorical values with mode or 'Unknown'\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = df_cleaned.isnull().sum().sum()\n",
    "print(f\"\\nâœ“ Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. OUTLIER DETECTION AND HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Focus on price outliers\n",
    "if 'price' in df_cleaned.columns:\n",
    "    # Remove listings with price = 0 or extremely high prices\n",
    "    initial_rows = len(df_cleaned)\n",
    "    \n",
    "    # Remove price = 0\n",
    "    df_cleaned = df_cleaned[df_cleaned['price'] > 0]\n",
    "    \n",
    "    # Remove extreme outliers (using IQR method)\n",
    "    Q1 = df_cleaned['price'].quantile(0.25)\n",
    "    Q3 = df_cleaned['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    df_cleaned = df_cleaned[(df_cleaned['price'] >= lower_bound) & \n",
    "                            (df_cleaned['price'] <= upper_bound)]\n",
    "    \n",
    "    rows_removed = initial_rows - len(df_cleaned)\n",
    "    print(f\"\\nâœ“ Removed {rows_removed} rows with price outliers\")\n",
    "    print(f\"   - Price range: ${df_cleaned['price'].min():.2f} - ${df_cleaned['price'].max():.2f}\")\n",
    "    print(f\"   - Remaining rows: {len(df_cleaned):,}\")\n",
    "\n",
    "# Visualize price distribution after cleaning\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_cleaned['price'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution (After Outlier Removal)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df_cleaned['price'])\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Price Boxplot (After Outlier Removal)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/price_distribution_cleaned.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization saved: outputs/figures/price_distribution_cleaned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify categorical columns (excluding datetime)\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove date columns if they're still object type\n",
    "date_related = ['host_since', 'first_review', 'last_review']\n",
    "categorical_cols = [col for col in categorical_cols if col not in date_related]\n",
    "\n",
    "print(f\"\\nðŸ“Š Categorical columns to encode: {len(categorical_cols)}\")\n",
    "print(f\"   Columns: {categorical_cols[:10]}...\")  # Show first 10\n",
    "\n",
    "# One-hot encode categorical variables with low cardinality\n",
    "low_cardinality_cols = []\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].nunique() <= 10:  # Only encode if <=10 unique values\n",
    "        low_cardinality_cols.append(col)\n",
    "\n",
    "if low_cardinality_cols:\n",
    "    df_encoded = pd.get_dummies(df_cleaned, columns=low_cardinality_cols, drop_first=True)\n",
    "    print(f\"\\nâœ“ One-hot encoded {len(low_cardinality_cols)} categorical columns\")\n",
    "    print(f\"   Columns: {low_cardinality_cols}\")\n",
    "else:\n",
    "    df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Drop remaining high-cardinality categorical columns\n",
    "remaining_categorical = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "if remaining_categorical:\n",
    "    df_encoded = df_encoded.drop(columns=remaining_categorical)\n",
    "    print(f\"\\nâœ“ Dropped {len(remaining_categorical)} high-cardinality categorical columns\")\n",
    "    print(f\"   Columns: {remaining_categorical}\")\n",
    "\n",
    "print(f\"\\nâœ“ Final dataset shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Target Variable (FP Score Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. CREATE TARGET VARIABLE - FP SCORE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate FP Score (Fair Price Score)\n",
    "# FP Score = (Rating / Price) * 100 (scaled)\n",
    "\n",
    "if 'review_scores_rating' in df_encoded.columns and 'price' in df_encoded.columns:\n",
    "    # Filter listings with reviews\n",
    "    df_with_reviews = df_encoded[df_encoded['review_scores_rating'].notna()].copy()\n",
    "    \n",
    "    # Normalize rating (0-5 scale) and price\n",
    "    df_with_reviews['rating_normalized'] = df_with_reviews['review_scores_rating'] / 20  # Convert 0-100 to 0-5\n",
    "    df_with_reviews['price_normalized'] = (df_with_reviews['price'] - df_with_reviews['price'].min()) / \\\n",
    "                                          (df_with_reviews['price'].max() - df_with_reviews['price'].min())\n",
    "    \n",
    "    # Calculate FP Score\n",
    "    df_with_reviews['fp_score'] = df_with_reviews['rating_normalized'] / (df_with_reviews['price_normalized'] + 0.1)\n",
    "    \n",
    "    # Classify into 3 categories based on FP Score\n",
    "    fp_33 = df_with_reviews['fp_score'].quantile(0.33)\n",
    "    fp_67 = df_with_reviews['fp_score'].quantile(0.67)\n",
    "    \n",
    "    def classify_value(fp_score):\n",
    "        if fp_score <= fp_33:\n",
    "            return 'Poor_Value'\n",
    "        elif fp_score <= fp_67:\n",
    "            return 'Fair_Value'\n",
    "        else:\n",
    "            return 'Excellent_Value'\n",
    "    \n",
    "    df_with_reviews['value_category'] = df_with_reviews['fp_score'].apply(classify_value)\n",
    "    \n",
    "    print(f\"\\nâœ“ Created FP Score and Value Category\")\n",
    "    print(f\"   - Listings with reviews: {len(df_with_reviews):,}\")\n",
    "    print(f\"   - FP Score range: {df_with_reviews['fp_score'].min():.2f} - {df_with_reviews['fp_score'].max():.2f}\")\n",
    "    print(f\"\\nðŸ“Š Value Category Distribution:\")\n",
    "    print(df_with_reviews['value_category'].value_counts())\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df_with_reviews['value_category'].value_counts().plot(kind='bar', color=['red', 'orange', 'green'])\n",
    "    plt.xlabel('Value Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Value Categories')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(df_with_reviews['fp_score'], bins=50, edgecolor='black')\n",
    "    plt.xlabel('FP Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('FP Score Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/figures/value_category_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Visualization saved: outputs/figures/value_category_distribution.png\")\n",
    "    \n",
    "    # Use df_with_reviews for further processing\n",
    "    df_final = df_with_reviews.copy()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Warning: 'review_scores_rating' or 'price' not found. Skipping target creation.\")\n",
    "    df_final = df_encoded.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prepare Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. PREPARE FEATURES FOR MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop columns that shouldn't be used as features\n",
    "columns_to_exclude = [\n",
    "    'id', 'host_id', 'value_category', 'fp_score', \n",
    "    'rating_normalized', 'price_normalized',\n",
    "    'review_scores_rating',  # Target leakage\n",
    "    'host_since', 'first_review', 'last_review'  # Datetime columns\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df_final.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Separate features and target\n",
    "X = df_final[feature_cols]\n",
    "y = df_final['value_category']\n",
    "\n",
    "print(f\"\\nâœ“ Features prepared\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "print(f\"   - Number of samples: {X.shape[0]:,}\")\n",
    "print(f\"   - Target variable: value_category\")\n",
    "print(f\"\\nðŸ“Š Feature columns (first 20):\")\n",
    "print(X.columns.tolist()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Data split completed\")\n",
    "print(f\"   - Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nðŸ“Š Training set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nðŸ“Š Test set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"12. FEATURE SCALING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ“ Features scaled using StandardScaler\")\n",
    "print(f\"   - Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"   - Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"13. SAVE PROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../../data/processed', exist_ok=True)\n",
    "\n",
    "# Save cleaned full dataset\n",
    "df_final.to_csv('../../data/processed/listings_cleaned_with_target.csv', index=False)\n",
    "print(\"\\nâœ“ Saved: data/processed/listings_cleaned_with_target.csv\")\n",
    "\n",
    "# Save train-test splits\n",
    "X_train.to_csv('../../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../../data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Saved: data/processed/X_train.csv\")\n",
    "print(\"âœ“ Saved: data/processed/X_test.csv\")\n",
    "print(\"âœ“ Saved: data/processed/y_train.csv\")\n",
    "print(\"âœ“ Saved: data/processed/y_test.csv\")\n",
    "\n",
    "# Save scaled data as numpy arrays\n",
    "np.save('../../data/processed/X_train_scaled.npy', X_train_scaled)\n",
    "np.save('../../data/processed/X_test_scaled.npy', X_test_scaled)\n",
    "\n",
    "print(\"âœ“ Saved: data/processed/X_train_scaled.npy\")\n",
    "print(\"âœ“ Saved: data/processed/X_test_scaled.npy\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "with open('../../data/processed/feature_names.txt', 'w') as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(\"âœ“ Saved: data/processed/feature_names.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"14. PREPROCESSING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "DATA PREPROCESSING COMPLETED SUCCESSFULLY!\n",
    "{'='*80}\n",
    "\n",
    "ORIGINAL DATA:\n",
    "  - San Francisco: 7,780 listings\n",
    "  - San Diego: 13,162 listings\n",
    "  - Combined: {df.shape[0]:,} listings, {df.shape[1]} columns\n",
    "\n",
    "AFTER CLEANING:\n",
    "  - Final dataset: {df_final.shape[0]:,} listings, {df_final.shape[1]} columns\n",
    "  - Features for modeling: {X.shape[1]}\n",
    "  - Training samples: {X_train.shape[0]:,}\n",
    "  - Test samples: {X_test.shape[0]:,}\n",
    "\n",
    "TARGET VARIABLE:\n",
    "  - Name: value_category\n",
    "  - Classes: Poor_Value, Fair_Value, Excellent_Value\n",
    "  - Distribution:\n",
    "{y.value_counts().to_string()}\n",
    "\n",
    "KEY STEPS PERFORMED:\n",
    "  âœ“ Removed irrelevant columns (URLs, IDs, text descriptions)\n",
    "  âœ“ Converted data types (price, percentages, booleans, dates)\n",
    "  âœ“ Feature engineering (host_years, price_per_person, etc.)\n",
    "  âœ“ Handled missing values (dropped >50% missing, imputed rest)\n",
    "  âœ“ Removed outliers (price outliers using IQR method)\n",
    "  âœ“ Encoded categorical variables (one-hot encoding)\n",
    "  âœ“ Created target variable (FP Score classification)\n",
    "  âœ“ Train-test split (80-20, stratified)\n",
    "  âœ“ Feature scaling (StandardScaler)\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - data/processed/listings_cleaned_with_target.csv\n",
    "  - data/processed/X_train.csv\n",
    "  - data/processed/X_test.csv\n",
    "  - data/processed/y_train.csv\n",
    "  - data/processed/y_test.csv\n",
    "  - data/processed/X_train_scaled.npy\n",
    "  - data/processed/X_test_scaled.npy\n",
    "  - data/processed/feature_names.txt\n",
    "\n",
    "VISUALIZATIONS:\n",
    "  - outputs/figures/missing_values_analysis.png\n",
    "  - outputs/figures/price_distribution_cleaned.png\n",
    "  - outputs/figures/value_category_distribution.png\n",
    "\n",
    "NEXT STEPS:\n",
    "  1. Exploratory Data Analysis (EDA)\n",
    "  2. Build baseline models (XGBoost, K-Means)\n",
    "  3. Model evaluation and comparison\n",
    "  4. Hyperparameter tuning\n",
    "  5. Final model selection\n",
    "\n",
    "{'='*80}\n",
    "Task 1.2 COMPLETED! Ready for modeling.\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "os.makedirs('../../outputs/reports', exist_ok=True)\n",
    "with open('../../outputs/reports/preprocessing_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ“ Summary saved to : outputs/reports/preprocessing_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
