{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering\n",
    "**Tasks:** T1.9 (Sentiment Analysis) & T1.10 (Text Features)\n",
    "**Input:** `data/processed/listings_text_cleaned.csv` (Output of mfa_T1.7_T1.8_nlp_pipeline.ipynb)\n",
    "\n",
    "### Plan\n",
    "In this notebook, we extract numerical features from the text data. We use a **Hybrid Approach**:\n",
    "\n",
    "1.  **Setup:** Load libraries and the prepared text dataset.\n",
    "2.  **T1.9 Sentiment Analysis (VADER):**\n",
    "    * We will calculate sentiment scores (Positive, Negative, Neutral) using the **NLTK VADER** tool.\n",
    "    * **Strategy:** We use the **Original (Raw) Text** (e.g., `description`) because VADER uses capitalization (\"GREAT\"), punctuation (\"!!!\"), and emojis to measure emotion intensity.\n",
    "3.  **T1.10 Text Feature Extraction:**\n",
    "    * We will calculate structural features to understand the listing quality:\n",
    "        * **Word Count:** How long is the description?\n",
    "        * **Capital Letter Ratio:** Is the host \"shouting\" in the title?\n",
    "4.  **Save:** Export the new features for the final merge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Loading\n",
    "In this section, we import the necessary libraries and download the **VADER lexicon**, which is a dictionary specifically designed for sentiment analysis in social media and marketing contexts.\n",
    "\n",
    "We also load the processed dataset from data/processed/listings_text_cleaned.csv. We explicitly fill missing values with empty strings to ensure the VADER analyzer does not fail when encountering `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & LOAD DATA\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Download VADER lexicon (Sentiment Dictionary)\n",
    "print(\"Downloading VADER resources...\")\n",
    "try:\n",
    "    nltk.download('vader_lexicon')\n",
    "    print(\"VADER lexicon downloaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading VADER: {e}\")\n",
    "\n",
    "# Load the processed text data from Block A\n",
    "input_path = \"../../data/processed/listings_text_cleaned.csv\"\n",
    "\n",
    "if os.path.exists(input_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # CRITICAL: Fill NaN values that might have reappeared during CSV reload\n",
    "    # We fill them with empty strings to avoid errors in VADER or Length checks\n",
    "    text_cols = [col for col in df.columns if 'id' not in col]\n",
    "    df[text_cols] = df[text_cols].fillna(\"\")\n",
    "    \n",
    "    print(f\"Data Loaded Successfully.\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Check for missing values to ensure safety\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    print(f\"Total missing values after fix: {missing_count}\")\n",
    "    \n",
    "    display(df.head(3))\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {input_path}\")\n",
    "    print(\"Please check if data/processed/listings_text_cleaned.csv completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Step 2: T1.9 - Sentiment Analysis (VADER)\n",
    "In this step, we calculate the sentiment scores using **NLTK VADER**.\n",
    "\n",
    "**Strategy:**\n",
    "We apply VADER to the **Original Text Columns** (`description` and `host_about`) to capture the emotion intensity provided by capitalization and punctuation.\n",
    "\n",
    "**Scope & Exclusions:**\n",
    "* **Included:** `description`, `host_about` (Rich content).\n",
    "* **Excluded `name`:** Too short for reliable sentiment analysis. We will use it for structural features in T1.10 instead.\n",
    "* **Excluded `neighborhood_overview`:** Contains ~40% missing data. Filling these with neutral scores would bias the model significantly.\n",
    "\n",
    "**Output:**\n",
    "We calculate the **Compound Score**, which summarizes the sentiment into a single number between -1 (Negative) and +1 (Positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. T1.9: SENTIMENT ANALYSIS (VADER)\n",
    "# ==========================================\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Calculates the compound sentiment score for a given text.\n",
    "    Returns a float between -1.0 (Negative) and 1.0 (Positive).\n",
    "    \"\"\"\n",
    "    # Safety check for non-string values\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Return only the 'compound' score\n",
    "    return scores['compound']\n",
    "\n",
    "print(\"Starting Sentiment Analysis using VADER...\")\n",
    "\n",
    "# 1. Analyze 'description' (Original text)\n",
    "print(\"Processing: description -> description_sentiment\")\n",
    "df['description_sentiment'] = df['description'].apply(get_sentiment_score)\n",
    "\n",
    "# 2. Analyze 'host_about' (Original text)\n",
    "print(\"Processing: host_about -> host_about_sentiment\")\n",
    "df['host_about_sentiment'] = df['host_about'].apply(get_sentiment_score)\n",
    "\n",
    "print(\"Sentiment Analysis complete.\")\n",
    "\n",
    "# Display results: Show text with its score\n",
    "cols_to_check = ['description', 'description_sentiment']\n",
    "display(df[cols_to_check].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 3: T1.10 - Text Feature Extraction\n",
    "In this step, we extract structural features from the text. These features help the model understand the \"quality\" and \"style\" of the listing.\n",
    "\n",
    "**Features Created:**\n",
    "1.  **`name_length`**: The number of characters in the listing title. (Short titles might be less informative).\n",
    "2.  **`name_upper_ratio`**: The percentage of uppercase letters in the title.\n",
    "    * *Why?* Helps detect \"clickbait\" or aggressive marketing (e.g., \"AMAZING VIEW!!!\").\n",
    "3.  **`desc_length`**: Total character count of the original description.\n",
    "4.  **`desc_word_count`**: The count of meaningful words in the *cleaned* description.\n",
    "    * *Why?* Longer descriptions usually correlate with professional hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. T1.10: TEXT FEATURE EXTRACTION\n",
    "# ==========================================\n",
    "\n",
    "def calculate_upper_ratio(text):\n",
    "    \"\"\"Calculates the ratio of uppercase letters to total length.\"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return 0.0\n",
    "    upper_count = sum(1 for char in text if char.isupper())\n",
    "    return upper_count / len(text)\n",
    "\n",
    "print(\"Starting Feature Extraction...\")\n",
    "\n",
    "# 1. Name Features (Title Analysis)\n",
    "print(\"Processing: name -> name_length & name_upper_ratio\")\n",
    "df['name_length'] = df['name'].apply(lambda x: len(str(x)))\n",
    "df['name_upper_ratio'] = df['name'].apply(calculate_upper_ratio)\n",
    "\n",
    "# 2. Description Features (Structure Analysis)\n",
    "print(\"Processing: description -> desc_length\")\n",
    "df['desc_length'] = df['description'].apply(lambda x: len(str(x)))\n",
    "\n",
    "print(\"Processing: description_clean -> desc_word_count\")\n",
    "# We use the CLEANED version to count actual words (ignoring html tags/stopwords)\n",
    "df['desc_word_count'] = df['description_clean'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"Feature Extraction complete.\")\n",
    "\n",
    "# Display the new structural features\n",
    "new_features = ['name', 'name_length', 'name_upper_ratio', 'desc_length', 'desc_word_count']\n",
    "display(df[new_features].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Step 4: Saving the Final NLP Feature Set\n",
    "We save the final dataset containing the original IDs and the new NLP-derived features.\n",
    "\n",
    "**Filename:** `listings_nlp_features.csv`\n",
    "**Why this name?** To distinguish our work (Text/Sentiment features) from other team members who might be generating physical features (price, room count, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. SAVE FINAL FEATURE SET\n",
    "# ==========================================\n",
    "output_folder = \"../../data/processed\"\n",
    "\n",
    "output_path = os.path.join(output_folder, \"listings_nlp_features.csv\")\n",
    "\n",
    "# We select ONLY the numerical features we created + the ID to merge later.\n",
    "# We DROP the text columns now, as the model only needs numbers.\n",
    "final_columns = [\n",
    "    'id', \n",
    "    'description_sentiment', 'host_about_sentiment',  # T1.9 Features\n",
    "    'name_length', 'name_upper_ratio',                # T1.10 Features\n",
    "    'desc_length', 'desc_word_count'                  # T1.10 Features\n",
    "]\n",
    "\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"NLP Feature Engineering Completed Successfully!\")\n",
    "print(f\"NLP Features saved to: {output_path}\")\n",
    "print(f\"Final Shape: {df_final.shape}\")\n",
    "print(f\"Columns: {df_final.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
