{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Data Cleaning and Preprocessing\n",
    "\n",
    "**IMPORTANT NOTE ON DATA LEAKAGE:**\n",
    "This notebook creates a comprehensive feature set including review-based features. These review features are ESSENTIAL for creating our target variable (`value_category`) which measures \"value for money\" based on rating/price ratio.\n",
    "\n",
    "However, review-based features will be REMOVED from model input in Task 1.5 because:\n",
    "1. New listings have no reviews yet\n",
    "2. We need to predict value for listings without review history\n",
    "3. Using reviews as input features creates data leakage\n",
    "\n",
    "**Strategy:**\n",
    "- Keep ALL features (including reviews) in this task for target creation\n",
    "- Task 1.5 will filter out review-based features from X (input) while keeping them for y (target)\n",
    "- Price MUST remain as a feature - you cannot predict \"value for money\" without knowing the price!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T1.2: DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "sf_df = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_df = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "# Add city identifier\n",
    "sf_df['city'] = 'San Francisco'\n",
    "sd_df['city'] = 'San Diego'\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([sf_df, sd_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n Combined Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n Columns with Missing Values: {len(missing_df)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Data:\")\n",
    "print(missing_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_missing = missing_df.head(20)\n",
    "plt.barh(top_missing['Column'], top_missing['Missing_Percentage'])\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.title('Top 20 Columns with Missing Values')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: outputs/figures/missing_values_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection - Remove Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define columns to drop (URLs, IDs, text descriptions, etc.)\n",
    "columns_to_drop = [\n",
    "    # URLs and IDs\n",
    "    'listing_url', 'scrape_id', 'picture_url', 'host_url', \n",
    "    'host_thumbnail_url', 'host_picture_url',\n",
    "    \n",
    "    # Text descriptions (too noisy for initial model)\n",
    "    'description', 'neighborhood_overview', 'host_about', 'name',\n",
    "    \n",
    "    # Redundant or highly specific\n",
    "    'source', 'calendar_updated', 'last_scraped', 'calendar_last_scraped',\n",
    "    \n",
    "    # License (mostly missing or not useful)\n",
    "    'license',\n",
    "    \n",
    "    # Neighbourhood group (if empty)\n",
    "    'neighbourhood_group_cleansed',\n",
    "    \n",
    "    # Bathrooms (we'll use bathrooms_text instead)\n",
    "    'bathrooms',\n",
    "    \n",
    "    # Host verifications (complex nested data)\n",
    "    'host_verifications',\n",
    "    \n",
    "    # Amenities (complex nested data - can be processed later)\n",
    "    'amenities'\n",
    "]\n",
    "\n",
    "# Drop columns that exist in the dataframe\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"\\n Dropped {len(columns_to_drop)} columns\")\n",
    "print(f\"   - Original: {df.shape[1]} columns\")\n",
    "print(f\"   - After dropping: {df_cleaned.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversions and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DATA TYPE CONVERSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4.1 Clean price column (remove $ and commas)\n",
    "if 'price' in df_cleaned.columns:\n",
    "    df_cleaned['price'] = df_cleaned['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "    print(\"\\n Cleaned 'price' column (removed $ and commas)\")\n",
    "\n",
    "# 4.2 Convert percentage columns\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].str.rstrip('%').astype(float) / 100\n",
    "        print(f\" Converted '{col}' to decimal\")\n",
    "\n",
    "# 4.3 Convert boolean columns\n",
    "boolean_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', \n",
    "                'has_availability', 'instant_bookable']\n",
    "for col in boolean_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].map({'t': 1, 'f': 0})\n",
    "        print(f\" Converted '{col}' to binary (0/1)\")\n",
    "\n",
    "# 4.4 Convert date columns\n",
    "date_cols = ['host_since', 'first_review', 'last_review']\n",
    "for col in date_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "        print(f\" Converted '{col}' to datetime\")\n",
    "\n",
    "# 4.5 Extract number from bathrooms_text\n",
    "if 'bathrooms_text' in df_cleaned.columns:\n",
    "    df_cleaned['bathrooms_numeric'] = df_cleaned['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    print(\"\\nâœ“ Extracted numeric bathrooms from 'bathrooms_text'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1 Host experience (years as host)\n",
    "if 'host_since' in df_cleaned.columns:\n",
    "    df_cleaned['host_years'] = (pd.Timestamp.now() - df_cleaned['host_since']).dt.days / 365.25\n",
    "    print(\"\\n Created 'host_years' feature\")\n",
    "\n",
    "# 5.2 Days since first review\n",
    "if 'first_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_first_review'] = (pd.Timestamp.now() - df_cleaned['first_review']).dt.days\n",
    "    print(\" Created 'days_since_first_review' feature\")\n",
    "\n",
    "# 5.3 Days since last review\n",
    "if 'last_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_last_review'] = (pd.Timestamp.now() - df_cleaned['last_review']).dt.days\n",
    "    print(\" Created 'days_since_last_review' feature\")\n",
    "\n",
    "# 5.4 Price per person (LANDLORD-CONTROLLED)\n",
    "if 'price' in df_cleaned.columns and 'accommodates' in df_cleaned.columns:\n",
    "    df_cleaned['price_per_person'] = df_cleaned['price'] / df_cleaned['accommodates']\n",
    "    print(\" Created 'price_per_person' feature (LANDLORD-CONTROLLED)\")\n",
    "\n",
    "# 5.5 Reviews per month (REVIEW-BASED - will be removed in T1.5)\n",
    "if 'reviews_per_month' not in df_cleaned.columns:\n",
    "    if 'number_of_reviews' in df_cleaned.columns and 'days_since_first_review' in df_cleaned.columns:\n",
    "        df_cleaned['reviews_per_month'] = (df_cleaned['number_of_reviews'] / \n",
    "                    (df_cleaned['days_since_first_review'] / 30.44))\n",
    "        print(\" Created 'reviews_per_month' feature (REVIEW-BASED)\")\n",
    "\n",
    "# 5.6 Availability rate (LANDLORD-CONTROLLED)\n",
    "if 'availability_365' in df_cleaned.columns:\n",
    "    df_cleaned['availability_rate'] = df_cleaned['availability_365'] / 365\n",
    "    print(\" Created 'availability_rate' feature (LANDLORD-CONTROLLED)\")\n",
    "\n",
    "# 5.7 Average review score (REVIEW-BASED - will be removed in T1.5)\n",
    "review_score_cols = [col for col in df_cleaned.columns if 'review_scores_' in col and col != 'review_scores_rating']\n",
    "if review_score_cols:\n",
    "    df_cleaned['avg_review_score'] = df_cleaned[review_score_cols].mean(axis=1)\n",
    "    print(\" Created 'avg_review_score' feature (REVIEW-BASED)\")\n",
    "\n",
    "# 5.8 Has reviews flag (REVIEW-BASED - will be removed in T1.5)\n",
    "if 'number_of_reviews' in df_cleaned.columns:\n",
    "    df_cleaned['has_reviews'] = (df_cleaned['number_of_reviews'] > 0).astype(int)\n",
    "    print(\" Created 'has_reviews' feature (REVIEW-BASED)\")\n",
    "\n",
    "print(f\"\\n Total features after engineering: {df_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 6.1 Drop columns with >50% missing values\n",
    "missing_threshold = 0.5\n",
    "missing_pct = df_cleaned.isnull().sum() / len(df_cleaned)\n",
    "cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "    print(f\"\\n Dropped {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values\")\n",
    "    print(f\"   Columns dropped: {cols_to_drop}\")\n",
    "\n",
    "# 6.2 Fill missing values for specific columns\n",
    "# Numeric columns - fill with median\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\nFilled missing numeric values with median\")\n",
    "\n",
    "# Categorical columns - fill with mode or 'Unknown'\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        mode_val = df_cleaned[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_cleaned[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            df_cleaned[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(f\"Filled missing categorical values with mode or 'Unknown'\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = df_cleaned.isnull().sum().sum()\n",
    "print(f\"\\n Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. OUTLIER DETECTION AND HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Focus on price outliers\n",
    "if 'price' in df_cleaned.columns:\n",
    "    # Remove listings with price = 0 or extremely high prices\n",
    "    initial_rows = len(df_cleaned)\n",
    "    \n",
    "    # Remove price = 0\n",
    "    df_cleaned = df_cleaned[df_cleaned['price'] > 0]\n",
    "    \n",
    "    # Remove extreme outliers (using IQR method)\n",
    "    Q1 = df_cleaned['price'].quantile(0.25)\n",
    "    Q3 = df_cleaned['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    df_cleaned = df_cleaned[(df_cleaned['price'] >= lower_bound) & \n",
    "                    (df_cleaned['price'] <= upper_bound)]\n",
    "    \n",
    "    rows_removed = initial_rows - len(df_cleaned)\n",
    "    print(f\"\\n Removed {rows_removed} rows with price outliers\")\n",
    "    print(f\"   - Price range: ${df_cleaned['price'].min():.2f} - ${df_cleaned['price'].max():.2f}\")\n",
    "    print(f\"   - Remaining rows: {len(df_cleaned):,}\")\n",
    "\n",
    "# Visualize price distribution after cleaning\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_cleaned['price'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution (After Outlier Removal)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df_cleaned['price'])\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Price Boxplot (After Outlier Removal)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/price_distribution_cleaned.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: outputs/figures/price_distribution_cleaned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Target Variable (Value Category)\n",
    "\n",
    "**CRITICAL:** We use review_scores_rating here to create labels. This is CORRECT because:\n",
    "1. We need historical data (with reviews) to learn what makes good/bad value\n",
    "2. The target represents \"what value category WOULD this listing be if it had reviews\"\n",
    "3. Review features will be REMOVED from features(x) in Task 1.5, but kept for creating the target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. CREATE TARGET VARIABLE - VALUE CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate FP Score (Fair Price Score) = Rating / Price\n",
    "# This measures \"value for money\"\n",
    "\n",
    "if 'review_scores_rating' in df_cleaned.columns and 'price' in df_cleaned.columns:\n",
    "    # Filter listings with reviews (needed for labeling)\n",
    "    df_with_reviews = df_cleaned[df_cleaned['review_scores_rating'].notna()].copy()\n",
    "    \n",
    "    # Normalize rating (0-5 scale) and price\n",
    "    df_with_reviews['rating_normalized'] = df_with_reviews['review_scores_rating'] / 20  # Convert 0-100 to 0-5\n",
    "    df_with_reviews['price_normalized'] = (df_with_reviews['price'] - df_with_reviews['price'].min()) / \\\n",
    "                    (df_with_reviews['price'].max() - df_with_reviews['price'].min())\n",
    "    \n",
    "    # Calculate FP Score (higher = better value)\n",
    "    df_with_reviews['fp_score'] = df_with_reviews['rating_normalized'] / (df_with_reviews['price_normalized'] + 0.1)\n",
    "    \n",
    "    # Classify into 3 categories based on FP Score\n",
    "    fp_33 = df_with_reviews['fp_score'].quantile(0.33)\n",
    "    fp_67 = df_with_reviews['fp_score'].quantile(0.67)\n",
    "    \n",
    "    def classify_value(fp_score):\n",
    "        if fp_score <= fp_33:\n",
    "            return 'Poor_Value'\n",
    "        elif fp_score <= fp_67:\n",
    "            return 'Fair_Value'\n",
    "        else:\n",
    "            return 'Excellent_Value'\n",
    "    \n",
    "    df_with_reviews['value_category'] = df_with_reviews['fp_score'].apply(classify_value)\n",
    "    \n",
    "    print(f\"\\n Created FP Score and Value Category\")\n",
    "    print(f\"   - Listings with reviews: {len(df_with_reviews):,}\")\n",
    "    print(f\"   - FP Score range: {df_with_reviews['fp_score'].min():.2f} - {df_with_reviews['fp_score'].max():.2f}\")\n",
    "    print(f\"\\n Value Category Distribution:\")\n",
    "    print(df_with_reviews['value_category'].value_counts())\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df_with_reviews['value_category'].value_counts().plot(kind='bar', color=['red', 'orange', 'green'])\n",
    "    plt.xlabel('Value Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Value Categories')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(df_with_reviews['fp_score'], bins=50, edgecolor='black')\n",
    "    plt.xlabel('FP Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('FP Score Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/figures/value_category_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Visualization saved: outputs/figures/value_category_distribution.png\")\n",
    "    \n",
    "    # Use df_with_reviews for further processing\n",
    "    df_final = df_with_reviews.copy()\n",
    "else:\n",
    "    print(\"\\n Warning: 'review_scores_rating' or 'price' not found. Skipping target creation.\")\n",
    "    df_final = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Data (Before Train-Test Split)\n",
    "\n",
    "**Note:** This dataset contains ALL features including review-based ones.\n",
    "Task 1.3 and 1.4 will use this file.\n",
    "Task 1.5 will filter out review-based features from model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. SAVE CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../../data/processed', exist_ok=True)\n",
    "\n",
    "# Save cleaned full dataset with target\n",
    "df_final.to_csv('../../data/processed/listings_cleaned_with_target.csv', index=False)\n",
    "print(\"\\n Saved: data/processed/listings_cleaned_with_target.csv\")\n",
    "print(f\"   - Shape: {df_final.shape}\")\n",
    "print(f\"   - Contains ALL features (including review-based)\")\n",
    "print(f\"   - Review features will be filtered in Task 1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. PREPROCESSING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "DATA PREPROCESSING COMPLETED SUCCESSFULLY!\n",
    "{'='*80}\n",
    "\n",
    "ORIGINAL DATA:\n",
    "  - San Francisco: 7,780 listings\n",
    "  - San Diego: 13,162 listings\n",
    "  - Combined: {df.shape[0]:,} listings, {df.shape[1]} columns\n",
    "\n",
    "AFTER CLEANING:\n",
    "  - Final dataset: {df_final.shape[0]:,} listings, {df_final.shape[1]} columns\n",
    "\n",
    "TARGET VARIABLE:\n",
    "  - Name: value_category\n",
    "  - Classes: Poor_Value, Fair_Value, Excellent_Value\n",
    "  - Based on FP Score = Rating / Price (measures value for money)\n",
    "  - Distribution:\n",
    "{df_final['value_category'].value_counts().to_string()}\n",
    "\n",
    "KEY STEPS PERFORMED:\n",
    "   Removed irrelevant columns (URLs, IDs, text descriptions)\n",
    "   Converted data types (price, percentages, booleans, dates)\n",
    "   Feature engineering (host_years, price_per_person, etc.)\n",
    "   Handled missing values (dropped >50% missing, imputed rest)\n",
    "   Removed outliers (price outliers using IQR method)\n",
    "   Created target variable (FP Score classification)\n",
    "\n",
    " IMPORTANT - DATA LEAKAGE PREVENTION:\n",
    "  - This dataset contains review-based features (number_of_reviews, review_scores, etc.)\n",
    "  - These features were KEPT for creating the target variable (value_category)\n",
    "  - Task 1.5 will REMOVE review-based features from model input (X)\n",
    "  - Price MUST remain as a feature - cannot predict value without knowing price!\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - data/processed/listings_cleaned_with_target.csv\n",
    "\n",
    "VISUALIZATIONS:\n",
    "  - outputs/figures/missing_values_analysis.png\n",
    "  - outputs/figures/price_distribution_cleaned.png\n",
    "  - outputs/figures/value_category_distribution.png\n",
    "\n",
    "\n",
    "{'='*80}\n",
    "Task 1.2 COMPLETED!\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "os.makedirs('../../outputs/reports', exist_ok=True)\n",
    "with open('../../outputs/reports/T1.2_preprocessing_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n Summary saved to: outputs/reports/T1.2_preprocessing_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
