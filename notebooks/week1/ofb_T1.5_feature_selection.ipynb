{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Task 1.5: Feature Selection & Correlation Analysis\n",
    "\n",
    "Not all features are created equal—and this task separated the signal from the noise. We deployed a triangulated approach using Mutual Information (capturing non-linear dependencies), ANOVA F-scores (measuring class separability), and Pearson correlation (quantifying linear relationships) to rank feature importance from multiple perspectives. VIF analysis then hunted down multicollinearity, flagging redundant features with VIF > 10 that would destabilize our models. The verdict: a powerful reduction from 94 features to just 28 elite predictors—a 70% trim that drains computational burden without sacrificing predictive power. This new feature set represents the pure essence of what drives listing value, setting the stage for efficient, interpretable models in the upcoming work ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directories\n",
    "Path('../../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs').mkdir(parents=True, exist_ok=True)\n",
    "Path('../../outputs/figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" TASK 1.5: FEATURE SELECTION & CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the encoded dataset\n",
    "df = pd.read_csv('../../data/processed/listings_with_categorical_encoding.csv')\n",
    "print(f\"\\n Loaded dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"   Total features: {df.shape[1]}\")\n",
    "print(f\"   Target variable: value_encoded\")\n",
    "\n",
    "# Prepare Features and Target\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" PREPARING FEATURES FOR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate target variable\n",
    "target = 'value_encoded'\n",
    "y = df[target]\n",
    "\n",
    "# Identify feature columns to analyze\n",
    "# Exclude: id, target, original categorical columns, date columns\n",
    "exclude_cols = [\n",
    "    'id', 'value_encoded', 'value_category', 'fp_score',  # <-- added fp_score too\n",
    "    'property_type', 'room_type', 'neighbourhood_cleansed',\n",
    "    'host_since', 'first_review', 'last_review'\n",
    "]\n",
    "\n",
    "# Check if 'city' column exists\n",
    "if 'city' in df.columns:\n",
    "    exclude_cols.append('city')\n",
    "\n",
    "# Get all numeric feature columns\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Further filter to only numeric columns\n",
    "X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"\\n Feature Matrix Prepared:\")\n",
    "print(f\"   Total features for analysis: {X.shape[1]}\")\n",
    "print(f\"   Target distribution:\")\n",
    "for val in sorted(y.unique()):\n",
    "    count = (y == val).sum()\n",
    "    pct = (count / len(y)) * 100\n",
    "    label = ['Poor_Value', 'Fair_Value', 'Excellent_Value'][int(val)]\n",
    "    print(f\"      {val} ({label}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = X.isnull().sum().sum()\n",
    "if missing > 0:\n",
    "    print(f\"\\n Warning: {missing} missing values found. Filling with median...\")\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(f\"\\n No missing values in feature matrix\")\n",
    "\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Correlation Analysis with Target\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CORRELATION ANALYSIS WITH TARGET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlations with target\n",
    "correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n Top 30 Features by Correlation with Target:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (feature, corr) in enumerate(correlations.head(30).items(), 1):\n",
    "    print(f\"   {i:2d}. {feature:45s} | Correlation: {corr:.4f}\")\n",
    "\n",
    "print(f\"\\n Bottom 10 Features by Correlation with Target:\")\n",
    "print(\"-\" * 80)\n",
    "for i, (feature, corr) in enumerate(correlations.tail(10).items(), 1):\n",
    "    print(f\"   {i:2d}. {feature:45s} | Correlation: {corr:.4f}\")\n",
    "\n",
    "# Save correlation results\n",
    "corr_df = pd.DataFrame({\n",
    "    'feature': correlations.index,\n",
    "    'correlation_with_target': correlations.values\n",
    "})\n",
    "corr_df.to_csv('../../outputs/feature_target_correlations.csv', index=False)\n",
    "print(f\"\\n Saved: outputs/feature_target_correlations.csv\")\n",
    "\n",
    "# Remove features with NaN correlation\n",
    "X_clean = X.loc[:, X.corrwith(y).notna()]\n",
    "print(f\"\\n Removed features with NaN correlation. New shape: {X_clean.shape}\")\n",
    "\n",
    "# Recalculate correlations\n",
    "correlations = X_clean.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "# Multicollinearity Analysis (VIF)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" MULTICOLLINEARITY ANALYSIS (VIF)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top 40 features for VIF analysis\n",
    "top_features = correlations.head(40).index.tolist()\n",
    "X_vif = X_clean[top_features].copy()\n",
    "\n",
    "print(f\"\\n Calculating VIF for top 40 features...\")\n",
    "print(f\"(This may take a moment...)\")\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n",
    "                   for i in range(len(X_vif.columns))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(f\"\\n VIF Results (VIF > 10 indicates high multicollinearity):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Feature':<45s} | {'VIF':>10s} | {'Status'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "high_vif = []\n",
    "for idx, row in vif_data.iterrows():\n",
    "    status = \" HIGH\" if row['VIF'] > 10 else \"✅ OK\"\n",
    "    print(f\"{row['feature']:<45s} | {row['VIF']:>10.2f} | {status}\")\n",
    "    if row['VIF'] > 10:\n",
    "        high_vif.append(row['feature'])\n",
    "\n",
    "print(f\"\\n Features with high multicollinearity (VIF > 10): {len(high_vif)}\")\n",
    "if high_vif:\n",
    "    for feat in high_vif:\n",
    "        print(f\"   - {feat}\")\n",
    "\n",
    "# Save VIF results\n",
    "vif_data.to_csv('../../outputs/vif_analysis.csv', index=False)\n",
    "print(f\"\\n Saved: outputs/vif_analysis.csv\")\n",
    "\n",
    "# Feature Importance using Multiple Methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Method 1: Mutual Information\n",
    "print(\"\\n Calculating Mutual Information...\")\n",
    "mi_scores = mutual_info_classif(X_clean, y, random_state=42)\n",
    "mi_importance = pd.Series(mi_scores, index=X_clean.columns).sort_values(ascending=False)\n",
    "\n",
    "# Method 2: ANOVA F-statistic\n",
    "print(\" Calculating ANOVA F-scores...\")\n",
    "f_scores, _ = f_classif(X_clean, y)\n",
    "f_importance = pd.Series(f_scores, index=X_clean.columns).sort_values(ascending=False)\n",
    "\n",
    "# Method 3: Correlation (already calculated)\n",
    "print(\" Using Correlation scores...\")\n",
    "corr_importance = correlations\n",
    "\n",
    "# Combine all methods (normalized)\n",
    "print(\" Combining importance scores...\")\n",
    "\n",
    "# Normalize each method to 0-1 scale\n",
    "mi_norm = (mi_importance - mi_importance.min()) / (mi_importance.max() - mi_importance.min())\n",
    "f_norm = (f_importance - f_importance.min()) / (f_importance.max() - f_importance.min())\n",
    "corr_norm = (corr_importance - corr_importance.min()) / (corr_importance.max() - corr_importance.min())\n",
    "\n",
    "# Combined score (average of all methods)\n",
    "combined_importance = (mi_norm + f_norm + corr_norm) / 3\n",
    "combined_importance = combined_importance.sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n Top 30 Features by Combined Importance Score:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Rank':<6s} | {'Feature':<45s} | {'Combined':>10s} | {'MI':>8s} | {'F-Score':>8s} | {'Corr':>8s}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, feature in enumerate(combined_importance.head(30).index, 1):\n",
    "    print(f\"{i:<6d} | {feature:<45s} | {combined_importance[feature]:>10.4f} | \"\n",
    "          f\"{mi_norm[feature]:>8.4f} | {f_norm[feature]:>8.4f} | {corr_norm[feature]:>8.4f}\")\n",
    "\n",
    "# Save importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': combined_importance.index,\n",
    "    'combined_score': combined_importance.values,\n",
    "    'mutual_information': mi_norm[combined_importance.index].values,\n",
    "    'f_score': f_norm[combined_importance.index].values,\n",
    "    'correlation': corr_norm[combined_importance.index].values\n",
    "})\n",
    "importance_df.to_csv('../../outputs/feature_importance_scores.csv', index=False)\n",
    "print(f\"\\n Saved: outputs/feature_importance_scores.csv\")\n",
    "\n",
    "# Feature Selection Strategy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE SELECTION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy: Remove high VIF features, keep top features by importance\n",
    "# Identify features to remove based on VIF and redundancy\n",
    "features_to_remove = []\n",
    "\n",
    "# Add high VIF features that are redundant\n",
    "for feat in high_vif:\n",
    "    # Keep the feature if it's in top 10 by importance, otherwise consider removing\n",
    "    if feat not in combined_importance.head(10).index:\n",
    "        features_to_remove.append(feat)\n",
    "\n",
    "print(f\"\\n Removing {len(features_to_remove)} highly collinear/redundant features:\")\n",
    "for feat in features_to_remove:\n",
    "    if feat in combined_importance.index:\n",
    "        vif_val = vif_data[vif_data['feature']==feat]['VIF'].values\n",
    "        if len(vif_val) > 0:\n",
    "            print(f\"   - {feat} (VIF: {vif_val[0]:.2f})\")\n",
    "\n",
    "# Get remaining features after removing collinear ones\n",
    "remaining_features = [f for f in combined_importance.index if f not in features_to_remove]\n",
    "\n",
    "# Select top 28 features (target is 25-30)\n",
    "n_features = 28\n",
    "selected_features = remaining_features[:n_features]\n",
    "\n",
    "print(f\"\\n Selected Top {n_features} Features:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Rank':<6s} | {'Feature':<45s} | {'Combined Score':>15s} | {'Correlation':>12s}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"{i:<6d} | {feature:<45s} | {combined_importance[feature]:>15.4f} | {correlations[feature]:>12.4f}\")\n",
    "\n",
    "# Save selected features list\n",
    "selected_features_df = pd.DataFrame({\n",
    "    'rank': range(1, len(selected_features) + 1),\n",
    "    'feature': selected_features,\n",
    "    'combined_score': [combined_importance[f] for f in selected_features],\n",
    "    'correlation': [correlations[f] for f in selected_features]\n",
    "})\n",
    "selected_features_df.to_csv('../../outputs/selected_features_list.csv', index=False)\n",
    "print(f\"\\n Saved: outputs/selected_features_list.csv\")\n",
    "\n",
    "# Create Final Dataset with Selected Features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CREATING FINAL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create final dataset with selected features + target + id\n",
    "final_columns = ['id'] + selected_features + ['value_encoded', 'value_category']\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "print(f\"\\n Final Dataset Shape: {df_final.shape[0]:,} rows x {df_final.shape[1]} columns\")\n",
    "print(f\"\\nColumns included:\")\n",
    "print(f\"   - id (identifier)\")\n",
    "print(f\"   - {len(selected_features)} selected features\")\n",
    "print(f\"   - value_encoded (target - numeric)\")\n",
    "print(f\"   - value_category (target - categorical)\")\n",
    "\n",
    "# Save final dataset\n",
    "df_final.to_csv('../../data/processed/listings_final_selected_features.csv', index=False)\n",
    "print(f\"\\n Saved: data/processed/listings_final_selected_features.csv\")\n",
    "\n",
    "# Create a summary statistics file\n",
    "print(\"\\n Generating summary statistics...\")\n",
    "summary_stats = df_final[selected_features].describe().T\n",
    "summary_stats['missing'] = df_final[selected_features].isnull().sum()\n",
    "summary_stats['dtype'] = df_final[selected_features].dtypes\n",
    "summary_stats.to_csv('../../outputs/selected_features_summary_statistics.csv')\n",
    "print(\" Saved: outputs/selected_features_summary_statistics.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION SECTION \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# VISUALIZATION 1: Correlation Heatmap of Selected Features\n",
    "print(\"\\n Creating correlation heatmap for selected features...\")\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "corr_matrix = df_final[selected_features].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, \n",
    "            cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "plt.title('Correlation Heatmap of Selected 28 Features', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/correlation_heatmap_selected_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/correlation_heatmap_selected_features.png\")\n",
    "\n",
    "# VISUALIZATION 2: Feature Importance Comparison (4-panel)\n",
    "print(\"\\n2️⃣ Creating feature importance comparison plot...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Top 20 features for each method\n",
    "top_n = 20\n",
    "\n",
    "# Panel 1: Mutual Information\n",
    "top_mi = mi_norm.sort_values(ascending=False).head(top_n)\n",
    "axes[0, 0].barh(range(len(top_mi)), top_mi.values, color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(top_mi)))\n",
    "axes[0, 0].set_yticklabels(top_mi.index, fontsize=9)\n",
    "axes[0, 0].set_xlabel('Normalized Score', fontsize=12)\n",
    "axes[0, 0].set_title('Top 20 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Panel 2: F-Score\n",
    "top_f = f_norm.sort_values(ascending=False).head(top_n)\n",
    "axes[0, 1].barh(range(len(top_f)), top_f.values, color='coral')\n",
    "axes[0, 1].set_yticks(range(len(top_f)))\n",
    "axes[0, 1].set_yticklabels(top_f.index, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Normalized Score', fontsize=12)\n",
    "axes[0, 1].set_title('Top 20 Features by ANOVA F-Score', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Panel 3: Correlation\n",
    "top_corr = corr_norm.sort_values(ascending=False).head(top_n)\n",
    "axes[1, 0].barh(range(len(top_corr)), top_corr.values, color='mediumseagreen')\n",
    "axes[1, 0].set_yticks(range(len(top_corr)))\n",
    "axes[1, 0].set_yticklabels(top_corr.index, fontsize=9)\n",
    "axes[1, 0].set_xlabel('Normalized Score', fontsize=12)\n",
    "axes[1, 0].set_title('Top 20 Features by Correlation', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Panel 4: Combined Score\n",
    "top_combined = combined_importance.head(top_n)\n",
    "axes[1, 1].barh(range(len(top_combined)), top_combined.values, color='mediumpurple')\n",
    "axes[1, 1].set_yticks(range(len(top_combined)))\n",
    "axes[1, 1].set_yticklabels(top_combined.index, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Combined Score', fontsize=12)\n",
    "axes[1, 1].set_title('Top 20 Features by Combined Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Importance Comparison Across Methods', fontsize=18, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"   Saved: outputs/figures/feature_importance_comparison.png\")\n",
    "\n",
    "# VISUALIZATION 3: VIF Analysis Visualization\n",
    "print(\"\\n Creating VIF analysis visualization...\")\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Color code by VIF threshold\n",
    "colors = ['red' if vif > 10 else 'orange' if vif > 5 else 'green' \n",
    "          for vif in vif_data['VIF']]\n",
    "\n",
    "bars = ax.barh(range(len(vif_data)), vif_data['VIF'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(vif_data)))\n",
    "ax.set_yticklabels(vif_data['feature'], fontsize=9)\n",
    "ax.set_xlabel('VIF Score', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Variance Inflation Factor (VIF) Analysis - Top 40 Features', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.axvline(x=10, color='red', linestyle='--', linewidth=2, label='High Multicollinearity (VIF > 10)')\n",
    "ax.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='Moderate Multicollinearity (VIF > 5)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add text annotation\n",
    "ax.text(0.98, 0.02, f'Features with VIF > 10: {len(high_vif)}', \n",
    "        transform=ax.transAxes, fontsize=12, verticalalignment='bottom',\n",
    "        horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/vif_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/vif_analysis_visualization.png\")\n",
    "\n",
    "# VISUALIZATION 4: Feature Selection Summary (4-panel)\n",
    "print(\"\\n Creating feature selection summary plot...\")\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Selected Features Correlation with Target\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "selected_corrs = [correlations[f] for f in selected_features]\n",
    "ax1.barh(range(len(selected_features)), selected_corrs, color='steelblue', alpha=0.7)\n",
    "ax1.set_yticks(range(len(selected_features)))\n",
    "ax1.set_yticklabels(selected_features, fontsize=8)\n",
    "ax1.set_xlabel('Absolute Correlation with Target', fontsize=12)\n",
    "ax1.set_title('Selected 28 Features - Correlation with Target', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Panel 2: Feature Selection Process\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "stages = ['Original\\nFeatures', 'After\\nCleaning', 'After VIF\\nAnalysis', 'Final\\nSelected']\n",
    "counts = [X.shape[1], X_clean.shape[1], len(remaining_features), n_features]\n",
    "colors_bar = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "bars = ax2.bar(stages, counts, color=colors_bar, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Feature Selection Process', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(count)}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Panel 3: Distribution of Combined Importance Scores\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.hist(combined_importance.values, bins=30, color='mediumpurple', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(combined_importance[selected_features[-1]], color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Selection Threshold ({combined_importance[selected_features[-1]]:.3f})')\n",
    "ax3.set_xlabel('Combined Importance Score', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Distribution of Combined Importance Scores', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Panel 4: VIF Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "vif_bins = [0, 5, 10, 50, 200, vif_data['VIF'].max() + 1]\n",
    "vif_labels = ['0-5\\n(Low)', '5-10\\n(Moderate)', '10-50\\n(High)', '50-200\\n(Very High)', '>200\\n(Extreme)']\n",
    "vif_counts = pd.cut(vif_data['VIF'], bins=vif_bins, labels=vif_labels).value_counts().sort_index()\n",
    "colors_vif = ['green', 'yellowgreen', 'orange', 'orangered', 'red']\n",
    "bars_vif = ax4.bar(range(len(vif_counts)), vif_counts.values, color=colors_vif, alpha=0.7, \n",
    "                   edgecolor='black', linewidth=2)\n",
    "ax4.set_xticks(range(len(vif_counts)))\n",
    "ax4.set_xticklabels(vif_labels, fontsize=10)\n",
    "ax4.set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('VIF Distribution (Top 40 Features)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "# Add value labels\n",
    "for bar, count in zip(bars_vif, vif_counts.values):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(count)}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Feature Selection Summary Dashboard', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.savefig('../../outputs/figures/feature_selection_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"    Saved: outputs/figures/feature_selection_summary.png\")\n",
    "\n",
    "print(\"\\n All 4 visualizations generated successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TASK 1.5: FEATURE SELECTION & CORRELATION ANALYSIS - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n SUMMARY REPORT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Original features analyzed: {X.shape[1]}\")\n",
    "print(f\"Features after removing NaN correlations: {X_clean.shape[1]}\")\n",
    "print(f\"Features analyzed for VIF: 40\")\n",
    "print(f\"Features with high multicollinearity (VIF > 10): {len(high_vif)}\")\n",
    "print(f\"Features removed due to multicollinearity: {len(features_to_remove)}\")\n",
    "print(f\"Final selected features: {n_features}\")\n",
    "print(f\"Reduction: {((X.shape[1]-n_features)/X.shape[1]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\n KEY STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "selected_corrs = [correlations[f] for f in selected_features]\n",
    "print(f\"Average correlation with target: {np.mean(selected_corrs):.4f}\")\n",
    "print(f\"Median correlation with target: {np.median(selected_corrs):.4f}\")\n",
    "print(f\"Max correlation with target: {max(selected_corrs):.4f}\")\n",
    "print(f\"Min correlation with target: {min(selected_corrs):.4f}\")\n",
    "\n",
    "print(f\"\\n FILES GENERATED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Data Files (data/processed/):\")\n",
    "print(\"    listings_final_selected_features.csv\")\n",
    "print(\"\\nAnalysis Files (outputs/):\")\n",
    "print(\"    feature_target_correlations.csv\")\n",
    "print(\"    vif_analysis.csv\")\n",
    "print(\"    feature_importance_scores.csv\")\n",
    "print(\"    selected_features_list.csv\")\n",
    "print(\"    selected_features_summary_statistics.csv\")\n",
    "print(\"\\nVisualization Files (outputs/figures/):\")\n",
    "print(\"    correlation_heatmap_selected_features.png\")\n",
    "print(\"    feature_importance_comparison.png\")\n",
    "print(\"    vif_analysis_visualization.png\")\n",
    "print(\"    feature_selection_summary.png\")\n",
    "\n",
    "print(f\"\\n DATA QUALITY CHECK\")\n",
    "print(\"-\" * 80)\n",
    "missing_final = df_final[selected_features].isnull().sum().sum()\n",
    "print(f\"   Missing values in selected features: {missing_final}\")\n",
    "print(f\"   All features validated across 3 importance methods: ✓\")\n",
    "print(f\"   High multicollinearity features removed: ✓\")\n",
    "print(f\"   Dataset ready for model training: ✓\")\n",
    "\n",
    "print(f\"\\n NEXT STEPS (Task 1.6)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   1. Train/test split (80-20)\")\n",
    "print(\"   2. Feature scaling/normalization (StandardScaler)\")\n",
    "print(\"   3. Save processed datasets (X_train, X_test, y_train, y_test)\")\n",
    "print(\"   4. Ready for Week 2 model training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TASK 1.5 SUCCESSFULLY COMPLETED! \")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
