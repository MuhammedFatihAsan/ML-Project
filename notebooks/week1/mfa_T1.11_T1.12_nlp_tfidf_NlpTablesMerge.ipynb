{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f4c65d-ad6b-4bfe-bd36-56f6b9c064a7",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering (Part 2) & Merge\n",
    "**Tasks:** T1.11 (TF-IDF Vectorization) & T1.12 (NLP Tables Merge)\n",
    "**Inputs:** <br>1. `data/processed/listings_text_cleaned.csv` (Text Data from mfa_T1.7_T1.8_nlp_pipeline)\n",
    "<br>2. `data/processed/listings_nlp_features.csv` (Sentiment/Structure Data from mfa_T1.9_T1.10_sentiment_features)\n",
    "\n",
    "### Plan\n",
    "This notebook completes the NLP pipeline by generating keyword features and creating the final master dataset for NLP part.\n",
    "\n",
    "1.  **Setup & Load:** Load the cleaned text data and the previously generated NLP features.\n",
    "2.  **Sanity Check:** Verify that row counts match across datasets to ensure data integrity.\n",
    "3.  **T1.11 TF-IDF Transformation:**\n",
    "    * Convert `description_clean` into numerical vectors using TF-IDF.\n",
    "    * Limit to **Top 100 keywords** to focus on the most important terms (e.g., \"luxury\", \"beach\", \"downtown\").\n",
    "4.  **T1.12 Feature Integration (Merge):**\n",
    "    * Merge the new TF-IDF features with the Sentiment & Structural features from `data/processed/listings_nlp_features.csv` using `id`.\n",
    "5.  **Final Save:** Export the complete NLP dataset (`nlp_master_features.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7767258-ea87-4115-b303-b63c01213970",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Loading\n",
    "In this section, we prepare the environment and load the necessary datasets.\n",
    "\n",
    "**Inputs Loaded:**\n",
    "1.  **`listings_text_cleaned.csv` (from mfa_T1.7_T1.8_nlp_pipeline.ipynb):** Contains the cleaned text (`description_clean`) which is the input for the TF-IDF model.\n",
    "2.  **`listings_nlp_features.csv` (from mfa_T1.9_T1.10_sentiment_features.ipynb):** Contains the previously generated Sentiment and Structural features.\n",
    "\n",
    "**Critical Checks:**\n",
    "* **NaN Handling:** We explicitly fill missing values in the text column to prevent the TF-IDF vectorizer from crashing.\n",
    "* **Row Consistency:** We perform a sanity check to ensure both datasets have the exact same number of rows (`id` count) before proceeding to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efb9d97-9bb9-42a8-a72f-ba7cb6b04192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files found.\n",
      "Text Data Loaded. Shape: (20942, 9)\n",
      "Sentiment & Structural Data Loaded. Shape: (20942, 7)\n",
      "Row counts match. Ready for T1.11 and T1.12.\n",
      "Sample Text Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>beautiful craftsman house modern convenience w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12447</td>\n",
       "      <td>san diego hideaway awaits private room wprivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29967</td>\n",
       "      <td>bedroom full bathroom home offer comfort famil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                  description_clean\n",
       "0      6  beautiful craftsman house modern convenience w...\n",
       "1  12447  san diego hideaway awaits private room wprivat...\n",
       "2  29967  bedroom full bathroom home offer comfort famil..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Feature Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description_sentiment</th>\n",
       "      <th>host_about_sentiment</th>\n",
       "      <th>name_length</th>\n",
       "      <th>name_upper_ratio</th>\n",
       "      <th>desc_length</th>\n",
       "      <th>desc_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9451</td>\n",
       "      <td>47</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>506</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12447</td>\n",
       "      <td>0.9551</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>28</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>523</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29967</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>32</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>232</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  description_sentiment  host_about_sentiment  name_length  \\\n",
       "0      6                 0.9167                0.9451           47   \n",
       "1  12447                 0.9551                0.9883           28   \n",
       "2  29967                 0.9274                0.9552           32   \n",
       "\n",
       "   name_upper_ratio  desc_length  desc_word_count  \n",
       "0          0.127660          506               42  \n",
       "1          0.142857          523               47  \n",
       "2          0.062500          232               23  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & LOAD DATA\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define file paths\n",
    "text_data_path = \"../../data/processed/listings_text_cleaned.csv\"\n",
    "features_data_path = \"../../data/processed/listings_nlp_features.csv\"\n",
    "\n",
    "# Check if input files exist\n",
    "if os.path.exists(text_data_path) and os.path.exists(features_data_path):\n",
    "    print(\"Input files found.\")\n",
    "    \n",
    "    # 1. Load Text Data\n",
    "    df_text = pd.read_csv(text_data_path)\n",
    "    # Handle missing values immediately to prevent errors in TF-IDF\n",
    "    df_text['description_clean'] = df_text['description_clean'].fillna(\"\")\n",
    "    print(f\"Text Data Loaded. Shape: {df_text.shape}\")\n",
    "    \n",
    "    # 2. Load NLP Features Data\n",
    "    df_features = pd.read_csv(features_data_path)\n",
    "    print(f\"Sentiment & Structural Data Loaded. Shape: {df_features.shape}\")\n",
    "    \n",
    "    # 3. Sanity Check: Row Count Verification\n",
    "    if df_text.shape[0] == df_features.shape[0]:\n",
    "        print(\"Row counts match. Ready for T1.11 and T1.12.\")\n",
    "        \n",
    "        # Display samples to confirm correct loading\n",
    "        print(\"Sample Text Data:\")\n",
    "        display(df_text[['id', 'description_clean']].head(3))\n",
    "        print(\"Sample Feature Data:\")\n",
    "        display(df_features.head(3))\n",
    "    else:\n",
    "        print(\"WARNING: Row count mismatch between Text Data and Feature Data.\")\n",
    "        print(f\"Text Rows: {df_text.shape[0]}\")\n",
    "        print(f\"Feature Rows: {df_features.shape[0]}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Missing input files. Please ensure fa_T1.7_T1.8_nlp_pipeline.ipynb and mfa_T1.9_T1.10_sentiment_features.ipynb are completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7b1f6-d9fe-4cf0-9680-bf99c0fe0aba",
   "metadata": {},
   "source": [
    "### Step 2: T1.11 - TF-IDF Vectorization & T1.12 - Final Merge\n",
    "In this section, we perform the final feature extraction and dataset consolidation.\n",
    "\n",
    "**1. T1.11 TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "* We convert the `description_clean` text into numerical vectors.\n",
    "* **Settings:** We limit the vocabulary to the **Top 100** most important words to keep the dataset lightweight and avoid overfitting.\n",
    "* **Output:** Columns like `tfidf_beach`, `tfidf_luxury`, etc.\n",
    "\n",
    "**2. T1.12 Merging:**\n",
    "* We combine the **Sentiment/Structure Features** (loaded from `mfa_T1.9_T1.10_sentiment_features`) with the new **TF-IDF Features**.\n",
    "* **Join Key:** We merge strictly on `id` to ensure data integrity.\n",
    "\n",
    "**3. Final Save:**\n",
    "* The consolidated dataset is saved as `nlp_master_features.csv`. This file contains all NLP insights and is ready for the project-wide merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46604054-788b-4ab6-949f-3eedc5ed6d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TF-IDF Transformation...\n",
      "TF-IDF Complete. Created 100 features.\n",
      "Top 10 features example: ['tfidf_access', 'tfidf_amenity', 'tfidf_apartment', 'tfidf_area', 'tfidf_attraction', 'tfidf_away', 'tfidf_balboa', 'tfidf_bar', 'tfidf_bath', 'tfidf_bathroom']\n",
      "\n",
      "Starting Merge Process...\n",
      "Merge Complete.\n",
      "Master Dataset Shape: (20942, 107)\n",
      "\n",
      "SUCCESS: Pipeline Finished.\n",
      "Master NLP Dataset saved to: ../../data/processed/nlp_master_features.csv\n",
      "Columns (First 10): ['id', 'description_sentiment', 'host_about_sentiment', 'name_length', 'name_upper_ratio', 'desc_length', 'desc_word_count', 'tfidf_access', 'tfidf_amenity', 'tfidf_apartment']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. T1.11: TF-IDF VECTORIZATION\n",
    "# ==========================================\n",
    "# We limit to top 100 features to keep the dataset manageable.\n",
    "print(\"Starting TF-IDF Transformation...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100,       # Keep only top 100 important words\n",
    "    stop_words='english',   # Remove common English words\n",
    "    dtype=np.float32        # Use less memory\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned descriptions\n",
    "# Note: df_text was loaded in the previous cell\n",
    "tfidf_matrix = tfidf.fit_transform(df_text['description_clean'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_cols = [f\"tfidf_{word}\" for word in feature_names]\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_cols)\n",
    "\n",
    "# Add ID back to TF-IDF dataframe for merging\n",
    "df_tfidf['id'] = df_text['id']\n",
    "\n",
    "print(f\"TF-IDF Complete. Created {len(tfidf_cols)} features.\")\n",
    "print(f\"Top 10 features example: {tfidf_cols[:10]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. T1.12: MERGE ALL NLP FEATURES\n",
    "# ==========================================\n",
    "print(\"\\nStarting Merge Process...\")\n",
    "\n",
    "# Merge TF-IDF features with Sentiment/Structure features (Block B)\n",
    "# We use 'id' as the key.\n",
    "# df_features was loaded in the previous cell\n",
    "df_master = pd.merge(df_features, df_tfidf, on='id', how='inner')\n",
    "\n",
    "print(\"Merge Complete.\")\n",
    "print(f\"Master Dataset Shape: {df_master.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. SAVE MASTER NLP DATASET\n",
    "# ==========================================\n",
    "output_folder = \"../../data/processed\"\n",
    "output_path = os.path.join(output_folder, \"nlp_master_features.csv\")\n",
    "\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSUCCESS: Pipeline Finished.\")\n",
    "print(f\"Master NLP Dataset saved to: {output_path}\")\n",
    "print(f\"Columns (First 10): {df_master.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf5507-c46c-4476-8b6d-e97cff0f7ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
