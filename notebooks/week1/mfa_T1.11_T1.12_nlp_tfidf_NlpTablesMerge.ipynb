{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering (Part 2) & Merge\n",
    "**Tasks:** T1.11 (TF-IDF Vectorization) & T1.12 (NLP Tables Merge)\n",
    "**Inputs:** <br>1. `data/processed/listings_text_cleaned.csv` (Text Data from mfa_T1.7_T1.8_nlp_pipeline)\n",
    "<br>2. `data/processed/listings_nlp_features.csv` (Sentiment/Structure Data from mfa_T1.9_T1.10_sentiment_features)\n",
    "\n",
    "### Plan\n",
    "This notebook completes the NLP pipeline by generating keyword features and creating the final master dataset for NLP part.\n",
    "\n",
    "1.  **Setup & Load:** Load the cleaned text data and the previously generated NLP features.\n",
    "2.  **Sanity Check:** Verify that row counts match across datasets to ensure data integrity.\n",
    "3.  **T1.11 TF-IDF Transformation:**\n",
    "    * Convert `description_clean` into numerical vectors using TF-IDF.\n",
    "    * Limit to **Top 100 keywords** to focus on the most important terms (e.g., \"luxury\", \"beach\", \"downtown\").\n",
    "4.  **T1.12 Feature Integration (Merge):**\n",
    "    * Merge the new TF-IDF features with the Sentiment & Structural features from `data/processed/listings_nlp_features.csv` using `id`.\n",
    "5.  **Final Save:** Export the complete NLP dataset (`nlp_master_features.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Loading\n",
    "In this section, we prepare the environment and load the necessary datasets.\n",
    "\n",
    "**Inputs Loaded:**\n",
    "1.  **`listings_text_cleaned.csv` (from mfa_T1.7_T1.8_nlp_pipeline.ipynb):** Contains the cleaned text (`description_clean`) which is the input for the TF-IDF model.\n",
    "2.  **`listings_nlp_features.csv` (from mfa_T1.9_T1.10_sentiment_features.ipynb):** Contains the previously generated Sentiment and Structural features.\n",
    "\n",
    "**Critical Checks:**\n",
    "* **NaN Handling:** We explicitly fill missing values in the text column to prevent the TF-IDF vectorizer from crashing.\n",
    "* **Row Consistency:** We perform a sanity check to ensure both datasets have the exact same number of rows (`id` count) before proceeding to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP & LOAD DATA\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define file paths\n",
    "text_data_path = \"../../data/processed/listings_text_cleaned.csv\"\n",
    "features_data_path = \"../../data/processed/listings_nlp_features.csv\"\n",
    "\n",
    "# Check if input files exist\n",
    "if os.path.exists(text_data_path) and os.path.exists(features_data_path):\n",
    "    print(\"Input files found.\")\n",
    "    \n",
    "    # 1. Load Text Data\n",
    "    df_text = pd.read_csv(text_data_path)\n",
    "    # Handle missing values immediately to prevent errors in TF-IDF\n",
    "    df_text['description_clean'] = df_text['description_clean'].fillna(\"\")\n",
    "    print(f\"Text Data Loaded. Shape: {df_text.shape}\")\n",
    "    \n",
    "    # 2. Load NLP Features Data\n",
    "    df_features = pd.read_csv(features_data_path)\n",
    "    print(f\"Sentiment & Structural Data Loaded. Shape: {df_features.shape}\")\n",
    "    \n",
    "    # 3. Sanity Check: Row Count Verification\n",
    "    if df_text.shape[0] == df_features.shape[0]:\n",
    "        print(\"Row counts match. Ready for T1.11 and T1.12.\")\n",
    "        \n",
    "        # Display samples to confirm correct loading\n",
    "        print(\"Sample Text Data:\")\n",
    "        display(df_text[['id', 'description_clean']].head(3))\n",
    "        print(\"Sample Feature Data:\")\n",
    "        display(df_features.head(3))\n",
    "    else:\n",
    "        print(\"WARNING: Row count mismatch between Text Data and Feature Data.\")\n",
    "        print(f\"Text Rows: {df_text.shape[0]}\")\n",
    "        print(f\"Feature Rows: {df_features.shape[0]}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Missing input files. Please ensure fa_T1.7_T1.8_nlp_pipeline.ipynb and mfa_T1.9_T1.10_sentiment_features.ipynb are completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Step 2: T1.11 - TF-IDF Vectorization & T1.12 - Final Merge\n",
    "In this section, we perform the final feature extraction and dataset consolidation.\n",
    "\n",
    "**1. T1.11 TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "* We convert the `description_clean` text into numerical vectors.\n",
    "* **Settings:** We limit the vocabulary to the **Top 100** most important words to keep the dataset lightweight and avoid overfitting.\n",
    "* **Output:** Columns like `tfidf_beach`, `tfidf_luxury`, etc.\n",
    "\n",
    "**2. T1.12 Merging:**\n",
    "* We combine the **Sentiment/Structure Features** (loaded from `mfa_T1.9_T1.10_sentiment_features`) with the new **TF-IDF Features**.\n",
    "* **Join Key:** We merge strictly on `id` to ensure data integrity.\n",
    "\n",
    "**3. Final Save:**\n",
    "* The consolidated dataset is saved as `nlp_master_features.csv`. This file contains all NLP insights and is ready for the project-wide merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. T1.11: TF-IDF VECTORIZATION\n",
    "# ==========================================\n",
    "# We limit to top 100 features to keep the dataset manageable.\n",
    "print(\"Starting TF-IDF Transformation...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100,       # Keep only top 100 important words\n",
    "    stop_words='english',   # Remove common English words\n",
    "    dtype=np.float32        # Use less memory\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned descriptions\n",
    "# Note: df_text was loaded in the previous cell\n",
    "tfidf_matrix = tfidf.fit_transform(df_text['description_clean'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_cols = [f\"tfidf_{word}\" for word in feature_names]\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_cols)\n",
    "\n",
    "# Add ID back to TF-IDF dataframe for merging\n",
    "df_tfidf['id'] = df_text['id']\n",
    "\n",
    "print(f\"TF-IDF Complete. Created {len(tfidf_cols)} features.\")\n",
    "print(f\"Top 10 features example: {tfidf_cols[:10]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. T1.12: MERGE ALL NLP FEATURES\n",
    "# ==========================================\n",
    "print(\"\\nStarting Merge Process...\")\n",
    "\n",
    "# Merge TF-IDF features with Sentiment/Structure features (Block B)\n",
    "# We use 'id' as the key.\n",
    "# df_features was loaded in the previous cell\n",
    "df_master = pd.merge(df_features, df_tfidf, on='id', how='inner')\n",
    "\n",
    "print(\"Merge Complete.\")\n",
    "print(f\"Master Dataset Shape: {df_master.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. SAVE MASTER NLP DATASET\n",
    "# ==========================================\n",
    "output_folder = \"../../data/processed\"\n",
    "output_path = os.path.join(output_folder, \"nlp_master_features.csv\")\n",
    "\n",
    "df_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSUCCESS: Pipeline Finished.\")\n",
    "print(f\"Master NLP Dataset saved to: {output_path}\")\n",
    "print(f\"Columns (First 10): {df_master.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Airbnb Project)",
   "language": "python",
   "name": "airbnb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
