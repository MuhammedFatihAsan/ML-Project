{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# \n",
    "## Airbnb Value Prediction - San Francisco & San Diego\n",
    "\n",
    "**Project Goal:** Predict value-for-money category for Airbnb listings using only landlord-controlled features.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "- **Task 1.1:** Initial Data Exploration\n",
    "- **Task 1.2:** Data Cleaning and Preprocessing\n",
    "- **Task 1.3:** Algebraic Feature Engineering\n",
    "- **Task 1.4:** Categorical Encoding\n",
    "- **Task 1.5:** Feature Selection \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load both datasets\n",
    "sf_df = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_df = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T1.1: Initial Data Exploration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. Dataset Overview\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n San Francisco Dataset:\")\n",
    "print(f\"   - Rows: {sf_df.shape[0]:,}\")\n",
    "print(f\"   - Columns: {sf_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n San Diego Dataset:\")\n",
    "print(f\"   - Rows: {sd_df.shape[0]:,}\")\n",
    "print(f\"   - Columns: {sd_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n Combined Dataset:\")\n",
    "print(f\"   - Total Rows: {sf_df.shape[0] + sd_df.shape[0]:,}\")\n",
    "\n",
    "# Check if columns match\n",
    "sf_cols = set(sf_df.columns)\n",
    "sd_cols = set(sd_df.columns)\n",
    "print(f\"\\n✓ Column names match: {sf_cols == sd_cols}\")\n",
    "\n",
    "if sf_cols != sd_cols:\n",
    "    print(f\"   - Columns only in SF: {sf_cols - sd_cols}\")\n",
    "    print(f\"   - Columns only in SD: {sd_cols - sf_cols}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. Column summary in San Francisco\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Columns: {sf_df.shape[1]}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(sf_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. Data types analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze data types\n",
    "sf_dtypes = sf_df.dtypes.value_counts()\n",
    "print(\"\\n San Francisco Data Types:\")\n",
    "for dtype, count in sf_dtypes.items():\n",
    "    print(f\"   - {dtype}: {count} columns\")\n",
    "\n",
    "sd_dtypes = sd_df.dtypes.value_counts()\n",
    "print(\"\\n San Diego Data Types:\")\n",
    "for dtype, count in sd_dtypes.items():\n",
    "    print(f\"   - {dtype}: {count} columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. Missing values analysis - San Francisco\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sf_missing = sf_df.isnull().sum()\n",
    "sf_missing_pct = (sf_missing / len(sf_df) * 100).round(2)\n",
    "sf_missing_df = pd.DataFrame({\n",
    "    'Column': sf_missing.index,\n",
    "    'Missing_Count': sf_missing.values,\n",
    "    'Missing_Percentage': sf_missing_pct.values\n",
    "})\n",
    "sf_missing_df = sf_missing_df[sf_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n Columns with Missing Values: {len(sf_missing_df)}/{len(sf_df.columns)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Values:\")\n",
    "print(sf_missing_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. Missing Values Analysis - San Diego\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sd_missing = sd_df.isnull().sum()\n",
    "sd_missing_pct = (sd_missing / len(sd_df) * 100).round(2)\n",
    "sd_missing_df = pd.DataFrame({\n",
    "    'Column': sd_missing.index,\n",
    "    'Missing_Count': sd_missing.values,\n",
    "    'Missing_Percentage': sd_missing_pct.values\n",
    "})\n",
    "sd_missing_df = sd_missing_df[sd_missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n Columns with Missing Values: {len(sd_missing_df)}/{len(sd_df.columns)}\")\n",
    "print(\"\\n Top 20 Columns with Most Missing Values:\")\n",
    "print(sd_missing_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. Key numerical features summary - San Francisco\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key numerical columns to analyze\n",
    "key_numerical = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms', \n",
    "                 'minimum_nights', 'maximum_nights', 'number_of_reviews',\n",
    "                 'review_scores_rating', 'review_scores_accuracy', \n",
    "                 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                 'review_scores_communication', 'review_scores_location',\n",
    "                 'review_scores_value']\n",
    "\n",
    "# Check which columns exist\n",
    "existing_numerical = [col for col in key_numerical if col in sf_df.columns]\n",
    "\n",
    "sf_summary = sf_df[existing_numerical].describe().T\n",
    "sf_summary['missing'] = sf_df[existing_numerical].isnull().sum()\n",
    "sf_summary['missing_pct'] = (sf_summary['missing'] / len(sf_df) * 100).round(2)\n",
    "print(sf_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'missing', 'missing_pct']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. Key numerical features summary - San Diego\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sd_summary = sd_df[existing_numerical].describe().T\n",
    "sd_summary['missing'] = sd_df[existing_numerical].isnull().sum()\n",
    "sd_summary['missing_pct'] = (sd_summary['missing'] / len(sd_df) * 100).round(2)\n",
    "print(sd_summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'missing', 'missing_pct']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. Key categorical features analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "key_categorical = ['property_type', 'room_type', 'neighbourhood_cleansed', \n",
    "                   'host_is_superhost', 'instant_bookable']\n",
    "\n",
    "existing_categorical = [col for col in key_categorical if col in sf_df.columns]\n",
    "\n",
    "print(\"\\n San Francisco - Categorical Features:\")\n",
    "for col in existing_categorical:\n",
    "    unique_count = sf_df[col].nunique()\n",
    "    missing = sf_df[col].isnull().sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   - Unique values: {unique_count}\")\n",
    "    print(f\"   - Missing: {missing} ({missing/len(sf_df)*100:.2f}%)\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"   - Value counts:\")\n",
    "        print(sf_df[col].value_counts().head(10).to_string())\n",
    "\n",
    "print(\"\\n San Diego - Categorical Features:\")\n",
    "for col in existing_categorical:\n",
    "    unique_count = sd_df[col].nunique()\n",
    "    missing = sd_df[col].isnull().sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   - Unique values: {unique_count}\")\n",
    "    print(f\"   - Missing: {missing} ({missing/len(sd_df)*100:.2f}%)\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"   - Value counts:\")\n",
    "        print(sd_df[col].value_counts().head(10).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. PRICE ANALYSIS \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n San Francisco - Price Column:\")\n",
    "print(f\"   - Data type: {sf_df['price'].dtype}\")\n",
    "print(f\"   - Sample values: {sf_df['price'].head(10).tolist()}\")\n",
    "print(f\"   - Missing: {sf_df['price'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\n San Diego - Price Column:\")\n",
    "print(f\"   - Data type: {sd_df['price'].dtype}\")\n",
    "print(f\"   - Sample values: {sd_df['price'].head(10).tolist()}\")\n",
    "print(f\"   - Missing: {sd_df['price'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\n  Note: Price column is stored as string with '$' and ',' - needs cleaning in T1.2\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check for duplicates\n",
    "sf_dupes = sf_df.duplicated().sum()\n",
    "sd_dupes = sd_df.duplicated().sum()\n",
    "if sf_dupes > 0 or sd_dupes > 0:\n",
    "    issues.append(f\"Duplicate rows: SF={sf_dupes}, SD={sd_dupes}\")\n",
    "\n",
    "# Check price format\n",
    "if sf_df['price'].dtype == 'object':\n",
    "    issues.append(\"Price column needs cleaning (contains '$' and ',')\")\n",
    "\n",
    "# Check high missing value columns\n",
    "high_missing_sf = sf_missing_df[sf_missing_df['Missing_Percentage'] > 50]\n",
    "high_missing_sd = sd_missing_df[sd_missing_df['Missing_Percentage'] > 50]\n",
    "issues.append(f\"Columns with >50% missing: SF={len(high_missing_sf)}, SD={len(high_missing_sd)}\")\n",
    "\n",
    "# Check for columns with all missing\n",
    "all_missing_sf = sf_missing_df[sf_missing_df['Missing_Percentage'] == 100]\n",
    "all_missing_sd = sd_missing_df[sd_missing_df['Missing_Percentage'] == 100]\n",
    "if len(all_missing_sf) > 0 or len(all_missing_sd) > 0:\n",
    "    issues.append(f\"Columns with 100% missing: SF={len(all_missing_sf)}, SD={len(all_missing_sd)}\")\n",
    "\n",
    "print(\"\\n Issues Found:\")\n",
    "for i, issue in enumerate(issues, 1):\n",
    "    print(f\"{i}. {issue}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    " Task T1.1 Completed: Initial Data Exploration\n",
    "\n",
    " Dataset Overview:\n",
    "   - San Francisco: {sf_df.shape[0]:,} rows × {sf_df.shape[1]} columns\n",
    "   - San Diego: {sd_df.shape[0]:,} rows × {sd_df.shape[1]} columns\n",
    "   - Combined: {sf_df.shape[0] + sd_df.shape[0]:,} rows\n",
    "\n",
    " Key Findings:\n",
    "   1. Both datasets have {sf_df.shape[1]} columns with matching structure\n",
    "   2. {len(sf_missing_df)} columns in SF and {len(sd_missing_df)} columns in SD have missing values\n",
    "   3. Review scores have significant missing values (~30-40%)\n",
    "   4. Text columns (description, host_about etc.) needs NLP processing \n",
    "   5. Categorical encoding needed for property_type, room_type, neighbourhood\n",
    "   \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Task 1.2: Data Cleaning and Preprocessing\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "**Important note on data leakage:**\n",
    "This notebook creates a comprehensive feature set including review-based features. These review features are essential for creating our target variable (`value_category`) which measures \"value for money\" based on rating/price ratio.\n",
    "\n",
    "However, review-based features will be REMOVED from model input in Task 1.5 because:\n",
    "1. New listings have no reviews yet\n",
    "2. We need to predict value for listings without review history\n",
    "3. Using reviews as input features creates data leakage\n",
    "\n",
    "**Strategy:**\n",
    "- Keep all features (including review-based ones) in this task for target creation\n",
    "- Task 1.5 will filter out review-based features from X (input) while keeping them for y (target)\n",
    "- Price MUST remain as a feature - you cannot predict \"value for money\" without knowing the price!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Task 1.2: Data cleaning and preprocessing\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "sf_df = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_df = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "# Add city identifier\n",
    "sf_df['city'] = 'San Francisco'\n",
    "sd_df['city'] = 'San Diego'\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([sf_df, sd_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n Combined Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Total Rows: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "\n",
    "# Filter columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"\\n Columns with Missing Values: {len(missing_df)}\")\n",
    "print(\"\\nTop 20 Columns with Most Missing Data:\")\n",
    "print(missing_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_missing = missing_df.head(20)\n",
    "plt.barh(top_missing['Column'], top_missing['Missing_Percentage'])\n",
    "plt.xlabel('Missing Percentage (%)')\n",
    "plt.title('Top 20 Columns with Missing Values')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/missing_values_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: outputs/figures/missing_values_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Feature Selection - Remove Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define columns to drop (URLs, IDs, text descriptions, etc.)\n",
    "columns_to_drop = [\n",
    "    # URLs and IDs\n",
    "    'listing_url', 'scrape_id', 'picture_url', 'host_url', \n",
    "    'host_thumbnail_url', 'host_picture_url',\n",
    "    \n",
    "    # Text descriptions (too noisy for initial model)\n",
    "    'description', 'neighborhood_overview', 'host_about', 'name',\n",
    "    \n",
    "    # Redundant or highly specific\n",
    "    'source', 'calendar_updated', 'last_scraped', 'calendar_last_scraped',\n",
    "    \n",
    "    # License (mostly missing or not useful)\n",
    "    'license',\n",
    "    \n",
    "    # Neighbourhood group (if empty)\n",
    "    'neighbourhood_group_cleansed',\n",
    "    \n",
    "    # Bathrooms (we'll use bathrooms_text instead)\n",
    "    'bathrooms',\n",
    "    \n",
    "    # Host verifications (complex nested data)\n",
    "    'host_verifications',\n",
    "    \n",
    "    # Amenities (complex nested data - can be processed later)\n",
    "    'amenities'\n",
    "]\n",
    "\n",
    "# Drop columns that exist in the dataframe\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"\\n Dropped {len(columns_to_drop)} columns\")\n",
    "print(f\"   - Original: {df.shape[1]} columns\")\n",
    "print(f\"   - After dropping: {df_cleaned.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Data Type Conversions and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DATA TYPE CONVERSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4.1 Clean price column (remove $ and commas)\n",
    "if 'price' in df_cleaned.columns:\n",
    "    df_cleaned['price'] = df_cleaned['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "    print(\"\\nCleaned 'price' column (removed $ and commas)\")\n",
    "\n",
    "# 4.2 Convert percentage columns\n",
    "percentage_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in percentage_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].str.rstrip('%').astype(float) / 100\n",
    "        print(f\"Converted '{col}' to decimal\")\n",
    "\n",
    "# 4.3 Convert boolean columns\n",
    "boolean_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', \n",
    "                'has_availability', 'instant_bookable']\n",
    "for col in boolean_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].map({'t': 1, 'f': 0})\n",
    "        print(f\"Converted '{col}' to binary (0/1)\")\n",
    "\n",
    "# 4.4 Convert date columns\n",
    "date_cols = ['host_since', 'first_review', 'last_review']\n",
    "for col in date_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce')\n",
    "        print(f\"Converted '{col}' to datetime\")\n",
    "\n",
    "# 4.5 Extract number from bathrooms_text\n",
    "if 'bathrooms_text' in df_cleaned.columns:\n",
    "    df_cleaned['bathrooms_numeric'] = df_cleaned['bathrooms_text'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    print(\"\\n Extracted numeric bathrooms from 'bathrooms_text'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1 Host experience (years as host)\n",
    "if 'host_since' in df_cleaned.columns:\n",
    "    df_cleaned['host_years'] = (pd.Timestamp.now() - df_cleaned['host_since']).dt.days / 365.25\n",
    "    print(\"\\n Created 'host_years' feature\")\n",
    "\n",
    "# 5.2 Days since first review\n",
    "if 'first_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_first_review'] = (pd.Timestamp.now() - df_cleaned['first_review']).dt.days\n",
    "    print(\" Created 'days_since_first_review' feature\")\n",
    "\n",
    "# 5.3 Days since last review\n",
    "if 'last_review' in df_cleaned.columns:\n",
    "    df_cleaned['days_since_last_review'] = (pd.Timestamp.now() - df_cleaned['last_review']).dt.days\n",
    "    print(\" Created 'days_since_last_review' feature\")\n",
    "\n",
    "# 5.4 Price per person \n",
    "if 'price' in df_cleaned.columns and 'accommodates' in df_cleaned.columns:\n",
    "    df_cleaned['price_per_person'] = df_cleaned['price'] / df_cleaned['accommodates']\n",
    "    print(\" Created 'price_per_person' feature \")\n",
    "\n",
    "# 5.5 Reviews per month (will be removed in T1.5)\n",
    "if 'reviews_per_month' not in df_cleaned.columns:\n",
    "    if 'number_of_reviews' in df_cleaned.columns and 'days_since_first_review' in df_cleaned.columns:\n",
    "        df_cleaned['reviews_per_month'] = (df_cleaned['number_of_reviews'] / \n",
    "                    (df_cleaned['days_since_first_review'] / 30.44))\n",
    "        print(\" Created 'reviews_per_month' feature\")\n",
    "\n",
    "# 5.6 Availability rate \n",
    "if 'availability_365' in df_cleaned.columns:\n",
    "    df_cleaned['availability_rate'] = df_cleaned['availability_365'] / 365\n",
    "    print(\" Created 'availability_rate' feature \")\n",
    "\n",
    "# 5.7 Average review score (will be removed in T1.5)\n",
    "review_score_cols = [col for col in df_cleaned.columns if 'review_scores_' in col and col != 'review_scores_rating']\n",
    "if review_score_cols:\n",
    "    df_cleaned['avg_review_score'] = df_cleaned[review_score_cols].mean(axis=1)\n",
    "    print(\" Created 'avg_review_score' feature \")\n",
    "\n",
    "# 5.8 Has reviews flag (will be removed in T1.5)\n",
    "if 'number_of_reviews' in df_cleaned.columns:\n",
    "    df_cleaned['has_reviews'] = (df_cleaned['number_of_reviews'] > 0).astype(int)\n",
    "    print(\" Created 'has_reviews' feature \")\n",
    "\n",
    "print(f\"\\n Total features after engineering: {df_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 6. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 6.1 Drop columns with >50% missing values\n",
    "missing_threshold = 0.5\n",
    "missing_pct = df_cleaned.isnull().sum() / len(df_cleaned)\n",
    "cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "    print(f\"\\n Dropped {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values\")\n",
    "    print(f\"   Columns dropped: {cols_to_drop}\")\n",
    "\n",
    "# 6.2 Fill missing values for specific columns\n",
    "# Numeric columns - fill with median\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\n Filled missing numeric values with median\")\n",
    "\n",
    "# Categorical columns - fill with mode or 'Unknown'\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        mode_val = df_cleaned[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_cleaned[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            df_cleaned[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(f\"Filled missing categorical values with mode or 'Unknown'\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing = df_cleaned.isnull().sum().sum()\n",
    "print(f\"\\n Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 7. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. OUTLIER DETECTION AND HANDLING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Focus on price outliers\n",
    "if 'price' in df_cleaned.columns:\n",
    "    # Removing listings with price = 0 or extremely high prices\n",
    "    initial_rows = len(df_cleaned)\n",
    "    \n",
    "    # Removing price = 0\n",
    "    df_cleaned = df_cleaned[df_cleaned['price'] > 0]\n",
    "    \n",
    "    # Remove extreme outliers (using IQR method)\n",
    "    Q1 = df_cleaned['price'].quantile(0.25)\n",
    "    Q3 = df_cleaned['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    df_cleaned = df_cleaned[(df_cleaned['price'] >= lower_bound) & \n",
    "                    (df_cleaned['price'] <= upper_bound)]\n",
    "    \n",
    "    rows_removed = initial_rows - len(df_cleaned)\n",
    "    print(f\"\\n Removed {rows_removed} rows with price outliers\")\n",
    "    print(f\" - Price range: ${df_cleaned['price'].min():.2f} - ${df_cleaned['price'].max():.2f}\")\n",
    "    print(f\" - Remaining rows: {len(df_cleaned):,}\")\n",
    "\n",
    "# Visualize price distribution after cleaning\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_cleaned['price'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price Distribution (After Outlier Removal)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df_cleaned['price'])\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Price Boxplot (After Outlier Removal)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/price_distribution_cleaned.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved: outputs/figures/price_distribution_cleaned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 8. Create Target Variable (Value Category)\n",
    "\n",
    "**CRITICAL:** We use review_scores_rating here to create labels. This is correct because:\n",
    "1. We need historical data (with reviews) to learn what makes good/bad value\n",
    "2. The target represents \"what value category would this listing be if it had reviews\"\n",
    "3. Review features will be removed from features (x) in Task 1.5, but will be kept for creating target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. Creating target variable: Value Category\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate FP Score (Fair Price Score) = Rating / Price\n",
    "# This measures \"value for money\"\n",
    "\n",
    "if 'review_scores_rating' in df_cleaned.columns and 'price' in df_cleaned.columns:\n",
    "    # Filter listings with reviews (needed for labeling)\n",
    "    df_with_reviews = df_cleaned[df_cleaned['review_scores_rating'].notna()].copy()\n",
    "    \n",
    "    # Normalize rating (0-5 scale) and price\n",
    "    df_with_reviews['rating_normalized'] = df_with_reviews['review_scores_rating'] / 20  # Convert 0-100 to 0-5\n",
    "    df_with_reviews['price_normalized'] = (df_with_reviews['price'] - df_with_reviews['price'].min()) / \\\n",
    "                    (df_with_reviews['price'].max() - df_with_reviews['price'].min())\n",
    "    \n",
    "    # Calculate FP Score (higher = better value)\n",
    "    df_with_reviews['fp_score'] = df_with_reviews['rating_normalized'] / (df_with_reviews['price_normalized'] + 0.1)\n",
    "    \n",
    "    # Classify into 3 categories based on FP Score\n",
    "    fp_33 = df_with_reviews['fp_score'].quantile(0.33)\n",
    "    fp_67 = df_with_reviews['fp_score'].quantile(0.67)\n",
    "    \n",
    "    def classify_value(fp_score):\n",
    "        if fp_score <= fp_33:\n",
    "            return 'Poor_Value'\n",
    "        elif fp_score <= fp_67:\n",
    "            return 'Fair_Value'\n",
    "        else:\n",
    "            return 'Excellent_Value'\n",
    "    \n",
    "    df_with_reviews['value_category'] = df_with_reviews['fp_score'].apply(classify_value)\n",
    "    \n",
    "    print(f\"\\n Created FP Score and Value Category\")\n",
    "    print(f\"   - Listings with reviews: {len(df_with_reviews):,}\")\n",
    "    print(f\"   - FP Score range: {df_with_reviews['fp_score'].min():.2f} - {df_with_reviews['fp_score'].max():.2f}\")\n",
    "    print(f\"\\n Value Category Distribution:\")\n",
    "    print(df_with_reviews['value_category'].value_counts())\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df_with_reviews['value_category'].value_counts().plot(kind='bar', color=['red', 'orange', 'green'])\n",
    "    plt.xlabel('Value Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Value Categories')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(df_with_reviews['fp_score'], bins=50, edgecolor='black')\n",
    "    plt.xlabel('FP Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('FP Score Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/figures/value_category_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Visualization saved: outputs/figures/value_category_distribution.png\")\n",
    "    \n",
    "    # Use df_with_reviews for further processing\n",
    "    df_final = df_with_reviews.copy()\n",
    "else:\n",
    "    print(\"\\n Warning: 'review_scores_rating' or 'price' not found. Skipping target creation.\")\n",
    "    df_final = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Data \n",
    "\n",
    "**Note:** This dataset contains all features including review-based ones.\n",
    "Task 1.3 and 1.4 will use this file.\n",
    "Task 1.5 will filter out review-based features from model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. SAVE CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save cleaned full dataset with target\n",
    "df_final.to_csv('../../data/processed/listings_cleaned_with_target.csv', index=False)\n",
    "print(\"\\n Saved to: data/processed/listings_cleaned_with_target.csv\")\n",
    "print(f\"   - Shape: {df_final.shape}\")\n",
    "print(f\"   - Contains all features (including review-based ones)\")\n",
    "print(f\"   - Review features will be filtered in Task 1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "\n",
    "Original data:  \n",
    "  - San Francisco: 7,780 listings\n",
    "  - San Diego: 13,162 listings\n",
    "  - Combined: {df.shape[0]:,} listings, {df.shape[1]} columns\n",
    "\n",
    "After cleaning and preprocessing steps:\n",
    "  - Final dataset: {df_final.shape[0]:,} listings, {df_final.shape[1]} columns\n",
    "\n",
    "Target variable properties:\n",
    "  - Name: value_category\n",
    "  - Classes: Poor_Value, Fair_Value, Excellent_Value\n",
    "  - Based on FP Score = Rating / Price (measures value for money)\n",
    "  - Distribution:\n",
    "{df_final['value_category'].value_counts().to_string()}\n",
    "\n",
    "After cleaning and preprocessing steps:\n",
    "  - Final dataset: {df_final.shape[0]:,} listings, {df_final.shape[1]} columns\n",
    "\n",
    "Key steps performed:\n",
    "  - Removed irrelevant columns (URLs, IDs, text descriptions)\n",
    "  - Converted data types (price, percentages, booleans, dates)\n",
    "  - Feature engineering (host_years, price_per_person, etc.)\n",
    "  - Handled missing values (dropped >50% missing, imputed rest)\n",
    "  - Removed outliers (price outliers using IQR method)\n",
    "  - Created target variable (FP Score classification)\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - data/processed/listings_cleaned_with_target.csv\n",
    "\n",
    "VISUALIZATIONS:\n",
    "  - outputs/figures/missing_values_analysis.png\n",
    "  - outputs/figures/price_distribution_cleaned.png\n",
    "  - outputs/figures/value_category_distribution.png\n",
    "\n",
    "\n",
    "{'='*80}\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Task 1.3: Algebraic Feature Engineering\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Task 1.3: Algebraic Feature Engineering and Visualization\")\n",
    "print(\"San Francisco & San Diego Airbnb Dataset\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 1. Load Data from T1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data with target from T1.2\n",
    "df = pd.read_csv('../../data/processed/listings_cleaned_with_target.csv')\n",
    "print(f\"\\n Loaded Dataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "\n",
    "# Check existing columns\n",
    "print(\"\\n Existing Columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 2. Verify Required Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Checking Required Columns for Algebraic Features:\")\n",
    "required_cols = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms_numeric', \n",
    "                 'availability_365', 'host_total_listings_count', 'minimum_nights']\n",
    "\n",
    "all_present = True\n",
    "for col in required_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\" {col}\")\n",
    "    else:\n",
    "        print(f\"{col} - Missing!\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n All required columns present! Ready to create algebraic features.\")\n",
    "else:\n",
    "    print(\"\\n Some required columns are missing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 3. Create 10 Landlord-Controlled Algebraic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING 10 NEW LANDLORD-CONTROLLED ALGEBRAIC FEATURES\")\n",
    "print(\"NO REVIEW-BASED DATA - PREVENTS DATA LEAKAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature 1: space_efficiency (beds per bedroom) \n",
    "print(\"\\n Creating: space_efficiency = beds / bedrooms \")\n",
    "df['space_efficiency'] = df['beds'] / df['bedrooms'].replace(0, np.nan)\n",
    "df['space_efficiency'] = df['space_efficiency'].fillna(df['space_efficiency'].median())\n",
    "print(f\"   Range: {df['space_efficiency'].min():.2f} to {df['space_efficiency'].max():.2f}\")\n",
    "print(f\"   Mean: {df['space_efficiency'].mean():.2f}\")\n",
    "print(f\"   Median: {df['space_efficiency'].median():.2f}\")\n",
    "\n",
    "# Feature 2: price_per_bedroom \n",
    "print(\"\\n Creating: price_per_bedroom = price / bedrooms \")\n",
    "df['price_per_bedroom'] = df['price'] / df['bedrooms'].replace(0, np.nan)\n",
    "df['price_per_bedroom'] = df['price_per_bedroom'].fillna(df['price_per_bedroom'].median())\n",
    "print(f\"   Range: ${df['price_per_bedroom'].min():.2f} to ${df['price_per_bedroom'].max():.2f}\")\n",
    "print(f\"   Mean: ${df['price_per_bedroom'].mean():.2f}\")\n",
    "print(f\"   Median: ${df['price_per_bedroom'].median():.2f}\")\n",
    "\n",
    "# Feature 3: price_per_bathroom \n",
    "print(\"\\n Creating: price_per_bathroom = price / bathrooms_numeric \")\n",
    "df['price_per_bathroom'] = df['price'] / df['bathrooms_numeric'].replace(0, np.nan)\n",
    "df['price_per_bathroom'] = df['price_per_bathroom'].fillna(df['price_per_bathroom'].median())\n",
    "print(f\"   Range: ${df['price_per_bathroom'].min():.2f} to ${df['price_per_bathroom'].max():.2f}\")\n",
    "print(f\"   Mean: ${df['price_per_bathroom'].mean():.2f}\")\n",
    "print(f\"   Median: ${df['price_per_bathroom'].median():.2f}\")\n",
    "\n",
    "# Feature 4: occupancy_rate \n",
    "print(\"\\n Creating: occupancy_rate = (365 - availability_365) / 365 \")\n",
    "df['occupancy_rate'] = (365 - df['availability_365']) / 365\n",
    "df['occupancy_rate'] = df['occupancy_rate'].clip(0, 1)\n",
    "print(f\"   Range: {df['occupancy_rate'].min():.2f} to {df['occupancy_rate'].max():.2f}\")\n",
    "print(f\"   Mean: {df['occupancy_rate'].mean():.2f}\")\n",
    "print(f\"   Median: {df['occupancy_rate'].median():.2f}\")\n",
    "\n",
    "# Feature 5: booking_flexibility_score \n",
    "print(\"\\n Creating: booking_flexibility_score = 1 / (minimum_nights + 1) \")\n",
    "df['booking_flexibility_score'] = 1 / (df['minimum_nights'] + 1)\n",
    "print(f\"   Range: {df['booking_flexibility_score'].min():.6f} to {df['booking_flexibility_score'].max():.6f}\")\n",
    "print(f\"   Mean: {df['booking_flexibility_score'].mean():.6f}\")\n",
    "print(f\"   Median: {df['booking_flexibility_score'].median():.6f}\")\n",
    "\n",
    "# Feature 6: space_per_person \n",
    "print(\"\\n Creating: space_per_person = bedrooms / accommodates \")\n",
    "df['space_per_person'] = df['bedrooms'] / df['accommodates'].replace(0, np.nan)\n",
    "df['space_per_person'] = df['space_per_person'].fillna(df['space_per_person'].median())\n",
    "print(f\"   Range: {df['space_per_person'].min():.2f} to {df['space_per_person'].max():.2f}\")\n",
    "print(f\"   Mean: {df['space_per_person'].mean():.2f}\")\n",
    "print(f\"   Median: {df['space_per_person'].median():.2f}\")\n",
    "\n",
    "# Feature 7: host_portfolio_intensity \n",
    "print(\"\\n Creating: host_portfolio_intensity = host_total_listings_count / accommodates \")\n",
    "df['host_portfolio_intensity'] = df['host_total_listings_count'] / df['accommodates'].replace(0, np.nan)\n",
    "df['host_portfolio_intensity'] = df['host_portfolio_intensity'].fillna(df['host_portfolio_intensity'].median())\n",
    "print(f\"   Range: {df['host_portfolio_intensity'].min():.2f} to {df['host_portfolio_intensity'].max():.2f}\")\n",
    "print(f\"   Mean: {df['host_portfolio_intensity'].mean():.2f}\")\n",
    "print(f\"   Median: {df['host_portfolio_intensity'].median():.2f}\")\n",
    "\n",
    "# Feature 8: bathroom_to_bedroom_ratio \n",
    "print(\"\\n Creating: bathroom_to_bedroom_ratio = bathrooms_numeric / bedrooms \")\n",
    "df['bathroom_to_bedroom_ratio'] = df['bathrooms_numeric'] / df['bedrooms'].replace(0, np.nan)\n",
    "df['bathroom_to_bedroom_ratio'] = df['bathroom_to_bedroom_ratio'].fillna(df['bathroom_to_bedroom_ratio'].median())\n",
    "print(f\"   Range: {df['bathroom_to_bedroom_ratio'].min():.2f} to {df['bathroom_to_bedroom_ratio'].max():.2f}\")\n",
    "print(f\"   Mean: {df['bathroom_to_bedroom_ratio'].mean():.2f}\")\n",
    "print(f\"   Median: {df['bathroom_to_bedroom_ratio'].median():.2f}\")\n",
    "print(f\"   Interpretation: Higher ratio = more luxury (more bathrooms per bedroom)\")\n",
    "\n",
    "# Feature 9: price_to_capacity_ratio  \n",
    "print(\"\\n Creating: price_to_capacity_ratio = price / (accommodates × bedrooms) \")\n",
    "df['price_to_capacity_ratio'] = df['price'] / (df['accommodates'] * df['bedrooms'].replace(0, np.nan))\n",
    "df['price_to_capacity_ratio'] = df['price_to_capacity_ratio'].fillna(df['price_to_capacity_ratio'].median())\n",
    "print(f\"   Range: ${df['price_to_capacity_ratio'].min():.2f} to ${df['price_to_capacity_ratio'].max():.2f}\")\n",
    "print(f\"   Mean: ${df['price_to_capacity_ratio'].mean():.2f}\")\n",
    "print(f\"   Median: ${df['price_to_capacity_ratio'].median():.2f}\")\n",
    "print(f\"   Interpretation: Price efficiency per unit of space\")\n",
    "\n",
    "# Feature 10: availability_flexibility_score  \n",
    "print(\"\\n Creating: availability_flexibility_score = availability_365 / minimum_nights \")\n",
    "df['availability_flexibility_score'] = df['availability_365'] / df['minimum_nights'].replace(0, np.nan)\n",
    "df['availability_flexibility_score'] = df['availability_flexibility_score'].fillna(df['availability_flexibility_score'].median())\n",
    "df['availability_flexibility_score'] = df['availability_flexibility_score'].clip(0, 365)  # Cap at 365\n",
    "print(f\"   Range: {df['availability_flexibility_score'].min():.2f} to {df['availability_flexibility_score'].max():.2f}\")\n",
    "print(f\"   Mean: {df['availability_flexibility_score'].mean():.2f}\")\n",
    "print(f\"   Median: {df['availability_flexibility_score'].median():.2f}\")\n",
    "print(f\"   Interpretation: High availability + low minimum nights = more flexible booking\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" All 10 new features created successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 4. Summary and Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "new_features = [\n",
    "    'space_efficiency', 'price_per_bedroom', 'price_per_bathroom',\n",
    "    'occupancy_rate', 'booking_flexibility_score', 'space_per_person',\n",
    "    'host_portfolio_intensity', 'bathroom_to_bedroom_ratio',\n",
    "    'price_to_capacity_ratio', 'availability_flexibility_score'\n",
    "]\n",
    "\n",
    "print(f\"\\n New Dataset Shape: {df.shape}\")\n",
    "print(f\" Added: {len(new_features)} new algebraic features\")\n",
    "\n",
    "# Check data quality\n",
    "print(\"\\n Data Quality Check:\")\n",
    "quality_ok = True\n",
    "for feature in new_features:\n",
    "    nan_count = df[feature].isna().sum()\n",
    "    inf_count = np.isinf(df[feature]).sum()\n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(f\"{feature}: {nan_count} NaN, {inf_count} Inf values\")\n",
    "        quality_ok = False\n",
    "        \n",
    "if quality_ok:\n",
    "    print(\" All features are clean (no NaN or Inf values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 5. Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "output_path = '../../data/processed/listings_with_algebraic_features.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved dataset to: {output_path}\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 6. Detailed Statistics for All 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Detailed Statictical Summary of New Features\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stats_data = []\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    stats = {\n",
    "        'No.': i,\n",
    "        'Feature Name': feature,\n",
    "        'Mean': df[feature].mean(),\n",
    "        'Median': df[feature].median(),\n",
    "        'Std Dev': df[feature].std(),\n",
    "        'Min': df[feature].min(),\n",
    "        'Max': df[feature].max(),\n",
    "        'Q1': df[feature].quantile(0.25),\n",
    "        'Q3': df[feature].quantile(0.75),\n",
    "        'Skewness': df[feature].skew(),\n",
    "        'Missing': df[feature].isna().sum()\n",
    "    }\n",
    "    stats_data.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# Display formatted table\n",
    "print(\"\\n\")\n",
    "for idx, row in stats_df.iterrows():\n",
    "    print(f\"{row['No.']}. {row['Feature Name'].upper()}\")\n",
    "    print(f\"   Mean: {row['Mean']:.4f} | Median: {row['Median']:.4f} | Std: {row['Std Dev']:.4f}\")\n",
    "    print(f\"   Range: [{row['Min']:.4f}, {row['Max']:.4f}] | IQR: [{row['Q1']:.4f}, {row['Q3']:.4f}]\")\n",
    "    print(f\"   Skewness: {row['Skewness']:.4f} | Missing: {row['Missing']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for the new features\n",
    "import os\n",
    "os.makedirs('../../outputs/figures', exist_ok=True)\n",
    "\n",
    "# 1. Distribution plots for all 10 features\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(new_features):\n",
    "    axes[idx].hist(df[feature], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feature}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/algebraic_features_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n Saved to : outputs/figures/algebraic_features_distributions.png\")\n",
    "\n",
    "# 2. Correlation heatmap of new features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[new_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of 10 Landlord-Controlled Algebraic Features', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/algebraic_features_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Saved to : outputs/figures/algebraic_features_correlation.png\")\n",
    "\n",
    "# 3. Box plots for outlier detection\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(new_features):\n",
    "    axes[idx].boxplot(df[feature].dropna())\n",
    "    axes[idx].set_title(f'{feature}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/algebraic_features_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved to : outputs/figures/algebraic_features_boxplots.png\")\n",
    "\n",
    "# 4. Summary statistics visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "x_pos = np.arange(len(new_features))\n",
    "means = [df[f].mean() for f in new_features]\n",
    "stds = [df[f].std() for f in new_features]\n",
    "\n",
    "# Normalize for visualization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "means_normalized = scaler.fit_transform(np.array(means).reshape(-1, 1)).flatten()\n",
    "stds_normalized = scaler.fit_transform(np.array(stds).reshape(-1, 1)).flatten()\n",
    "\n",
    "ax.bar(x_pos - 0.2, means_normalized, 0.4, label='Mean (normalized)', alpha=0.8)\n",
    "ax.bar(x_pos + 0.2, stds_normalized, 0.4, label='Std Dev (normalized)', alpha=0.8)\n",
    "ax.set_xlabel('Features', fontweight='bold')\n",
    "ax.set_ylabel('Normalized Value', fontweight='bold')\n",
    "ax.set_title('Mean and Standard Deviation of Algebraic Features (Normalized)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(new_features, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/algebraic_features_variability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Saved to : outputs/figures/algebraic_features_variability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## 8. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "\n",
    "INPUT DATA:\n",
    "  - Source: data/processed/listings_cleaned_with_target.csv (from T1.2)\n",
    "  - Shape: {df.shape[0]:,} rows × {df.shape[1] - len(new_features)} columns (before)\n",
    "\n",
    "OUTPUT DATA:\n",
    "  - Destination: data/processed/listings_with_algebraic_features.csv\n",
    "  - Shape: {df.shape[0]:,} rows × {df.shape[1]} columns (after)\n",
    "  - Added: {len(new_features)} new algebraic features\n",
    "\n",
    "  New algebraic features created:\n",
    "  1. space_efficiency = beds / bedrooms\n",
    "  2. price_per_bedroom = price / bedrooms\n",
    "  3. price_per_bathroom = price / bathrooms_numeric\n",
    "  4. occupancy_rate = (365 - availability_365) / 365\n",
    "  5. booking_flexibility_score = 1 / (minimum_nights + 1)\n",
    "  6. space_per_person = bedrooms / accommodates\n",
    "  7. host_portfolio_intensity = host_total_listings_count / accommodates\n",
    "  8. bathroom_to_bedroom_ratio = bathrooms_numeric / bedrooms (luxury indicator)\n",
    "  9. price_to_capacity_ratio = price / (accommodates × bedrooms) (price efficiency)\n",
    "  10. availability_flexibility_score = availability_365 / minimum_nights (booking flexibility)\n",
    "\n",
    " Data leakage prevention:\n",
    "  - No review-based features used in these calculations\n",
    "  - All features derived from landlord-controllable data only\n",
    "  - Features are available for NEW listings without review history\n",
    "  - Model can predict value for listings with zero reviews\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - data/processed/listings_with_algebraic_features.csv\n",
    "  - data/processed/algebraic_features_statistics.csv\n",
    "\n",
    "VISUALIZATIONS:\n",
    "  - outputs/figures/algebraic_features_distributions.png\n",
    "  - outputs/figures/algebraic_features_correlation.png\n",
    "  - outputs/figures/algebraic_features_boxplots.png\n",
    "  - outputs/figures/algebraic_features_variability.png\n",
    "\n",
    "{'='*80}\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Task 1.4: Categorical encoding\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Task 1.4: Categorical encoding\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the dataset with algebraic features from T1.3\n",
    "df = pd.read_csv('../../data/processed/listings_with_algebraic_features.csv')\n",
    "print(f\"\\n Loaded dataset from T1.3: {df.shape}\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## 2. Merge Categorical Columns from Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Loading raw data to get categorical columns...\")\n",
    "\n",
    "# Load raw datasets\n",
    "sf_raw = pd.read_csv('../../data/raw/san francisco.csv')\n",
    "sd_raw = pd.read_csv('../../data/raw/san diego.csv')\n",
    "\n",
    "print(f\" San Francisco: {sf_raw.shape}\")\n",
    "print(f\" San Diego: {sd_raw.shape}\")\n",
    "\n",
    "# Combine raw datasets\n",
    "raw_combined = pd.concat([sf_raw, sd_raw], ignore_index=True)\n",
    "print(f\" Combined raw: {raw_combined.shape}\")\n",
    "\n",
    "# Select only needed categorical columns\n",
    "categorical_cols = raw_combined[['id', 'property_type', 'room_type', 'neighbourhood_cleansed']]\n",
    "\n",
    "# Drop existing categorical columns from df before merging to avoid duplicates\n",
    "cols_to_drop = ['property_type', 'room_type', 'neighbourhood_cleansed']\n",
    "existing_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "if existing_cols:\n",
    "    df = df.drop(columns=existing_cols)\n",
    "    print(f\"\\n Dropped existing columns: {existing_cols}\")\n",
    "\n",
    "# Merge with main dataset\n",
    "df = df.merge(categorical_cols, on='id', how='left')\n",
    "print(f\"\\n After merging categorical columns: {df.shape}\")\n",
    "\n",
    "# Check for missing values in categorical columns\n",
    "print(\"\\n Checking categorical columns:\")\n",
    "for col in ['property_type', 'room_type', 'neighbourhood_cleansed']:\n",
    "    missing = df[col].isna().sum()\n",
    "    unique = df[col].nunique()\n",
    "    print(f\"   {col}: {unique} unique values, {missing} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## 3. Perform Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Categorical encoding - 4 variables, 10 new features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize label encoders\n",
    "le_property = LabelEncoder()\n",
    "le_neighbourhood = LabelEncoder()\n",
    "\n",
    "# 1. ROOM TYPE - One-Hot Encoding \n",
    "print(\"\\n ROOM TYPE - One-Hot Encoding \")\n",
    "print(f\" Original categories: {df['room_type'].nunique()}\")\n",
    "print(f\" Categories: {df['room_type'].unique().tolist()}\")\n",
    "\n",
    "room_dummies = pd.get_dummies(df['room_type'], prefix='room_type')\n",
    "df = pd.concat([df, room_dummies], axis=1)\n",
    "\n",
    "print(f\" Created {len(room_dummies.columns)} binary columns:\")\n",
    "for col in room_dummies.columns:\n",
    "    count = df[col].sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"      {col}: {int(count):,} ({pct:.2f}%)\")\n",
    "\n",
    "# 2. PROPERTY TYPE - Label + Frequency Encoding \n",
    "print(\"\\n Property Type - Label + Frequency Encoding \")\n",
    "print(f\"   Original categories: {df['property_type'].nunique()}\")\n",
    "\n",
    "# Label encoding\n",
    "df['property_type_label'] = le_property.fit_transform(df['property_type'])\n",
    "\n",
    "# Frequency encoding\n",
    "df['property_type_frequency'] = df['property_type'].map(\n",
    "    df['property_type'].value_counts(normalize=True)\n",
    ")\n",
    "\n",
    "print(f\" Created 2 columns:\")\n",
    "print(f\" property_type_label: Range 0-{int(df['property_type_label'].max())}\")\n",
    "print(f\" property_type_frequency: Range {df['property_type_frequency'].min():.4f}-{df['property_type_frequency'].max():.4f}\")\n",
    "print(f\" Mean frequency: {df['property_type_frequency'].mean():.4f}\")\n",
    "\n",
    "# 3. NEIGHBOURHOOD - Target + Frequency + Label Encoding \n",
    "print(\"\\n NEIGHBOURHOOD - Target + Frequency + Label Encoding \")\n",
    "print(f\"  Original categories: {df['neighbourhood_cleansed'].nunique()}\")\n",
    "\n",
    "# First encode value_category for target encoding\n",
    "value_mapping = {'Poor_Value': 0, 'Fair_Value': 1, 'Excellent_Value': 2}\n",
    "df['value_encoded'] = df['value_category'].map(value_mapping)\n",
    "\n",
    "# Target encoding (mean value_encoded per neighbourhood)\n",
    "neighbourhood_target = df.groupby('neighbourhood_cleansed')['value_encoded'].mean()\n",
    "df['neighbourhood_target_encoded'] = df['neighbourhood_cleansed'].map(neighbourhood_target)\n",
    "\n",
    "# Frequency encoding\n",
    "df['neighbourhood_frequency'] = df['neighbourhood_cleansed'].map(\n",
    "    df['neighbourhood_cleansed'].value_counts(normalize=True)\n",
    ")\n",
    "\n",
    "# Label encoding\n",
    "df['neighbourhood_label'] = le_neighbourhood.fit_transform(df['neighbourhood_cleansed'])\n",
    "\n",
    "print(f\" Created 3 columns:\")\n",
    "print(f\" neighbourhood_label: Range 0-{int(df['neighbourhood_label'].max())}\")\n",
    "print(f\" neighbourhood_target_encoded: Range {df['neighbourhood_target_encoded'].min():.4f}-{df['neighbourhood_target_encoded'].max():.4f}\")\n",
    "print(f\" neighbourhood_frequency: Range {df['neighbourhood_frequency'].min():.4f}-{df['neighbourhood_frequency'].max():.4f}\")\n",
    "print(f\" Mean target encoding: {df['neighbourhood_target_encoded'].mean():.4f}\")\n",
    "\n",
    "# 4. VALUE CATEGORY - Already encoded as value_encoded [TARGET VARIABLE]\n",
    "print(\"\\n VALUE CATEGORY - Label Encoding [Target Variable] \")\n",
    "print(f\"  Original categories: {df['value_category'].nunique()}\")\n",
    "print(f\"  Mapping: Poor_Value=0, Fair_Value=1, Excellent_Value=2\")\n",
    "print(f\"  Created 1 column: value_encoded\")\n",
    "\n",
    "value_dist = df['value_encoded'].value_counts().sort_index()\n",
    "for val, count in value_dist.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    label = ['Poor_Value', 'Fair_Value', 'Excellent_Value'][int(val)]\n",
    "    print(f\"      {val} ({label}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" All categorical encoding completed!\")\n",
    "print(f\" Total new encoded features: 10\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## 4. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Data Quality Check:\")\n",
    "\n",
    "# Check for duplicate columns\n",
    "duplicate_cols = df.columns[df.columns.duplicated()].tolist()\n",
    "if duplicate_cols:\n",
    "    print(f\" WARNING: Duplicate columns found: {duplicate_cols}\")\n",
    "    print(f\" Removing duplicate columns...\")\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    print(f\" Duplicates removed. New shape: {df.shape}\")\n",
    "else:\n",
    "    print(f\" No duplicate columns found\")\n",
    "\n",
    "# Check all new encoding columns for missing values\n",
    "all_clean = True\n",
    "new_encoding_cols = [\n",
    "    'room_type_Entire home/apt', 'room_type_Hotel room', \n",
    "    'room_type_Private room', 'room_type_Shared room',\n",
    "    'property_type_label', 'property_type_frequency',\n",
    "    'neighbourhood_label', 'neighbourhood_target_encoded', \n",
    "    'neighbourhood_frequency', 'value_encoded'\n",
    "]\n",
    "\n",
    "print(\"\\n   Checking encoded columns for missing values:\")\n",
    "for col in new_encoding_cols:\n",
    "    if col in df.columns:\n",
    "        missing = int(df[col].isna().sum())\n",
    "        if missing > 0:\n",
    "            print(f\" {col}: {missing} missing values\")\n",
    "            all_clean = False\n",
    "        else:\n",
    "            print(f\" {col}: No missing values\")\n",
    "\n",
    "if all_clean:\n",
    "    print(\"\\n All encoded columns are complete (no missing values)\")\n",
    "else:\n",
    "    print(\"\\n Some columns have missing values - review required\")\n",
    "\n",
    "print(f\"\\n Final Dataset Shape: {df.shape}\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## 5. Save Encoded Dataset and Mapping Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import os\n",
    "\n",
    "\n",
    "\n",
    "# Save the encoded dataset\n",
    "output_path = '../../data/processed/listings_with_categorical_encoding.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\n Saved encoded dataset to: {output_path}\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Create encoding reference files\n",
    "print(f\"\\n Creating encoding reference files...\")\n",
    "\n",
    "# 1. Property Type Mapping\n",
    "property_mapping = pd.DataFrame({\n",
    "    'property_type': le_property.classes_,\n",
    "    'label': range(len(le_property.classes_))\n",
    "})\n",
    "property_mapping = property_mapping.merge(\n",
    "    df.groupby('property_type')['property_type_frequency'].first().reset_index(),\n",
    "    on='property_type'\n",
    ")\n",
    "property_mapping = property_mapping.merge(\n",
    "    df['property_type'].value_counts().reset_index().rename(columns={'count': 'count'}),\n",
    "    on='property_type'\n",
    ")\n",
    "property_mapping = property_mapping.sort_values('count', ascending=False)\n",
    "property_mapping.to_csv('../../outputs/property_type_encoding_map.csv', index=False)\n",
    "print(f\" Saved to: outputs/property_type_encoding_map.csv ({len(property_mapping)} property types)\")\n",
    "\n",
    "# 2. Neighbourhood Mapping\n",
    "neighbourhood_mapping = pd.DataFrame({\n",
    "    'neighbourhood': le_neighbourhood.classes_,\n",
    "    'label': range(len(le_neighbourhood.classes_))\n",
    "})\n",
    "neighbourhood_mapping = neighbourhood_mapping.merge(\n",
    "    df.groupby('neighbourhood_cleansed').agg({\n",
    "        'neighbourhood_target_encoded': 'first',\n",
    "        'neighbourhood_frequency': 'first'\n",
    "    }).reset_index(),\n",
    "    left_on='neighbourhood',\n",
    "    right_on='neighbourhood_cleansed'\n",
    ").drop('neighbourhood_cleansed', axis=1)\n",
    "neighbourhood_mapping = neighbourhood_mapping.merge(\n",
    "    df['neighbourhood_cleansed'].value_counts().reset_index().rename(columns={'count': 'count'}),\n",
    "    left_on='neighbourhood',\n",
    "    right_on='neighbourhood_cleansed'\n",
    ").drop('neighbourhood_cleansed', axis=1)\n",
    "neighbourhood_mapping = neighbourhood_mapping.sort_values('count', ascending=False)\n",
    "neighbourhood_mapping.to_csv('../../outputs/neighbourhood_encoding_map.csv', index=False)\n",
    "print(f\" Saved to : outputs/neighbourhood_encoding_map.csv ({len(neighbourhood_mapping)} neighbourhoods)\")\n",
    "\n",
    "# 3. Value Category Mapping\n",
    "value_mapping_df = pd.DataFrame({\n",
    "    'value_category': ['Poor_Value', 'Fair_Value', 'Excellent_Value'],\n",
    "    'encoded_value': [0, 1, 2],\n",
    "    'count': [df[df['value_encoded']==i].shape[0] for i in range(3)]\n",
    "})\n",
    "value_mapping_df.to_csv('../../outputs/value_category_encoding_map.csv', index=False)\n",
    "print(f\" Saved to: outputs/value_category_encoding_map.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## 6. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Creating visualizations...\")\n",
    "\n",
    "\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "# Figure 1: Categorical Encoding Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Categorical Encoding Analysis - Task 1.4', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Room Type Distribution\n",
    "room_type_cols = ['room_type_Entire home/apt', 'room_type_Hotel room', \n",
    "                  'room_type_Private room', 'room_type_Shared room']\n",
    "room_type_data = df[room_type_cols].sum()\n",
    "axes[0, 0].bar(range(len(room_type_data)), room_type_data.values, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xticks(range(len(room_type_data)))\n",
    "axes[0, 0].set_xticklabels(['Entire home/apt', 'Hotel room', 'Private room', 'Shared room'], \n",
    "                           rotation=45, ha='right')\n",
    "axes[0, 0].set_title('Room Type Distribution (One-Hot Encoded)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Property Type Frequency Distribution\n",
    "axes[0, 1].hist(df['property_type_frequency'], bins=30, color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Property Type Frequency Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Frequency')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Neighbourhood Target Encoding Distribution\n",
    "axes[1, 0].hist(df['neighbourhood_target_encoded'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Neighbourhood Target Encoding Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Target Encoded Value')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Value Category Distribution\n",
    "value_counts = df['value_encoded'].value_counts().sort_index()\n",
    "axes[1, 1].bar(['Poor Value', 'Fair Value', 'Excellent Value'], value_counts.values, \n",
    "               color=['#ff6b6b', '#ffd93d', '#6bcf7f'], edgecolor='black')\n",
    "axes[1, 1].set_title('Value Category Distribution (Label Encoded)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/categorical_encoding_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Saved to: outputs/figures/categorical_encoding_analysis.png\")\n",
    "\n",
    "# Figure 2: Encoding Methods Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Encoding Methods Comparison - Task 1.4', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Cardinality Comparison\n",
    "variables = ['room_type', 'property_type', 'neighbourhood', 'value_category']\n",
    "cardinalities = [4, df['property_type'].nunique(), df['neighbourhood_cleansed'].nunique(), 3]\n",
    "colors_card = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "axes[0, 0].barh(variables, cardinalities, color=colors_card, edgecolor='black')\n",
    "axes[0, 0].set_title('Original Cardinality by Variable', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Unique Categories')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Encoding Methods Used\n",
    "methods = ['One-Hot', 'Label +\\nFrequency', 'Target +\\nFrequency +\\nLabel', 'Label\\n(Ordinal)']\n",
    "columns_created = [4, 2, 3, 1]\n",
    "axes[0, 1].bar(range(len(methods)), columns_created, color=colors_card, edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(methods)))\n",
    "axes[0, 1].set_xticklabels(methods)\n",
    "axes[0, 1].set_title('Columns Created by Encoding Method', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Columns')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Property Type - Top 10\n",
    "top_properties = df['property_type'].value_counts().head(10)\n",
    "axes[1, 0].barh(range(len(top_properties)), top_properties.values, color='steelblue', edgecolor='black')\n",
    "axes[1, 0].set_yticks(range(len(top_properties)))\n",
    "axes[1, 0].set_yticklabels(top_properties.index, fontsize=9)\n",
    "axes[1, 0].set_title('Top 10 Property Types', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].invert_yaxis()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Neighbourhood - Top 10\n",
    "top_neighbourhoods = df['neighbourhood_cleansed'].value_counts().head(10)\n",
    "axes[1, 1].barh(range(len(top_neighbourhoods)), top_neighbourhoods.values, \n",
    "                color='mediumseagreen', edgecolor='black')\n",
    "axes[1, 1].set_yticks(range(len(top_neighbourhoods)))\n",
    "axes[1, 1].set_yticklabels(top_neighbourhoods.index, fontsize=9)\n",
    "axes[1, 1].set_title('Top 10 Neighbourhoods', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Count')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/encoding_methods_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Saved to: outputs/figures/encoding_methods_comparison.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## 7. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Task 1.4 Summary Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "\n",
    "Input Data:\n",
    "  - Source: data/processed/listings_with_algebraic_features.csv (from T1.3)\n",
    "  - Shape: {df.shape[0]:,} rows × {df.shape[1] - 10} columns (before encoding)\n",
    "\n",
    "Output Data:\n",
    "  - Destination: data/processed/listings_with_categorical_encoding.csv\n",
    "  - Shape: {df.shape[0]:,} rows × {df.shape[1]} columns (after encoding)\n",
    "  - Added: 10 new encoded features\n",
    "\n",
    "Encoding Breakdown:\n",
    "\n",
    "1. Room Type (One-Hot Encoding) [Landlord-Controlled]\n",
    "   - Original categories: 4\n",
    "   - Encoded columns: 4\n",
    "   - Method: Binary columns for each category\n",
    "\n",
    "2.  Property Type (Label + Frequency Encoding) [Landlord-Controlled]\n",
    "    Original categories: {df['property_type'].nunique()}\n",
    "   - Encoded columns: 2\n",
    "   - Methods: Label encoding + Frequency encoding\n",
    "\n",
    "3. Neighbourhood (Target + Frequency + Label Encoding) [Landlord-Controlled]\n",
    "   - Original categories: {df['neighbourhood_cleansed'].nunique()}\n",
    "   - Encoded columns: 3\n",
    "   - Methods: Target encoding + Frequency encoding + Label encoding\n",
    "\n",
    "4. Value Category (Label Encoding - Ordinal) [Target Variable]\n",
    "   - Original categories: 3\n",
    "   - Encoded columns: 1\n",
    "   - Mapping: Poor_Value=0, Fair_Value=1, Excellent_Value=2\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - data/processed/listings_with_categorical_encoding.csv\n",
    "  - outputs/property_type_encoding_map.csv\n",
    "  - outputs/neighbourhood_encoding_map.csv\n",
    "  - outputs/value_category_encoding_map.csv\n",
    "  \n",
    "\n",
    "VISUALIZATIONS:\n",
    "  - outputs/figures/categorical_encoding_analysis.png\n",
    "  - outputs/figures/encoding_methods_comparison.png\n",
    "\n",
    "\n",
    "{'='*80}\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Task 1.5 : Feature Selection\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from T1.4 (with categorical encoding)\n",
    "df = pd.read_csv('../../data/processed/listings_with_categorical_encoding.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Feature Categorization\n",
    "\n",
    "We categorize all features into:\n",
    "1. **Landlord-Controlled Features** - Available at listing creation\n",
    "2. **Review-Based Features**  - Only available after guests stay\n",
    "3. **Target-Related Features** - Used to create target, causes data leakage\n",
    "4. **Identifier Features** - Not useful for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories\n",
    "\n",
    "# 1. Landlord-controlled features (available at listing creation)\n",
    "landlord_features = [\n",
    "    # Price information\n",
    "    'price',\n",
    "    \n",
    "    # Property characteristics\n",
    "    'accommodates', 'bedrooms', 'beds', 'bathrooms',\n",
    "    \n",
    "    # Location\n",
    "    'latitude', 'longitude', 'city',\n",
    "    \n",
    "    # Host information (available at listing creation)\n",
    "    'host_is_superhost', 'host_identity_verified',\n",
    "    'host_response_time', 'host_response_rate',\n",
    "    \n",
    "    # Listing policies\n",
    "    'instant_bookable', 'cancellation_policy',\n",
    "    'minimum_nights', 'maximum_nights',\n",
    "    \n",
    "    # Availability\n",
    "    'availability_30', 'availability_60', 'availability_90', 'availability_365',\n",
    "    \n",
    "   \n",
    "    # Algebraic features (from T1.3) - landlord-controlled\n",
    "    'space_efficiency', 'price_per_bedroom', 'price_per_bathroom',\n",
    "     'bathroom_per_bedroom',\n",
    "    \n",
    "    # Categorical encodings (from T1.4)\n",
    "    'room_type_Entire home/apt', 'room_type_Private room', 'room_type_Shared room',\n",
    "    'property_type_label', 'property_type_frequency',\n",
    "     'neighbourhood_frequency', 'neighbourhood_label'\n",
    "]\n",
    "\n",
    "# 2. Review-based features (not available for new listings)\n",
    "review_features_to_remove = [\n",
    "    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "    'review_scores_value', 'number_of_reviews', 'number_of_reviews_ltm',\n",
    "    'number_of_reviews_l30d', 'reviews_per_month',\n",
    "    'first_review', 'last_review', 'days_since_first_review', 'days_since_last_review',\n",
    "    'review_recency_score', 'estimated_occupancy', 'estimated_revenue',\n",
    "    'quality_score'  # This is derived from review_scores_rating\n",
    "]\n",
    "\n",
    "# 3. Target-related features (causes data leakage)\n",
    "target_leakage_features = [\n",
    "    'fp_score',  # This is rating/price - directly related to target\n",
    "    'price_normalized', 'rating_normalized',  # Used to create fp_score\n",
    "    'value_category'  # This is our target variable\n",
    "]\n",
    "\n",
    "# 4. Identifier features (not useful for modeling)\n",
    "identifier_features = [\n",
    "    'id', 'listing_url', 'name', 'description', 'host_id', 'host_name'\n",
    "]\n",
    "\n",
    "print(\"Feature categories defined:\")\n",
    "print(f\"  - Landlord-controlled features: {len(landlord_features)}\")\n",
    "print(f\"  - Review-based features to remove: {len(review_features_to_remove)}\")\n",
    "print(f\"  - Target leakage features to remove: {len(target_leakage_features)}\")\n",
    "print(f\"  - Identifier features to remove: {len(identifier_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify which features actually exist in the dataset\n",
    "available_landlord_features = [f for f in landlord_features if f in df.columns]\n",
    "missing_landlord_features = [f for f in landlord_features if f not in df.columns]\n",
    "\n",
    "print(f\"Available landlord features: {len(available_landlord_features)}\")\n",
    "print(f\"Missing landlord features: {len(missing_landlord_features)}\")\n",
    "\n",
    "if missing_landlord_features:\n",
    "    print(f\"\\nMissing features: {missing_landlord_features}\")\n",
    "\n",
    "# Check if target variable exists\n",
    "if 'value_category' not in df.columns:\n",
    "    print(\"\\n WARNING: 'value_category' not found in dataset!\")\n",
    "    print(\"Creating target variable from fp_score.\")\n",
    "    \n",
    "    # Create target variable if it doesn't exist\n",
    "    if 'fp_score' in df.columns:\n",
    "        df['value_category'] = pd.qcut(df['fp_score'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "        print(\" Target variable created successfully\")\n",
    "    else:\n",
    "        print(\" Error: Cannot create target variable - fp_score not found!\")\n",
    "else:\n",
    "    print(\"\\n Target variable 'value_category' found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Create Clean Dataset with Landlord-Only Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df[available_landlord_features].copy()\n",
    "y = df['value_category'].copy()\n",
    "\n",
    "print(f\"Feature matrix (X): {X.shape}\")\n",
    "print(f\"Target variable (y): {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nTarget distribution (%):\\n{y.value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in landlord features\n",
    "missing_values = X.isnull().sum()\n",
    "missing_pct = (missing_values / len(X)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing values in landlord features:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Handle missing values if any\n",
    "if missing_df['Missing_Count'].sum() > 0:\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "    \n",
    "    # Fill categorical columns with mode\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 'Unknown')\n",
    "    \n",
    "    print(\" Missing values handled\")\n",
    "else:\n",
    "    print(\"\\n No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove non-numeric columns before scaling\n",
    "non_numeric = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"Removing non-numeric columns: {non_numeric}\")\n",
    "    X_train = X_train.drop(columns=non_numeric)\n",
    "    X_test = X_test.drop(columns=non_numeric)\n",
    "\n",
    "\n",
    "# Identify numeric columns for scaling\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features to scale: {len(numeric_features)}\")\n",
    "print(f\"Features: {numeric_features}\")\n",
    "\n",
    "# Initialize and fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale numeric features\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "print(\"\\n Feature scaling completed\")\n",
    "print(f\"\\nScaled training set: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set: {X_test_scaled.shape}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(scaler, '../../models/standard_scaler.pkl')\n",
    "print(\"\\n Saved: models/standard_scaler.pkl\")\n",
    "\n",
    "# Save scaled data for T2.1\n",
    "X_train_scaled.to_csv('../../data/processed/X_train_landlord.csv', index=False)\n",
    "X_test_scaled.to_csv('../../data/processed/X_test_landlord.csv', index=False)\n",
    "print(\" Saved scaled X_train and X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Saving processed datasets...\")\n",
    "\n",
    "# Save the clean dataset with landlord features only\n",
    "landlord_df = pd.concat([X, y], axis=1)\n",
    "landlord_df.to_csv('../../data/processed/listings_landlord_features_only.csv', index=False)\n",
    "print(\" Saved: listings_landlord_features_only.csv\")\n",
    "\n",
    "# Save train-test splits (unscaled)\n",
    "X_train.to_csv('../../data/processed/X_train_landlord.csv', index=False)\n",
    "X_test.to_csv('../../data/processed/X_test_landlord.csv', index=False)\n",
    "y_train.to_csv('../../data/processed/y_train_landlord.csv', index=False)\n",
    "y_test.to_csv('../../data/processed/y_test_landlord.csv', index=False)\n",
    "print(\" Saved: X_train_landlord.csv, X_test_landlord.csv, y_train_landlord.csv, y_test_landlord.csv\")\n",
    "\n",
    "# Save scaled versions\n",
    "X_train_scaled.to_csv('../../data/processed/X_train_landlord_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../../data/processed/X_test_landlord_scaled.csv', index=False)\n",
    "print(\" Saved: X_train_landlord_scaled.csv, X_test_landlord_scaled.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "## Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for landlord features\n",
    "print(\"Summary Statistics for Landlord Features:\")\n",
    "print(\"=\"*60)\n",
    "print(X[numeric_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "y.value_counts().plot(kind='bar', color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Distribution of Value Categories (Target Variable)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Value Category', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../outputs/figures/target_distribution_landlord.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Saved: target_distribution_landlord.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key landlord features\n",
    "key_features = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms']\n",
    "available_key_features = [f for f in key_features if f in X.columns]\n",
    "\n",
    "if available_key_features:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(available_key_features):\n",
    "        axes[idx].hist(X[feature].dropna(), bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(feature, fontsize=10)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(available_key_features), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/figures/key_features_distribution_landlord.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" Saved: key_features_distribution_landlord.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "if len(numeric_features) > 1:\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    correlation_matrix = X[numeric_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap - Landlord Features Only', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../outputs/figures/correlation_heatmap_landlord.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" Saved: correlation_heatmap_landlord.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "##  Documentation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive documentation\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "Task 1.5: Feature Selection \n",
    "{'='*80}\n",
    "\n",
    "Real-world use case: \n",
    "- Predict if a new listing while has no reviews will be good value for money\n",
    "- Based on: landlord's price + property characteristics\n",
    "\n",
    "{'='*80}\n",
    "Dataset Information:\n",
    "{'='*80}\n",
    "\n",
    "-Input Dataset: listings_with_categorical_encoding.csv\n",
    "-Total Samples: {len(df)}\n",
    "-Total Features (before filtering): {len(df.columns)}\n",
    "\n",
    "-Output Dataset: listings_landlord_features_only.csv\n",
    "-Total Samples: {len(landlord_df)}\n",
    "-Landlord Features: {len(available_landlord_features)}\n",
    "\n",
    "{'='*80}\n",
    "Feature Categories\n",
    "{'='*80}\n",
    "\n",
    "1. Landlord-Controlled Features : {len(available_landlord_features)}\n",
    "{chr(10).join(['   - ' + f for f in available_landlord_features])}\n",
    "\n",
    "2. Review-Based Features : {len(review_features_to_remove)}\n",
    "{chr(10).join(['   - ' + f for f in review_features_to_remove])}\n",
    "\n",
    "3. Target Leakage Features : {len(target_leakage_features)}\n",
    "{chr(10).join(['   - ' + f for f in target_leakage_features])}\n",
    "\n",
    "{'='*80}\n",
    "Target Variable Distribution\n",
    "{'='*80}\n",
    "\n",
    "{y.value_counts()}\n",
    "\n",
    "Percentage Distribution:\n",
    "{y.value_counts(normalize=True) * 100}\n",
    "\n",
    "\n",
    "{'='*80}\n",
    "Feature Scaling\n",
    "{'='*80}\n",
    "\n",
    "Method: StandardScaler (zero mean, unit variance)\n",
    "Numeric Features Scaled: {len(numeric_features)}\n",
    "\n",
    "{'='*80}\n",
    "Output files generated\n",
    "{'='*80}\n",
    "\n",
    "1. listings_landlord_features_only.csv - Full dataset with landlord features\n",
    "2. X_train_landlord.csv - Training features (unscaled)\n",
    "3. X_test_landlord.csv - Test features (unscaled)\n",
    "4. y_train_landlord.csv - Training target\n",
    "5. y_test_landlord.csv - Test target\n",
    "6. X_train_landlord_scaled.csv - Training features (scaled)\n",
    "7. X_test_landlord_scaled.csv - Test features (scaled)\n",
    "8. target_distribution_landlord.png - Target distribution plot\n",
    "9. key_features_distribution_landlord.png - Key features histograms\n",
    "10. correlation_heatmap_landlord.png - Feature correlation heatmap\n",
    "\n",
    "{'='*80}\n",
    "Critical Reminders\n",
    "{'='*80}\n",
    "\n",
    " -Price must be included as a feature \n",
    " -Review features used only for labeling, not for training\n",
    " -Model predicts value for new listings without reviews\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Did:\n",
    "1.  Loaded dataset from T1.4 (with categorical encoding)\n",
    "2.  Identified and categorized all features\n",
    "3.  Removed review-based features from model input\n",
    "4.  Removed target leakage features\n",
    "5.  Kept only landlord-controlled features\n",
    "6.  Created train-test split (80-20, stratified)\n",
    "7.  Applied feature scaling (StandardScaler)\n",
    "8.  Saved all processed datasets with 'landlord' suffix\n",
    "9.  Generated visualizations and documentation\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Data leakage fixed**: Review features removed from model input\n",
    "- **Price included**: Critical for predicting \"value for money\"\n",
    "- **Production-ready**: Model can predict for NEW listings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Summary of All Tasks\n",
    "\n",
    "### Task 1.1: Initial Data Exploration\n",
    "- Loaded SF (7,780) and SD (13,162) datasets\n",
    "- Identified 79 columns with matching structure\n",
    "- Analyzed missing values and data quality issues\n",
    "\n",
    "### Task 1.2: Data Cleaning and Preprocessing\n",
    "- Cleaned price column and converted data types\n",
    "- Created engineered features (host_years, price_per_person, etc.)\n",
    "- Handled missing values and outliers\n",
    "- Created target variable (value_category) from FP Score\n",
    "- Output: `listings_cleaned_with_target.csv` (19,912 rows × 73 columns)\n",
    "\n",
    "### Task 1.3: Algebraic Feature Engineering\n",
    "- Created 10 landlord-controlled algebraic features\n",
    "- Features: space_efficiency, price_per_bedroom, price_per_bathroom, etc.\n",
    "- All features use ONLY landlord-controlled data (no review features)\n",
    "- Output: `listings_with_algebraic_features.csv` (19,912 rows × 83 columns)\n",
    "\n",
    "### Task 1.4: Categorical Encoding\n",
    "- Encoded room_type (One-Hot)\n",
    "- Encoded property_type (Label + Frequency)\n",
    "- Encoded neighbourhood (Target + Frequency + Label)\n",
    "- Encoded value_category (Label - ordinal)\n",
    "- Output: `listings_with_categorical_encoding.csv` (19,912 rows × 90 columns)\n",
    "\n",
    "### Task 1.5: Feature Selection \n",
    "- Kept 28 landlord-controlled features \n",
    "- Created train-test split \n",
    "- Applied StandardScaler for feature scaling\n",
    "- Output files:\n",
    "  - `listings_landlord_features_only.csv`\n",
    "  - `X_train_landlord_scaled.csv`, `X_test_landlord_scaled.csv`\n",
    "  - `y_train_landlord.csv`, `y_test_landlord.csv`\n",
    "\n",
    "## Final Dataset for Modeling\n",
    "\n",
    "- **Features:** 28 landlord-controlled features\n",
    "- **Samples:** 19,912 listings\n",
    "- **Target:** value_category (Poor_Value, Fair_Value, Excellent_Value)\n",
    "- **Train/Test Split:** 15,929 / 3,983 (80/20)\n",
    "\n",
    "## Key Achievements\n",
    "\n",
    " - **Data Leakage Fixed:** Review features removed from model input\n",
    " - **Price Included:** Critical for predicting value-for-money\n",
    " - **Production-Ready:** Model can predict for new listings without reviews\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
