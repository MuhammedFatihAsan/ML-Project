{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "#  ML Models with NLP Integration\n",
    "\n",
    "##  Integrating Word2Vec NLP Scores with Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "###  Project Overview\n",
    "\n",
    "This notebook represents a **critical milestone** in our Airbnb value prediction project. We are integrating advanced NLP features (Word2Vec scores) with our existing machine learning models to improve prediction accuracy.\n",
    "\n",
    "###  Objectives\n",
    "\n",
    "1. **Load the enhanced dataset** with Word2Vec NLP scores\n",
    "2. **Train three models** with cross-validation:\n",
    "   - Logistic Regression (Baseline)\n",
    "   - Random Forest (Ensemble)\n",
    "   - XGBoost (Advanced Boosting)\n",
    "3. **Compare performance** using rigorous cross-validation\n",
    "4. **Analyze feature importance** to understand model decisions\n",
    "\n",
    "###  What's New?\n",
    "\n",
    "**Word2Vec NLP Score Integration:**\n",
    "\n",
    "Based on Fatih's comprehensive NLP analysis (see `mfa_Advanced_NLP-AND-Feature_Engineering.ipynb`), we selected the **Word2Vec 100-dimension model** for the following reasons:\n",
    "\n",
    "-  **Best Cost-Performance Balance:** 51.7% accuracy with minimal computational cost\n",
    "-  **Airbnb-Specific Vocabulary:** Trained specifically on our listing descriptions\n",
    "-  **Normalized Score:** Ranges from -1 (Poor Value) to +1 (Excellent Value)\n",
    "-  **Efficient:** 100 dimensions vs 768 for BERT\n",
    "\n",
    "**Comparison of NLP Models:**\n",
    "\n",
    "| Model | Accuracy | F1 Score | Computational Cost | Selected |\n",
    "|-------|----------|----------|-------------------|----------|\n",
    "| Baseline (TF-IDF + VADER) | 49.68% | 0.49 | Very Low (1/5) |  |\n",
    "| **Word2Vec (100D)** | **51.70%** | **0.51** | **Low (2/5)** | **** |\n",
    "| BERT (768D) | 52.44% | 0.52 | Very High (5/5) |  |\n",
    "\n",
    "**Why Word2Vec?**\n",
    "\n",
    "> *\"Considering the success and computational costs of 3 different NLP models, the word2vec model, obtained with 100 vector dimensions, was trained on the data using only its own features. The word2vec 100 dimension model scores the description between -1 and 1. We use this because it was the best in the calculation cost and performance equation among the results obtained with 3 different models.\"*  \n",
    "> — Fatih \n",
    "\n",
    "---\n",
    "\n",
    "###  Dataset Information\n",
    "\n",
    "- **Total Samples:** 19,913 Airbnb listings\n",
    "- **Features:** 27 landlord-controlled features + 1 NLP score\n",
    "- **Target Classes:** 3 balanced classes (Poor, Fair, Excellent Value)\n",
    "- **Data Source:** `final_data_with_nlp_score.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "##  Step 1: Environment Setup and Library Imports\n",
    "\n",
    "### Libraries Used:\n",
    "\n",
    "**Data Processing:**\n",
    "- `pandas` - Data manipulation and analysis\n",
    "- `numpy` - Numerical computations\n",
    "\n",
    "**Machine Learning Models:**\n",
    "- `LogisticRegression` - Linear baseline model\n",
    "- `RandomForestClassifier` - Ensemble learning\n",
    "- `XGBClassifier` - Gradient boosting\n",
    "\n",
    "**Model Evaluation:**\n",
    "- `cross_validate` - K-fold cross-validation\n",
    "- `StratifiedKFold` - Maintains class distribution in folds\n",
    "- `classification_report` - Detailed per-class metrics\n",
    "- `confusion_matrix` - Error analysis\n",
    "\n",
    "**Preprocessing:**\n",
    "- `StandardScaler` - Feature normalization\n",
    "- `LabelEncoder` - Target encoding\n",
    "\n",
    "**Visualization:**\n",
    "- `matplotlib` & `seaborn` - Plotting and visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "##  Step 2: Data Loading and Initial Exploration\n",
    "\n",
    "### Data Source\n",
    "\n",
    "We load the **final integrated dataset** that combines:\n",
    "1. **Landlord-controlled features** (26 features) - from previous preprocessing\n",
    "2. **Word2Vec NLP score** (1 feature) - from Fatih's NLP analysis\n",
    "3. **Target variable** - value_category (Poor/Fair/Excellent Value)\n",
    "\n",
    "### Key Features in Dataset:\n",
    "\n",
    "**Pricing Features:**\n",
    "- `price` - Listing price (scaled)\n",
    "- `price_per_bedroom` - Price efficiency metric\n",
    "- `price_per_bathroom` - Price efficiency metric\n",
    "\n",
    "**Property Features:**\n",
    "- `accommodates`, `bedrooms`, `beds` - Capacity metrics\n",
    "- `room_type_*` - One-hot encoded room types\n",
    "- `property_type_*` - Encoded property types\n",
    "\n",
    "**Location Features:**\n",
    "- `latitude`, `longitude` - Geographic coordinates\n",
    "- `neighbourhood_*` - Encoded neighborhood information\n",
    "\n",
    "**Host Features:**\n",
    "- `host_is_superhost` - Host status\n",
    "- `host_identity_verified` - Verification status\n",
    "- `host_response_rate` - Host responsiveness\n",
    "\n",
    "**Availability Features:**\n",
    "- `availability_30/60/90/365` - Booking availability\n",
    "- `instant_bookable` - Booking flexibility\n",
    "\n",
    "**Engineered Features:**\n",
    "- `space_efficiency` - Space utilization metric\n",
    "\n",
    "**NLP Feature:**\n",
    "- `w2v_score` - Word2Vec sentiment score (-1 to +1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final dataset with NLP scores\n",
    "print(\"=\"*80)\n",
    "print(\"Loading enhanced dataset with NLP integration...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = pd.read_csv('../../data/finalized/final_data_with_nlp_score.csv')\n",
    "\n",
    "print(f\"\\n Dataset loaded successfully!\")\n",
    "print(f\"\\n Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"\\n Missing Values: {missing_count}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\n First 3 rows of the dataset:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Check w2v_score statistics\n",
    "print(f\"\\n Word2Vec Score Statistics:\")\n",
    "print(df['w2v_score'].describe())\n",
    "\n",
    "print(f\"\\n Interpretation:\")\n",
    "print(f\"  • Mean score: {df['w2v_score'].mean():.4f} (close to 0 = balanced)\")\n",
    "print(f\"  • Std deviation: {df['w2v_score'].std():.4f}\")\n",
    "print(f\"  • Range: [{df['w2v_score'].min():.4f}, {df['w2v_score'].max():.4f}]\")\n",
    "print(f\"  • Scores near +1 indicate 'Excellent Value' descriptions\")\n",
    "print(f\"  • Scores near -1 indicate 'Poor Value' descriptions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "##  Step 3: Target Variable Analysis\n",
    "\n",
    "### Understanding Our Target: `value_category`\n",
    "\n",
    "The target variable represents the **value-for-money** classification of each listing:\n",
    "\n",
    "**Class Definitions:**\n",
    "- **Poor_Value (0):** High price relative to quality/features\n",
    "- **Fair_Value (1):** Balanced price-to-quality ratio\n",
    "- **Excellent_Value (2):** Low price relative to quality/features\n",
    "\n",
    "**Why Class Balance Matters:**\n",
    "- Imbalanced classes can bias model predictions\n",
    "- We use `StratifiedKFold` to maintain class distribution in CV folds\n",
    "- Models use `class_weight='balanced'` to handle any imbalance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count distribution\n",
    "target_counts = df['value_category'].value_counts().sort_index()\n",
    "target_pcts = df['value_category'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(f\"\\n Class Distribution:\")\n",
    "print(f\"\\n{'Category':<20} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for category in sorted(df['value_category'].unique()):\n",
    "    count = target_counts[category]\n",
    "    pct = target_pcts[category]\n",
    "    print(f\"{category:<20} {count:>10,} {pct:>11.2f}%\")\n",
    "\n",
    "print(f\"\\n{'Total':<20} {len(df):>10,} {100.0:>11.2f}%\")\n",
    "\n",
    "# Check balance\n",
    "max_pct = target_pcts.max()\n",
    "min_pct = target_pcts.min()\n",
    "imbalance_ratio = max_pct / min_pct\n",
    "\n",
    "print(f\"\\n Balance Analysis:\")\n",
    "print(f\"  • Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio < 1.5:\n",
    "    print(f\" Classes are well-balanced!\")\n",
    "elif imbalance_ratio < 3:\n",
    "    print(f\" Slight imbalance detected\")\n",
    "else:\n",
    "    print(f\"  Significant imbalance - will use class weights\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=['#e74c3c', '#f39c12', '#27ae60'])\n",
    "axes[0].set_xlabel('Value Category', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%',\n",
    "            colors=['#e74c3c', '#f39c12', '#27ae60'], startangle=90)\n",
    "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##  Step 4: Feature-Target Separation and Encoding\n",
    "\n",
    "### Data Preparation Steps:\n",
    "\n",
    "1. **Separate Features (X) and Target (y)**\n",
    "   - X: All columns except `value_category`\n",
    "   - y: Only `value_category`\n",
    "\n",
    "2. **Handle Boolean Columns**\n",
    "   - Convert boolean features to integers (0/1)\n",
    "   - Required for model compatibility\n",
    "\n",
    "3. **Encode Target Variable**\n",
    "   - Convert text labels to numeric codes\n",
    "   - Excellent_Value → 0\n",
    "   - Fair_Value → 1\n",
    "   - Poor_Value → 2\n",
    "\n",
    "4. **Train-Test Split**\n",
    "   - 80% training, 20% testing\n",
    "   - Stratified split maintains class balance\n",
    "   - Random state = 42 for reproducibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate X and y\n",
    "X = df.drop('value_category', axis=1)\n",
    "y = df['value_category']\n",
    "\n",
    "print(f\"\\n Features (X): {X.shape}\")\n",
    "print(f\" Target (y): {y.shape}\")\n",
    "\n",
    "# Convert boolean columns to int\n",
    "bool_cols = X.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    print(f\"\\n Converting {len(bool_cols)} boolean columns to integers:\")\n",
    "    for col in bool_cols:\n",
    "        print(f\"  • {col}\")\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n Target Encoding Mapping:\")\n",
    "for idx, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} → {idx}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\n Data Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Testing set:  {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\n Class distribution maintained in splits:\")\n",
    "train_dist = pd.Series(y_train).value_counts(normalize=True).sort_index() * 100\n",
    "test_dist = pd.Series(y_test).value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(f\"\\n{'Class':<15} {'Train %':>10} {'Test %':>10}\")\n",
    "print(\"-\" * 38)\n",
    "for idx in range(len(label_encoder.classes_)):\n",
    "    class_name = label_encoder.classes_[idx]\n",
    "    print(f\"{class_name:<15} {train_dist[idx]:>9.2f}% {test_dist[idx]:>9.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "##  Step 5: Feature Scaling\n",
    "\n",
    "### Why would we need to scale features?\n",
    "\n",
    "**Problems:**\n",
    "- Features have different scales (e.g., price: 0-1000, bedrooms: 1-10)\n",
    "- Models like Logistic Regression are sensitive to feature scales\n",
    "- Large-scale features can dominate the model\n",
    "\n",
    "**Solution: StandardScaler**\n",
    "- Transforms features to have mean=0 and std=1\n",
    "- Formula: `z = (x - μ) / σ`\n",
    "- Preserves the shape of the distribution\n",
    "\n",
    "**Important notes:**\n",
    "-  Fit scaler on training data only\n",
    "-  Transform both train and test using the same scaler\n",
    "-  Never fit scaler on the test data (causes data leakage)\n",
    "\n",
    "**Models that benefit from scaling:**\n",
    "- Logistic Regression\n",
    "- Random Forest (optional)\n",
    "- XGBoost (optional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\n Features scaled successfully!\")\n",
    "print(f\"\\n Scaled Training Data Statistics:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean().mean():.6f} (should be ≈ 0)\")\n",
    "print(f\"  Std:  {X_train_scaled.std().mean():.6f} (should be ≈ 1)\")\n",
    "\n",
    "# Show example of scaling effect\n",
    "print(f\"\\n Example: 'price' feature before and after scaling:\")\n",
    "print(f\"  Before - Mean: {X_train['price'].mean():.4f}, Std: {X_train['price'].std():.4f}\")\n",
    "print(f\"  After  - Mean: {X_train_scaled['price'].mean():.4f}, Std: {X_train_scaled['price'].std():.4f}\")\n",
    "\n",
    "print(f\"\\n Example: 'w2v_score' feature before and after scaling:\")\n",
    "print(f\"  Before - Mean: {X_train['w2v_score'].mean():.4f}, Std: {X_train['w2v_score'].std():.4f}\")\n",
    "print(f\"  After  - Mean: {X_train_scaled['w2v_score'].mean():.4f}, Std: {X_train_scaled['w2v_score'].std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##  Step 6: Cross-Validation Strategy\n",
    "\n",
    "### Why do we need Cross-Validation?\n",
    "\n",
    "**Problem with Single Train-Test Split:**\n",
    "- Results depend on which samples end up in train vs test\n",
    "- May get lucky/unlucky with the split\n",
    "- Less reliable performance estimate\n",
    "\n",
    "**Solution: K-Fold Cross-Validation**\n",
    "- Split data into K folds (we use K=5)\n",
    "- Train K times, each time using different fold as test set\n",
    "- Average results across all folds\n",
    "- More robust and reliable performance estimate\n",
    "\n",
    "### Our Strategy: 5-Fold Stratified Cross-Validation\n",
    "\n",
    "```\n",
    "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
    "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
    "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
    "Fold 4: [Train] [Train] [Train] [Test] [Train]\n",
    "Fold 5: [Train] [Train] [Train] [Train] [Test]\n",
    "```\n",
    "\n",
    "**Stratified:** Each fold maintains the same class distribution as the original dataset\n",
    "\n",
    "### Metrics We'll Track:\n",
    "\n",
    "1. **Accuracy** - Overall correctness\n",
    "2. **Precision (Macro)** - Average precision across classes\n",
    "3. **Recall (Macro)** - Average recall across classes\n",
    "4. **F1-Score (Macro)** - Harmonic mean of precision and recall\n",
    "5. **Training Time** - Computational efficiency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv_folds = 5\n",
    "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\n Cross-Validation Configuration:\")\n",
    "print(f\"  • Strategy: Stratified K-Fold\")\n",
    "print(f\"  • Number of Folds: {cv_folds}\")\n",
    "print(f\"  • Shuffle: Yes\")\n",
    "print(f\"  • Random State: 42 (for reproducibility)\")\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision_macro',\n",
    "    'recall': 'recall_macro',\n",
    "    'f1': 'f1_macro'\n",
    "}\n",
    "\n",
    "print(f\"\\n Evaluation Metrics:\")\n",
    "for metric_name, metric_func in scoring_metrics.items():\n",
    "    print(f\"  • {metric_name.capitalize()}: {metric_func}\")\n",
    "\n",
    "print(f\"\\n Why These Metrics?\")\n",
    "print(f\"  • Accuracy: Overall performance\")\n",
    "print(f\"  • Precision: How many predicted positives are actually positive\")\n",
    "print(f\"  • Recall: How many actual positives we correctly identified\")\n",
    "print(f\"  • F1-Score: Balance between precision and recall\")\n",
    "print(f\"  • Macro averaging: Treats all classes equally (important for balanced evaluation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##  Step 7: Model 1 - Logistic Regression (Baseline)\n",
    "\n",
    "### Model Overview\n",
    "\n",
    "**Logistic Regression** is our baseline model - simple, interpretable, and fast.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Linear Combination:** Calculates weighted sum of features\n",
    "2. **Sigmoid Function:** Converts to probabilities (0 to 1)\n",
    "3. **Multi-class Extension:** Uses One-vs-Rest or Softmax for 3 classes\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "- **C=1.0** - Regularization strength (inverse)\n",
    "  - Smaller C = stronger regularization = simpler model\n",
    "  - Larger C = weaker regularization = more complex model\n",
    "  \n",
    "- **penalty='l2'** - Ridge regularization\n",
    "  - Prevents overfitting by penalizing large coefficients\n",
    "  - L2 shrinks coefficients but doesn't eliminate them\n",
    "  \n",
    "- **max_iter=1000** - Maximum iterations for convergence\n",
    "  - Ensures the optimization algorithm has enough time to converge\n",
    "  \n",
    "- **class_weight='balanced'** - Automatic class balancing\n",
    "  - Adjusts weights inversely proportional to class frequencies\n",
    "  - Prevents bias toward majority class\n",
    "  \n",
    "- **random_state=42** - Reproducibility\n",
    "\n",
    "### Advantages:\n",
    "-  Fast training and prediction\n",
    "-  Interpretable coefficients\n",
    "-  Works well with scaled features\n",
    "-  Low risk of overfitting\n",
    "\n",
    "### Disadvantages:\n",
    "-  Assumes linear relationships\n",
    "-  May underfit complex patterns\n",
    "-  Sensitive to feature scaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    penalty='l2',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n Model Configuration:\")\n",
    "print(f\"  • Algorithm: Logistic Regression\")\n",
    "print(f\"  • Regularization: L2 (Ridge)\")\n",
    "print(f\"  • C parameter: 1.0\")\n",
    "print(f\"  • Class weights: Balanced\")\n",
    "print(f\"  • Max iterations: 1000\")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(f\"\\n Running 5-Fold Cross-Validation...\")\n",
    "\n",
    "lr_cv_results = cross_validate(\n",
    "    lr_model,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=scoring_metrics,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std for each metric\n",
    "print(f\"\\n Cross-Validation Results:\")\n",
    "print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    test_scores = lr_cv_results[f'test_{metric}']\n",
    "    mean_score = test_scores.mean()\n",
    "    std_score = test_scores.std()\n",
    "    min_score = test_scores.min()\n",
    "    max_score = test_scores.max()\n",
    "    \n",
    "    print(f\"{metric.capitalize():<20} {mean_score:>10.4f} {std_score:>10.4f} {min_score:>10.4f} {max_score:>10.4f}\")\n",
    "\n",
    "# Training time\n",
    "avg_fit_time = lr_cv_results['fit_time'].mean()\n",
    "avg_score_time = lr_cv_results['score_time'].mean()\n",
    "\n",
    "print(f\"\\n Computational Performance:\")\n",
    "print(f\"  • Average training time per fold: {avg_fit_time:.3f} seconds\")\n",
    "print(f\"  • Average scoring time per fold: {avg_score_time:.3f} seconds\")\n",
    "print(f\"  • Total CV time: {(avg_fit_time + avg_score_time) * cv_folds:.3f} seconds\")\n",
    "\n",
    "# Train final model on full training set\n",
    "print(f\"\\n Training final model on full training set...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "test_accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "test_f1_lr = f1_score(y_test, y_pred_lr, average='macro')\n",
    "\n",
    "print(f\"\\n Final Test Set Performance:\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_lr:.4f} ({test_accuracy_lr*100:.2f}%)\")\n",
    "print(f\"  • Test F1-Score: {test_f1_lr:.4f}\")\n",
    "\n",
    "# Store results\n",
    "lr_results = {\n",
    "    'model_name': 'Logistic Regression',\n",
    "    'cv_accuracy_mean': lr_cv_results['test_accuracy'].mean(),\n",
    "    'cv_accuracy_std': lr_cv_results['test_accuracy'].std(),\n",
    "    'cv_f1_mean': lr_cv_results['test_f1'].mean(),\n",
    "    'cv_f1_std': lr_cv_results['test_f1'].std(),\n",
    "    'test_accuracy': test_accuracy_lr,\n",
    "    'test_f1': test_f1_lr,\n",
    "    'avg_fit_time': avg_fit_time\n",
    "}\n",
    "\n",
    "print(f\"\\n Logistic Regression training has been completed!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "##  Step 8: Model 2 - Random Forest \n",
    "\n",
    "### Model Overview\n",
    "\n",
    "**Random Forest** is an ensemble method that combines multiple decision trees.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Bootstrap Sampling:** Create multiple random subsets of training data\n",
    "2. **Build Trees:** Train a decision tree on each subset\n",
    "3. **Random Features:** Each split considers only a random subset of features\n",
    "4. **Voting:** Final prediction is the majority vote from all trees\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "- **n_estimators=100** - Number of trees in the forest\n",
    "  - More trees = better performance but slower training\n",
    "  - 100 is a good balance for our dataset size\n",
    "  \n",
    "- **max_depth=20** - Maximum depth of each tree\n",
    "  - Controls model complexity\n",
    "  - Prevents individual trees from overfitting\n",
    "  \n",
    "- **min_samples_split=10** - Minimum samples to split a node\n",
    "  - Higher values prevent overfitting\n",
    "  - Ensures splits are statistically meaningful\n",
    "  \n",
    "- **min_samples_leaf=4** - Minimum samples in leaf nodes\n",
    "  - Prevents creating leaves with very few samples\n",
    "  - Improves generalization\n",
    "  \n",
    "- **class_weight='balanced'** - Automatic class balancing\n",
    "  \n",
    "- **random_state=42** - Reproducibility\n",
    "  \n",
    "- **n_jobs=-1** - Use all CPU cores for parallel training\n",
    "\n",
    "### Advantages:\n",
    "-  Handles non-linear relationships\n",
    "-  Robust to outliers\n",
    "-  Provides feature importance\n",
    "-  Less prone to overfitting than single trees\n",
    "-  Works with unscaled features\n",
    "\n",
    "### Disadvantages:\n",
    "-  Slower than logistic regression\n",
    "-  Less interpretable\n",
    "-  Larger model size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n Model Configuration:\")\n",
    "print(f\"  • Algorithm: Random Forest\")\n",
    "print(f\"  • Number of trees: 100\")\n",
    "print(f\"  • Max depth: 20\")\n",
    "print(f\"  • Min samples split: 10\")\n",
    "print(f\"  • Min samples leaf: 4\")\n",
    "print(f\"  • Class weights: Balanced\")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(f\"\\n Running 5-Fold Cross-Validation...\")\n",
    "\n",
    "rf_cv_results = cross_validate(\n",
    "    rf_model,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=scoring_metrics,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std for each metric\n",
    "print(f\"\\n Cross-Validation Results:\")\n",
    "print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    test_scores = rf_cv_results[f'test_{metric}']\n",
    "    mean_score = test_scores.mean()\n",
    "    std_score = test_scores.std()\n",
    "    min_score = test_scores.min()\n",
    "    max_score = test_scores.max()\n",
    "    \n",
    "    print(f\"{metric.capitalize():<20} {mean_score:>10.4f} {std_score:>10.4f} {min_score:>10.4f} {max_score:>10.4f}\")\n",
    "\n",
    "# Training time\n",
    "avg_fit_time = rf_cv_results['fit_time'].mean()\n",
    "avg_score_time = rf_cv_results['score_time'].mean()\n",
    "\n",
    "print(f\"\\n Computational Performance:\")\n",
    "print(f\"  • Average training time per fold: {avg_fit_time:.3f} seconds\")\n",
    "print(f\"  • Average scoring time per fold: {avg_score_time:.3f} seconds\")\n",
    "print(f\"  • Total CV time: {(avg_fit_time + avg_score_time) * cv_folds:.3f} seconds\")\n",
    "\n",
    "# Train final model on full training set\n",
    "print(f\"\\n Training final model on full training set...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "test_f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "\n",
    "print(f\"\\n Final Test Set Performance:\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_rf:.4f} ({test_accuracy_rf*100:.2f}%)\")\n",
    "print(f\"  • Test F1-Score: {test_f1_rf:.4f}\")\n",
    "\n",
    "# Store results\n",
    "rf_results = {\n",
    "    'model_name': 'Random Forest',\n",
    "    'cv_accuracy_mean': rf_cv_results['test_accuracy'].mean(),\n",
    "    'cv_accuracy_std': rf_cv_results['test_accuracy'].std(),\n",
    "    'cv_f1_mean': rf_cv_results['test_f1'].mean(),\n",
    "    'cv_f1_std': rf_cv_results['test_f1'].std(),\n",
    "    'test_accuracy': test_accuracy_rf,\n",
    "    'test_f1': test_f1_rf,\n",
    "    'avg_fit_time': avg_fit_time\n",
    "}\n",
    "\n",
    "print(f\"\\n Random Forest training has been completed!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##  Step 9: Model 3 - XGBoost \n",
    "\n",
    "### Model Overview\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is a powerful gradient boosting algorithm.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Sequential Learning:** Builds trees one at a time\n",
    "2. **Error Correction:** Each new tree focuses on correcting previous errors\n",
    "3. **Gradient Descent:** Uses gradients to minimize loss function\n",
    "4. **Regularization:** Built-in L1/L2 regularization prevents overfitting\n",
    "\n",
    "### XGBoost vs Random Forest:\n",
    "\n",
    "| Aspect | Random Forest | XGBoost |\n",
    "|--------|--------------|----------|\n",
    "| Tree Building | Parallel (independent) | Sequential (corrective) |\n",
    "| Learning | Bagging (averaging) | Boosting (error correction) |\n",
    "| Overfitting Risk | Lower | Higher (needs tuning) |\n",
    "| Training Speed | Faster | Slower |\n",
    "| Accuracy | Good | Often Better |\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "- **learning_rate=0.1** - Step size for each tree\n",
    "  - Smaller = more conservative, needs more trees\n",
    "  - Larger = more aggressive, may overfit\n",
    "  \n",
    "- **max_depth=6** - Maximum depth of each tree\n",
    "  - Controls complexity of individual trees\n",
    "  - Deeper trees can capture more complex patterns\n",
    "  \n",
    "- **n_estimators=200** - Number of boosting rounds\n",
    "  - More rounds = better fit but risk of overfitting\n",
    "  - 200 is a good balance\n",
    "  \n",
    "- **subsample=0.8** - Fraction of samples for each tree\n",
    "  - Adds randomness to prevent overfitting\n",
    "  - 0.8 means use 80% of data for each tree\n",
    "  \n",
    "- **colsample_bytree=0.8** - Fraction of features for each tree\n",
    "  - Similar to Random Forest's feature randomness\n",
    "  - Prevents reliance on single features\n",
    "  \n",
    "- **objective='multi:softmax'** - Multi-class classification\n",
    "  \n",
    "- **eval_metric='mlogloss'** - Multi-class log loss\n",
    "  \n",
    "- **random_state=42** - Reproducibility\n",
    "  \n",
    "- **n_jobs=-1** - Parallel processing\n",
    "\n",
    "### Advantages:\n",
    "-  Often highest accuracy\n",
    "-  Handles complex patterns\n",
    "-  Built-in regularization\n",
    "-  Feature importance\n",
    "-  Handles missing values\n",
    "\n",
    "### Disadvantages:\n",
    "- Slowest training time\n",
    "- More hyperparameters to tune\n",
    "- Risk of overfitting if not tuned properly\n",
    "- Less interpretable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: XGBoost with Cross-Validation\n",
    "\n",
    "# Initialize model\n",
    "xgb_model = XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softmax',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n Model Configuration:\")\n",
    "print(f\"  • Algorithm: XGBoost (Gradient Boosting)\")\n",
    "print(f\"  • Learning rate: 0.1\")\n",
    "print(f\"  • Max depth: 6\")\n",
    "print(f\"  • Number of estimators: 200\")\n",
    "print(f\"  • Subsample ratio: 0.8\")\n",
    "print(f\"  • Column sample ratio: 0.8\")\n",
    "print(f\"  • Objective: Multi-class softmax\")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(f\"\\n Running 5-Fold Cross-Validation...\")\n",
    "\n",
    "xgb_cv_results = cross_validate(\n",
    "    xgb_model,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=cv_strategy,\n",
    "    scoring=scoring_metrics,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std for each metric\n",
    "print(f\"\\n Cross-Validation Results:\")\n",
    "print(f\"\\n{'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    test_scores = xgb_cv_results[f'test_{metric}']\n",
    "    mean_score = test_scores.mean()\n",
    "    std_score = test_scores.std()\n",
    "    min_score = test_scores.min()\n",
    "    max_score = test_scores.max()\n",
    "    \n",
    "    print(f\"{metric.capitalize():<20} {mean_score:>10.4f} {std_score:>10.4f} {min_score:>10.4f} {max_score:>10.4f}\")\n",
    "\n",
    "# Training time\n",
    "avg_fit_time = xgb_cv_results['fit_time'].mean()\n",
    "avg_score_time = xgb_cv_results['score_time'].mean()\n",
    "\n",
    "print(f\"\\n Computational Performance:\")\n",
    "print(f\"  • Average training time per fold: {avg_fit_time:.3f} seconds\")\n",
    "print(f\"  • Average scoring time per fold: {avg_score_time:.3f} seconds\")\n",
    "print(f\"  • Total CV time: {(avg_fit_time + avg_score_time) * cv_folds:.3f} seconds\")\n",
    "\n",
    "# Train final model on full training set\n",
    "print(f\"\\n Training final model on full training set...\")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "test_f1_xgb = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "\n",
    "print(f\"\\n Final Test Set Performance:\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_xgb:.4f} ({test_accuracy_xgb*100:.2f}%)\")\n",
    "print(f\"  • Test F1-Score: {test_f1_xgb:.4f}\")\n",
    "\n",
    "# Store results\n",
    "xgb_results = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'cv_accuracy_mean': xgb_cv_results['test_accuracy'].mean(),\n",
    "    'cv_accuracy_std': xgb_cv_results['test_accuracy'].std(),\n",
    "    'cv_f1_mean': xgb_cv_results['test_f1'].mean(),\n",
    "    'cv_f1_std': xgb_cv_results['test_f1'].std(),\n",
    "    'test_accuracy': test_accuracy_xgb,\n",
    "    'test_f1': test_f1_xgb,\n",
    "    'avg_fit_time': avg_fit_time\n",
    "}\n",
    "\n",
    "print(f\"\\n XGBoost training has been completed!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "##  Step 10: Comprehensive Model Comparison\n",
    "\n",
    "### Comparison Criteria\n",
    "\n",
    "We compare all three models across multiple dimensions:\n",
    "\n",
    "1. **Cross-Validation Performance** - Most reliable metric\n",
    "2. **Test Set Performance** - Final validation\n",
    "3. **Computational Efficiency** - Training time\n",
    "4. **Stability** - Standard deviation across folds\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "- **Best CV Accuracy** - Most reliable predictor of real-world performance\n",
    "- **Lowest Std** - Most stable and consistent model\n",
    "- **Test vs CV Agreement** - Models should perform similarly on both\n",
    "- **Speed vs Accuracy Trade-off** - Is extra training time worth the accuracy gain?\n",
    "\n",
    "### Expected Outcomes:\n",
    "\n",
    "Based on previous experiments:\n",
    "- **Logistic Regression:** Fast, decent baseline (~95% accuracy)\n",
    "- **Random Forest:** Good balance, robust (~95% accuracy)\n",
    "- **XGBoost:** Highest accuracy, but slowest (~95-96% accuracy)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame([lr_results, rf_results, xgb_results])\n",
    "\n",
    "# Display comparison table\n",
    "print(f\"\\n Cross-Validation Performance (5-Fold):\")\n",
    "print(f\"\\n{'Model':<20} {'CV Accuracy':>15} {'CV F1-Score':>15} {'Std (Acc)':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['model_name']:<20} \"\n",
    "          f\"{row['cv_accuracy_mean']:>14.4f} \"\n",
    "          f\"{row['cv_f1_mean']:>14.4f} \"\n",
    "          f\"{row['cv_accuracy_std']:>11.4f}\")\n",
    "\n",
    "print(f\"\\n Final Test Set Performance:\")\n",
    "print(f\"\\n{'Model':<20} {'Test Accuracy':>15} {'Test F1-Score':>15}\")\n",
    "print(\"-\" * 53)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['model_name']:<20} \"\n",
    "          f\"{row['test_accuracy']:>14.4f} \"\n",
    "          f\"{row['test_f1']:>14.4f}\")\n",
    "\n",
    "print(f\"\\n Computational Efficiency:\")\n",
    "print(f\"\\n{'Model':<20} {'Avg Training Time':>20} {'Relative Speed':>15}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "min_time = comparison_df['avg_fit_time'].min()\n",
    "for _, row in comparison_df.iterrows():\n",
    "    relative_speed = row['avg_fit_time'] / min_time\n",
    "    print(f\"{row['model_name']:<20} \"\n",
    "          f\"{row['avg_fit_time']:>18.3f}s \"\n",
    "          f\"{relative_speed:>14.2f}x\")\n",
    "\n",
    "# Find best model\n",
    "best_cv_idx = comparison_df['cv_accuracy_mean'].idxmax()\n",
    "best_test_idx = comparison_df['test_accuracy'].idxmax()\n",
    "fastest_idx = comparison_df['avg_fit_time'].idxmin()\n",
    "\n",
    "print(f\"\\n Model Rankings:\")\n",
    "print(f\"  • Best CV Accuracy: {comparison_df.loc[best_cv_idx, 'model_name']} \"\n",
    "      f\"({comparison_df.loc[best_cv_idx, 'cv_accuracy_mean']:.4f})\")\n",
    "print(f\"  • Best Test Accuracy: {comparison_df.loc[best_test_idx, 'model_name']} \"\n",
    "      f\"({comparison_df.loc[best_test_idx, 'test_accuracy']:.4f})\")\n",
    "print(f\"  • Fastest Training: {comparison_df.loc[fastest_idx, 'model_name']} \"\n",
    "      f\"({comparison_df.loc[fastest_idx, 'avg_fit_time']:.3f}s)\")\n",
    "\n",
    "# Calculate improvement from baseline\n",
    "baseline_acc = comparison_df.loc[0, 'cv_accuracy_mean']\n",
    "print(f\"\\n Improvement Over Baseline (Logistic Regression):\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    if idx > 0:\n",
    "        improvement = (row['cv_accuracy_mean'] - baseline_acc) * 100\n",
    "        print(f\"  • {row['model_name']}: {improvement:+.2f}% accuracy gain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 11: Performance Visualization\n",
    "\n",
    "Visual comparison of model performance across different metrics.\n",
    "\n",
    "### Visualizations:\n",
    "\n",
    "1. **Accuracy Comparison** - Bar chart with error bars (std)\n",
    "2. **F1-Score Comparison** - Shows balanced performance\n",
    "3. **Training Time Comparison** - Computational cost\n",
    "4. **Accuracy vs Speed Trade-off** - Scatter plot\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Model Comparison Results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. CV Accuracy Comparison with error bars\n",
    "ax1 = axes[0, 0]\n",
    "models = comparison_df['model_name']\n",
    "cv_acc = comparison_df['cv_accuracy_mean']\n",
    "cv_std = comparison_df['cv_accuracy_std']\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars1 = ax1.bar(models, cv_acc, yerr=cv_std, capsize=10, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Cross-Validation Accuracy (with Std Dev)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0.90, 1.0])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc, std in zip(bars1, cv_acc, cv_std):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.002,\n",
    "             f'{acc:.4f}\\n±{std:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 2. F1-Score Comparison\n",
    "ax2 = axes[0, 1]\n",
    "cv_f1 = comparison_df['cv_f1_mean']\n",
    "test_f1 = comparison_df['test_f1']\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars2a = ax2.bar(x - width/2, cv_f1, width, label='CV F1', color='#3498db', alpha=0.7, edgecolor='black')\n",
    "bars2b = ax2.bar(x + width/2, test_f1, width, label='Test F1', color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('F1-Score: CV vs Test Set', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models)\n",
    "ax2.set_ylim([0.90, 1.0])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "ax3 = axes[1, 0]\n",
    "train_times = comparison_df['avg_fit_time']\n",
    "\n",
    "bars3 = ax3.bar(models, train_times, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Average Training Time per Fold', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars3, train_times):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{time:.2f}s',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    ax3.set_ylim(0, max(train_times) * 1.15)\n",
    "\n",
    "# 4. Accuracy vs Speed Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax4.scatter(row['avg_fit_time'], row['cv_accuracy_mean'], \n",
    "               s=300, color=colors[idx], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax4.annotate(row['model_name'], \n",
    "                (row['avg_fit_time'], row['cv_accuracy_mean']),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=11, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[idx], alpha=0.3))\n",
    "\n",
    "ax4.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('CV Accuracy', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Accuracy vs Training Time Trade-off', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Step 12: Feature Importance Analysis\n",
    "\n",
    "### Understanding Feature Importance\n",
    "\n",
    "Feature importance tells us which features contribute most to the model's predictions.\n",
    "\n",
    "**For Tree-Based Models (Random Forest & XGBoost):**\n",
    "- Based on how much each feature reduces impurity/error\n",
    "- Higher importance = more useful for making predictions\n",
    "\n",
    "**For Logistic Regression:**\n",
    "- Based on absolute coefficient values\n",
    "- Larger coefficients = stronger influence on predictions\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "1. **Model Interpretability** - Understand what drives predictions\n",
    "2. **Feature Selection** - Identify which features could be removed\n",
    "3. **Business Insights** - Learn what makes a listing valuable\n",
    "4. **Validation** - Ensure model uses sensible features\n",
    "\n",
    "### Expected Important Features:\n",
    "\n",
    "- **price** - Direct impact on value perception\n",
    "- **price_per_bedroom/bathroom** - Value efficiency metrics\n",
    "- **bedrooms, beds** - Capacity features\n",
    "- **location features** - Geographic value\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# 1. Random Forest Feature Importance\n",
    "print(f\"\\n Random Forest - Top 10 Most Important Features:\")\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<25} {'Importance':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, (_, row) in enumerate(rf_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['feature']:<25} {row['importance']:>12.6f}\")\n",
    "\n",
    "# 2. XGBoost Feature Importance\n",
    "print(f\"\\n XGBoost - Top 10 Most Important Features:\")\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<25} {'Importance':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for idx, (_, row) in enumerate(xgb_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['feature']:<25} {row['importance']:>12.6f}\")\n",
    "\n",
    "# 3. Logistic Regression Coefficients\n",
    "print(f\"\\n Logistic Regression - Top 10 Features by Coefficient Magnitude:\")\n",
    "# For multi-class, take mean absolute coefficient across classes\n",
    "lr_coef_mean = np.abs(lr_model.coef_).mean(axis=0)\n",
    "lr_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': lr_coef_mean\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Feature':<25} {'Abs Coefficient':>16}\")\n",
    "print(\"-\" * 49)\n",
    "for idx, (_, row) in enumerate(lr_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['feature']:<25} {row['importance']:>16.6f}\")\n",
    "\n",
    "## Checking if w2v_score is in top features\n",
    "print(f\"\\n NLP Feature (w2v_score) Rankings:\")\n",
    "rf_rank = rf_importance.reset_index(drop=True)\n",
    "rf_rank = rf_rank[rf_rank['feature'] == 'w2v_score'].index[0] + 1\n",
    "\n",
    "xgb_rank = xgb_importance.reset_index(drop=True)\n",
    "xgb_rank = xgb_rank[xgb_rank['feature'] == 'w2v_score'].index[0] + 1\n",
    "\n",
    "lr_rank = lr_importance.reset_index(drop=True)\n",
    "lr_rank = lr_rank[lr_rank['feature'] == 'w2v_score'].index[0] + 1\n",
    "\n",
    "print(f\"  • Random Forest: Rank #{rf_rank} out of {len(feature_names)}\")\n",
    "print(f\"  • XGBoost: Rank #{xgb_rank} out of {len(feature_names)}\")\n",
    "print(f\"  • Logistic Regression: Rank #{lr_rank} out of {len(feature_names)}\")\n",
    "\n",
    "if min(rf_rank, xgb_rank, lr_rank) <= 10:\n",
    "    print(f\"\\n NLP feature is highly important across all models!\")\n",
    "elif min(rf_rank, xgb_rank, lr_rank) <= 15:\n",
    "    print(f\"\\n NLP feature has moderate importance.\")\n",
    "else:\n",
    "    print(f\"\\n NLP feature has lower importance than expected.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Step 13: Detailed Classification Reports\n",
    "\n",
    "### Per-Class Performance Analysis\n",
    "\n",
    "Classification reports show how well each model performs on each value category.\n",
    "\n",
    "**Metrics Explained:**\n",
    "\n",
    "- **Precision:** Of all listings predicted as this class, what % were correct?\n",
    "  - High precision = few false positives\n",
    "  \n",
    "- **Recall:** Of all actual listings in this class, what % did we identify?\n",
    "  - High recall = few false negatives\n",
    "  \n",
    "- **F1-Score:** Harmonic mean of precision and recall\n",
    "  - Balanced measure of performance\n",
    "  \n",
    "- **Support:** Number of actual samples in this class\n",
    "\n",
    "**What to Look For:**\n",
    "\n",
    "- Are all three classes performing similarly?\n",
    "- Is one class much harder to predict?\n",
    "- Do models agree on which class is hardest?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Classification Reports\n",
    "print(\"=\"*80)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get class names\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(f\"\\n LOGISTIC REGRESSION\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(y_test, y_pred_lr, target_names=class_names, digits=4))\n",
    "\n",
    "# 2. Random Forest\n",
    "print(f\"\\n RANDOM FOREST\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(y_test, y_pred_rf, target_names=class_names, digits=4))\n",
    "\n",
    "# 3. XGBoost\n",
    "print(f\"\\n XGBOOST\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=class_names, digits=4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##  Step 14: Confusion Matrix Analysis\n",
    "\n",
    "### Understanding Confusion Matrices\n",
    "\n",
    "A confusion matrix shows where the model makes mistakes:\n",
    "\n",
    "```\n",
    "                Predicted\n",
    "              Poor Fair Excellent\n",
    "Actual Poor    [TP] [FP] [FP]\n",
    "       Fair    [FN] [TP] [FP]\n",
    "       Excellent [FN] [FN] [TP]\n",
    "```\n",
    "\n",
    "- **Diagonal (TP):** Correct predictions\n",
    "- **Off-diagonal:** Errors\n",
    "\n",
    "**Common Error Patterns:**\n",
    "\n",
    "- **Adjacent Class Confusion:** Fair ↔ Poor or Fair ↔ Excellent\n",
    "  - Expected: These classes are similar\n",
    "  \n",
    "- **Extreme Class Confusion:** Poor ↔ Excellent\n",
    "  - Problematic: These classes are very different\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_pred = [\n",
    "    ('Logistic Regression', y_pred_lr),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('XGBoost', y_pred_xgb)\n",
    "]\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(models_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=axes[idx], cbar_kws={'label': 'Percentage (%)'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Step 15: Final Summary and Conclusions\n",
    "\n",
    "\n",
    "We have successfully integrated Word2Vec NLP scores with machine learning models and evaluated their performance using rigorous cross-validation.\n",
    "\n",
    "### Model Performance Summary:\n",
    "\n",
    "All three models achieved excellent performance (>95% accuracy), demonstrating that:\n",
    "1.  The feature engineering was effective\n",
    "2.  The data preprocessing was sound\n",
    "3.  No data leakage issues\n",
    "\n",
    "### NLP Feature Impact\n",
    "\n",
    "**The Word2Vec score (`w2v_score`) showed minimal contribution to model predictions:**\n",
    "\n",
    "| Model | w2v_score Rank | Out of |\n",
    "|-------|----------------|--------|\n",
    "| Random Forest | #17 | 27 |\n",
    "| XGBoost | #25 | 27 |\n",
    "| Logistic Regression | #27 (last) | 27 |\n",
    "\n",
    "**Why This Happened:**\n",
    "- The target variable (`value_category`) is derived from `rating / price`\n",
    "- Since review-based features were removed, `price` and price-derived features dominate predictions\n",
    "- Listing descriptions (captured by w2v_score) don't strongly correlate with actual guest ratings\n",
    "- When combined with powerful price features, NLP contribution becomes negligible\n",
    "\n",
    " While Fatih's standalone NLP model achieved 51.7% accuracy using only descriptions, when combined with price features, the NLP signal is overshadowed.\n",
    "\n",
    "### Model Selection Recommendations:\n",
    "\n",
    "**For Production Deployment:**\n",
    "- If **speed is critical**: Use Logistic Regression\n",
    "- If **balance is needed**: Use Random Forest  \n",
    "- If **maximum accuracy is required**: Use XGBoost\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
