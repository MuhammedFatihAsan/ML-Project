# Airbnb Price-Quality Classification Project

## ğŸ“Š Dataset

### Download Links
- **San Francisco listings:** [Download here](https://insideairbnb.com/san-francisco)
- **San Diego listings:** [Download here](https://insideairbnb.com/san-diego)

### Place the files in the correct location:
```
data/raw/san_francisco.csv
data/raw/san_diego.csv
```

### Dataset Statistics:
- **San Francisco:** 7,780 listings Ã— 79 features
- **San Diego:** 13,162 listings Ã— 79 features
- **Combined:** 20,942 listings

---

## ğŸ“ Project Directory Structure

```
ML-Project/
â”‚
â”œâ”€â”€ README.md                          # Project overview and instructions
â”œâ”€â”€ .gitignore                         # Specifies files to be ignored by Git
â”œâ”€â”€ requirements.txt                   # List of Python dependencies
â”‚
â”œâ”€â”€ data/                              # DATA STORE (Ignored by Git, keep local)
â”‚   â”œâ”€â”€ raw/                          # Original, immutable data dump
â”‚   â”‚   â”œâ”€â”€ san_francisco.csv
â”‚   â”‚   â””â”€â”€ san_diego.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                    # Cleaned, canonical data sets for modeling
â”‚   â”‚   â”œâ”€â”€ listings_cleaned.csv
â”‚   â”‚   â”œâ”€â”€ listings_with_algebraic_features.csv
â”‚   â”‚   â”œâ”€â”€ listings_with_categorical_encoding.csv
â”‚   â”‚   â”œâ”€â”€ listings_final_selected_features.csv
â”‚   â”‚   â”œâ”€â”€ X_train_scaled.csv
â”‚   â”‚   â”œâ”€â”€ X_test_scaled.csv
â”‚   â”‚   â”œâ”€â”€ y_train.csv
â”‚   â”‚   â””â”€â”€ y_test.csv
â”‚   â”‚
â”‚   â””â”€â”€ external/                     # Data from third party sources
â”‚
â”œâ”€â”€ notebooks/                        # JUPYTER NOTEBOOKS
â”‚   â”œâ”€â”€ week1/                       # Week 1: Data Preparation & Feature Engineering
â”‚   â”‚   â”œâ”€â”€ 01_data_exploration_omer.ipynb          # T1.1-T1.6: Data prep
â”‚   â”‚   â”œâ”€â”€ 02_nlp_sentiment_fatih.ipynb            # T1.7-T1.12: NLP
â”‚   â”‚   â””â”€â”€ 03_eda_target_emircan.ipynb             # T1.13-T1.18: EDA
â”‚   â”‚
â”‚   â”œâ”€â”€ week2/                       # Week 2: Model Development & Comparison
â”‚   â”‚   â”œâ”€â”€ 04_supervised_models_omer.ipynb         # Supervised learning
â”‚   â”‚   â”œâ”€â”€ 05_unsupervised_models_fatih.ipynb      # Unsupervised learning
â”‚   â”‚   â””â”€â”€ 06_ensemble_evaluation_emircan.ipynb    # Ensemble & evaluation
â”‚   â”‚
â”‚   â””â”€â”€ week3/                       # Week 3: Advanced Techniques & Final Analysis
â”‚       â”œâ”€â”€ 07_deep_learning_nlp_omer.ipynb         # Deep learning & BERT
â”‚       â”œâ”€â”€ 08_optimization_tuning_fatih.ipynb      # Hyperparameter tuning
â”‚       â””â”€â”€ 09_interpretability_report_emircan.ipynb # SHAP, LIME, final report
â”‚
â”œâ”€â”€ src/                             # SOURCE CODE (Production ready code)
â”‚   â”œâ”€â”€ __init__.py                  # Makes src a Python module
â”‚   â”œâ”€â”€ data_loader.py               # Scripts to download or generate data
â”‚   â”œâ”€â”€ preprocessing.py             # Scripts to clean data and generate features
â”‚   â”œâ”€â”€ feature_engineering.py       # Algebraic feature creation
â”‚   â”œâ”€â”€ nlp_processing.py            # NLP and sentiment analysis functions
â”‚   â”œâ”€â”€ visualization.py             # Scripts to create common plots
â”‚   â””â”€â”€ models.py                    # Scripts to train models and make predictions
â”‚
â”œâ”€â”€ models/                          # SERIALIZED MODELS
â”‚   â”œâ”€â”€ standard_scaler.pkl          # Fitted StandardScaler
â”‚   â”œâ”€â”€ xgboost_classifier.pkl       # Trained XGBoost model
â”‚   â”œâ”€â”€ random_forest.pkl            # Trained Random Forest model
â”‚   â””â”€â”€ kmeans_clustering.pkl        # Trained K-Means model
â”‚
â”œâ”€â”€ outputs/                         # OUTPUT FILES
â”‚   â”œâ”€â”€ reports/                     # Generated reports
â”‚   â”‚   â”œâ”€â”€ Task_1.3_Algebraic_Features_Report.docx
â”‚   â”‚   â”œâ”€â”€ eda_report.pdf
â”‚   â”‚   â”œâ”€â”€ model_performance_report.pdf
â”‚   â”‚   â””â”€â”€ final_report.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€ figures/                     # Generated graphics and figures
â”‚   â”‚   â”œâ”€â”€ correlation_heatmap.png
â”‚   â”‚   â”œâ”€â”€ confusion_matrices.png
â”‚   â”‚   â”œâ”€â”€ roc_curves.png
â”‚   â”‚   â”œâ”€â”€ shap_analysis.png
â”‚   â”‚   â”œâ”€â”€ feature_importance_comparison.png
â”‚   â”‚   â”œâ”€â”€ scaling_comparison.png
â”‚   â”‚   â””â”€â”€ train_test_split_distribution.png
â”‚   â”‚
â”‚   â””â”€â”€ *.csv                        # Analysis results (correlations, VIF, etc.)
â”‚
â””â”€â”€ tests/                           # Unit tests for src/ code
```

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone <repository-url>
cd ML-Project
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Download datasets
Place the San Francisco and San Diego CSV files in `data/raw/`

### 4. Run notebooks
Navigate to `notebooks/week1/` and start with `01_data_exploration_omer.ipynb`

---

## ğŸ‘¥ Team Members

- **Omer:** Data preparation, supervised learning, deep learning
- **Fatih:** NLP & sentiment analysis, unsupervised learning, optimization
- **Emircan:** EDA, ensemble methods, interpretability & reporting

---

## ğŸ“… Project Timeline

### Week 1: Data Preparation & Feature Engineering
- âœ… Task 1.1-1.6: Data exploration, cleaning, feature engineering (Omer)
- Task 1.7-1.12: NLP and sentiment analysis (Fatih)
- Task 1.13-1.18: Exploratory data analysis (Emircan)

### Week 2: Model Development & Comparison
- Task 2.1-2.6: Supervised learning models (Omer)
- Task 2.7-2.12: Unsupervised learning models (Fatih)
- Task 2.13-2.18: Ensemble methods and evaluation (Emircan)

### Week 3: Advanced Techniques & Final Analysis
- Task 3.1-3.6: Deep learning and BERT (Omer)
- Task 3.7-3.12: Hyperparameter tuning and optimization (Fatih)
- Task 3.13-3.18: Model interpretability and final report (Emircan)

---

## ğŸ¯ Project Goal

Classify Airbnb listings into value categories (Poor Value, Fair Value, Excellent Value) based on the relationship between price and quality metrics.

### Target Variable
- **value_category:** 3-class classification
  - Poor Value (0): High price, low quality
  - Fair Value (1): Balanced price-quality ratio
  - Excellent Value (2): Low price, high quality

### Key Features (28 selected)
Selected through correlation analysis, VIF testing, and feature importance scoring from 94 engineered features.

---



---

## ğŸ“ˆ Model Performance (To be updated in Week 2)

| Model | Accuracy | F1-Score | Notes |
|-------|----------|----------|-------|
| XGBoost | TBD | TBD | Supervised |
| Random Forest | TBD | TBD | Supervised |
| K-Means | TBD | TBD | Unsupervised |

---

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **Data Processing:** pandas, numpy
- **Visualization:** matplotlib, seaborn
- **Machine Learning:** scikit-learn, xgboost
- **Deep Learning:** TensorFlow/PyTorch (Week 3)
- **NLP:** NLTK, spaCy, transformers (BERT)
- **Model Interpretation:** SHAP, LIME

---

## ğŸ“ License

This project is for educational purposes as part of a Machine Learning course.

---


---

